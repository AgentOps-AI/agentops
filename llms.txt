## README.md

<div align="center">
  <a href="https://agentops.ai?ref=gh">
    <img src="docs/images/external/logo/github-banner.png" alt="Logo">
  </a>
</div>

<div align="center">
  <em>Observability and DevTool platform for AI Agents</em>
</div>

<br />

<div align="center">
  <a href="https://pepy.tech/project/agentops">
    <img src="https://static.pepy.tech/badge/agentops/month" alt="Downloads">
  </a>
  <a href="https://github.com/agentops-ai/agentops/issues">
  <img src="https://img.shields.io/github/commit-activity/m/agentops-ai/agentops" alt="git commit activity">
  </a>
  <img src="https://img.shields.io/pypi/v/agentops?&color=3670A0" alt="PyPI - Version">
  <a href="https://github.com/AgentOps-AI/agentops-ts">
    <img src="https://img.shields.io/badge/TypeScript%20SDK-Available-blue?&color=3670A0" alt="TypeScript SDK">
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/License-MIT-yellow.svg?&color=3670A0" alt="License: MIT">
  </a>
  <a href="https://smithery.ai/server/@AgentOps-AI/agentops-mcp">
    <img src="https://smithery.ai/badge/@AgentOps-AI/agentops-mcp"/>
  </a>
</div>

<p align="center">
  <a href="https://twitter.com/agentopsai/">
    <img src="https://img.shields.io/twitter/follow/agentopsai?style=social" alt="Twitter" style="height: 20px;">
  </a>
  <a href="https://discord.gg/FagdcwwXRR">
    <img src="https://img.shields.io/badge/discord-7289da.svg?style=flat-square&logo=discord" alt="Discord" style="height: 20px;">
  </a>
  <a href="https://app.agentops.ai/?ref=gh">
    <img src="https://img.shields.io/badge/Dashboard-blue.svg?style=flat-square" alt="Dashboard" style="height: 20px;">
  </a>
  <a href="https://docs.agentops.ai/introduction">
    <img src="https://img.shields.io/badge/Documentation-orange.svg?style=flat-square" alt="Documentation" style="height: 20px;">
  </a>
  <a href="https://entelligence.ai/AgentOps-AI&agentops">
    <img src="https://img.shields.io/badge/Chat%20with%20Docs-green.svg?style=flat-square" alt="Chat with Docs" style="height: 20px;">
  </a>
</p>

<div align="center">
  <video src="https://github.com/user-attachments/assets/dfb4fa8d-d8c4-4965-9ff6-5b8514c1c22f" width="650" autoplay loop muted></video>
</div>

<br/>

AgentOps helps developers build, evaluate, and monitor AI agents. From prototype to production.

## Key Integrations üîå

<div align="center" style="background-color: white; padding: 20px; border-radius: 10px; margin: 0 auto; max-width: 800px;">
  <div style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 30px; margin-bottom: 20px;">
    <a href="https://docs.agentops.ai/v2/integrations/openai_agents_python"><img src="docs/images/external/openai/agents-sdk.svg" height="45" alt="OpenAI Agents SDK"></a>
    <a href="https://docs.agentops.ai/v1/integrations/crewai"><img src="docs/v1/img/docs-icons/crew-banner.png" height="45" alt="CrewAI"></a>
    <a href="https://docs.ag2.ai/docs/ecosystem/agentops"><img src="docs/images/external/ag2/ag2-logo.svg" height="45" alt="AG2 (AutoGen)"></a>
    <a href="https://docs.agentops.ai/v1/integrations/microsoft"><img src="docs/images/external/microsoft/microsoft_logo.svg" height="45" alt="Microsoft"></a>
  </div>
  
  <div style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 30px; margin-bottom: 20px;">
    <a href="https://docs.agentops.ai/v1/integrations/langchain"><img src="docs/images/external/langchain/langchain-logo.svg" height="45" alt="LangChain"></a>
    <a href="https://docs.agentops.ai/v1/integrations/camel"><img src="docs/images/external/camel/camel.png" height="45" alt="Camel AI"></a>
    <a href="https://docs.llamaindex.ai/en/stable/module_guides/observability/?h=agentops#agentops"><img src="docs/images/external/ollama/ollama-icon.png" height="45" alt="LlamaIndex"></a>
    <a href="https://docs.agentops.ai/v1/integrations/cohere"><img src="docs/images/external/cohere/cohere-logo.svg" height="45" alt="Cohere"></a>
  </div>
</div>

|                                       |                                                               |
| ------------------------------------- | ------------------------------------------------------------- |
| üìä **Replay Analytics and Debugging** | Step-by-step agent execution graphs                           |
| üí∏ **LLM Cost Management**            | Track spend with LLM foundation model providers               |
| üß™ **Agent Benchmarking**             | Test your agents against 1,000+ evals                         |
| üîê **Compliance and Security**        | Detect common prompt injection and data exfiltration exploits |
| ü§ù **Framework Integrations**         | Native Integrations with CrewAI, AG2 (AutoGen), Camel AI, & LangChain         |

## Quick Start ‚å®Ô∏è

```bash
pip install agentops
```


#### Session replays in 2 lines of code

Initialize the AgentOps client and automatically get analytics on all your LLM calls.

[Get an API key](https://app.agentops.ai/settings/projects)

```python
import agentops

# Beginning of your program (i.e. main.py, __init__.py)
agentops.init( < INSERT YOUR API KEY HERE >)

...

# End of program
agentops.end_session('Success')
```

All your sessions can be viewed on the [AgentOps dashboard](https://app.agentops.ai?ref=gh)
<br/>

<details>
  <summary>Agent Debugging</summary>
  <a href="https://app.agentops.ai?ref=gh">
    <img src="docs/images/external/app_screenshots/session-drilldown-metadata.png" style="width: 90%;" alt="Agent Metadata"/>
  </a>
  <a href="https://app.agentops.ai?ref=gh">
    <img src="docs/images/external/app_screenshots/chat-viewer.png" style="width: 90%;" alt="Chat Viewer"/>
  </a>
  <a href="https://app.agentops.ai?ref=gh">
    <img src="docs/images/external/app_screenshots/session-drilldown-graphs.png" style="width: 90%;" alt="Event Graphs"/>
  </a>
</details>

<details>
  <summary>Session Replays</summary>
  <a href="https://app.agentops.ai?ref=gh">
    <img src="docs/images/external/app_screenshots/session-replay.png" style="width: 90%;" alt="Session Replays"/>
  </a>
</details>

<details>
  <summary>Summary Analytics</summary>
  <a href="https://app.agentops.ai?ref=gh">
   <img src="docs/images/external/app_screenshots/overview.png" style="width: 90%;" alt="Summary Analytics"/>
  </a>
  <a href="https://app.agentops.ai?ref=gh">
   <img src="docs/images/external/app_screenshots/overview-charts.png" style="width: 90%;" alt="Summary Analytics Charts"/>
  </a>
</details>


### First class Developer Experience
Add powerful observability to your agents, tools, and functions with as little code as possible: one line at a time.
<br/>
Refer to our [documentation](http://docs.agentops.ai)

```python
# Create a session span (root for all other spans)
from agentops.sdk.decorators import session

@session
def my_workflow():
    # Your session code here
    return result
```

```python
# Create an agent span for tracking agent operations
from agentops.sdk.decorators import agent

@agent
class MyAgent:
    def __init__(self, name):
        self.name = name
        
    # Agent methods here
```

```python
# Create operation/task spans for tracking specific operations
from agentops.sdk.decorators import operation, task

@operation  # or @task
def process_data(data):
    # Process the data
    return result
```

```python
# Create workflow spans for tracking multi-operation workflows
from agentops.sdk.decorators import workflow

@workflow
def my_workflow(data):
    # Workflow implementation
    return result
```

```python
# Nest decorators for proper span hierarchy
from agentops.sdk.decorators import session, agent, operation

@agent
class MyAgent:
    @operation
    def nested_operation(self, message):
        return f"Processed: {message}"
        
    @operation
    def main_operation(self):
        result = self.nested_operation("test message")
        return result

@session
def my_session():
    agent = MyAgent()
    return agent.main_operation()
```

All decorators support:
- Input/Output Recording
- Exception Handling
- Async/await functions
- Generator functions
- Custom attributes and names

## Integrations ü¶æ

### OpenAI Agents SDK üñáÔ∏è

Build multi-agent systems with tools, handoffs, and guardrails. AgentOps natively integrates with the OpenAI Agents SDKs for both Python and TypeScript.

#### Python

```bash
pip install openai-agents
```

- [Python integration guide](https://docs.agentops.ai/v2/integrations/openai_agents_python)
- [OpenAI Agents Python documentation](https://openai.github.io/openai-agents-python/)

#### TypeScript

```bash
npm install agentops @openai/agents
```

- [TypeScript integration guide](https://docs.agentops.ai/v2/integrations/openai_agents_js)
- [OpenAI Agents JS documentation](https://openai.github.io/openai-agents-js)

### CrewAI üõ∂

Build Crew agents with observability in just 2 lines of code. Simply set an `AGENTOPS_API_KEY` in your environment, and your crews will get automatic monitoring on the AgentOps dashboard.

```bash
pip install 'crewai[agentops]'
```

- [AgentOps integration example](https://docs.agentops.ai/v1/integrations/crewai)
- [Official CrewAI documentation](https://docs.crewai.com/how-to/AgentOps-Observability)

### AG2 ü§ñ
With only two lines of code, add full observability and monitoring to AG2 (formerly AutoGen) agents. Set an `AGENTOPS_API_KEY` in your environment and call `agentops.init()`

- [AG2 Observability Example](https://docs.ag2.ai/notebooks/agentchat_agentops)
- [AG2 - AgentOps Documentation](https://docs.ag2.ai/docs/ecosystem/agentops)

### Camel AI üê™

Track and analyze CAMEL agents with full observability. Set an `AGENTOPS_API_KEY` in your environment and initialize AgentOps to get started.

- [Camel AI](https://www.camel-ai.org/) - Advanced agent communication framework
- [AgentOps integration example](https://docs.agentops.ai/v1/integrations/camel)
- [Official Camel AI documentation](https://docs.camel-ai.org/cookbooks/agents_tracking.html)

<details>
  <summary>Installation</summary>

```bash
pip install "camel-ai[all]==0.2.11"
pip install agentops
```

```python
import os
import agentops
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Initialize AgentOps
agentops.init(os.getenv("AGENTOPS_API_KEY"), tags=["CAMEL Example"])

# Import toolkits after AgentOps init for tracking
from camel.toolkits import SearchToolkit

# Set up the agent with search tools
sys_msg = BaseMessage.make_assistant_message(
    role_name='Tools calling operator',
    content='You are a helpful assistant'
)

# Configure tools and model
tools = [*SearchToolkit().get_tools()]
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
)

# Create and run the agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

response = camel_agent.step("What is AgentOps?")
print(response)

agentops.end_session("Success")
```

Check out our [Camel integration guide](https://docs.agentops.ai/v1/integrations/camel) for more examples including multi-agent scenarios.
</details>

### Langchain ü¶úüîó

AgentOps works seamlessly with applications built using Langchain. To use the handler, install Langchain as an optional dependency:

<details>
  <summary>Installation</summary>
  
```shell
pip install agentops[langchain]
```

To use the handler, import and set

```python
import os
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from agentops.integration.callbacks.langchain import LangchainCallbackHandler

AGENTOPS_API_KEY = os.environ['AGENTOPS_API_KEY']
handler = LangchainCallbackHandler(api_key=AGENTOPS_API_KEY, tags=['Langchain Example'])

llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
                 callbacks=[handler],
                 model='gpt-3.5-turbo')

agent = initialize_agent(tools,
                         llm,
                         agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
                         verbose=True,
                         callbacks=[handler], # You must pass in a callback handler to record your agent
                         handle_parsing_errors=True)
```

Check out the [Langchain Examples Notebook](./examples/langchain/langchain_examples.ipynb) for more details including Async handlers.

</details>

### Cohere ‚å®Ô∏è

First class support for Cohere(>=5.4.0). This is a living integration, should you need any added functionality please message us on Discord!

- [AgentOps integration example](https://docs.agentops.ai/v1/integrations/cohere)
- [Official Cohere documentation](https://docs.cohere.com/reference/about)

<details>
  <summary>Installation</summary>
  
```bash
pip install cohere
```

```python python
import cohere
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)
co = cohere.Client()

chat = co.chat(
    message="Is it pronounced ceaux-hear or co-hehray?"
)

print(chat)

agentops.end_session('Success')
```

```python python
import cohere
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)

co = cohere.Client()

stream = co.chat_stream(
    message="Write me a haiku about the synergies between Cohere and AgentOps"
)

for event in stream:
    if event.event_type == "text-generation":
        print(event.text, end='')

agentops.end_session('Success')
```
</details>


### Anthropic Ôπ®

Track agents built with the Anthropic Python SDK (>=0.32.0).

- [AgentOps integration guide](https://docs.agentops.ai/v1/integrations/anthropic)
- [Official Anthropic documentation](https://docs.anthropic.com/en/docs/welcome)

<details>
  <summary>Installation</summary>
  
```bash
pip install anthropic
```

```python python
import anthropic
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)

client = anthropic.Anthropic(
    # This is the default and can be omitted
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)

message = client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Tell me a cool fact about AgentOps",
            }
        ],
        model="claude-3-opus-20240229",
    )
print(message.content)

agentops.end_session('Success')
```

Streaming
```python python
import anthropic
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)

client = anthropic.Anthropic(
    # This is the default and can be omitted
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)

stream = client.messages.create(
    max_tokens=1024,
    model="claude-3-opus-20240229",
    messages=[
        {
            "role": "user",
            "content": "Tell me something cool about streaming agents",
        }
    ],
    stream=True,
)

response = ""
for event in stream:
    if event.type == "content_block_delta":
        response += event.delta.text
    elif event.type == "message_stop":
        print("\n")
        print(response)
        print("\n")
```

Async

```python python
import asyncio
from anthropic import AsyncAnthropic

client = AsyncAnthropic(
    # This is the default and can be omitted
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)


async def main() -> None:
    message = await client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Tell me something interesting about async agents",
            }
        ],
        model="claude-3-opus-20240229",
    )
    print(message.content)


await main()
```
</details>

### Mistral „ÄΩÔ∏è

Track agents built with the Mistral Python SDK (>=0.32.0).

- [AgentOps integration example](./examples/mistral//mistral_example.ipynb)
- [Official Mistral documentation](https://docs.mistral.ai)

<details>
  <summary>Installation</summary>
  
```bash
pip install mistralai
```

Sync

```python python
from mistralai import Mistral
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get("MISTRAL_API_KEY"),
)

message = client.chat.complete(
        messages=[
            {
                "role": "user",
                "content": "Tell me a cool fact about AgentOps",
            }
        ],
        model="open-mistral-nemo",
    )
print(message.choices[0].message.content)

agentops.end_session('Success')
```

Streaming

```python python
from mistralai import Mistral
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
agentops.init(<INSERT YOUR API KEY HERE>)

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get("MISTRAL_API_KEY"),
)

message = client.chat.stream(
        messages=[
            {
                "role": "user",
                "content": "Tell me something cool about streaming agents",
            }
        ],
        model="open-mistral-nemo",
    )

response = ""
for event in message:
    if event.data.choices[0].finish_reason == "stop":
        print("\n")
        print(response)
        print("\n")
    else:
        response += event.text

agentops.end_session('Success')
```

Async

```python python
import asyncio
from mistralai import Mistral

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get("MISTRAL_API_KEY"),
)


async def main() -> None:
    message = await client.chat.complete_async(
        messages=[
            {
                "role": "user",
                "content": "Tell me something interesting about async agents",
            }
        ],
        model="open-mistral-nemo",
    )
    print(message.choices[0].message.content)


await main()
```

Async Streaming

```python python
import asyncio
from mistralai import Mistral

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get("MISTRAL_API_KEY"),
)


async def main() -> None:
    message = await client.chat.stream_async(
        messages=[
            {
                "role": "user",
                "content": "Tell me something interesting about async streaming agents",
            }
        ],
        model="open-mistral-nemo",
    )

    response = ""
    async for event in message:
        if event.data.choices[0].finish_reason == "stop":
            print("\n")
            print(response)
            print("\n")
        else:
            response += event.text


await main()
```
</details>



### CamelAI Ôπ®

Track agents built with the CamelAI Python SDK (>=0.32.0).

- [CamelAI integration guide](https://docs.camel-ai.org/cookbooks/agents_tracking.html#)
- [Official CamelAI documentation](https://docs.camel-ai.org/index.html)

<details>
  <summary>Installation</summary>
  
```bash
pip install camel-ai[all]
pip install agentops
```

```python python
#Import Dependencies
import agentops
import os
from getpass import getpass
from dotenv import load_dotenv

#Set Keys
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or "<your openai key here>"
agentops_api_key = os.getenv("AGENTOPS_API_KEY") or "<your agentops key here>"



```
</details>

[You can find usage examples here!](examples/camelai_examples/README.md).



### LiteLLM üöÖ

AgentOps provides support for LiteLLM(>=1.3.1), allowing you to call 100+ LLMs using the same Input/Output Format. 

- [AgentOps integration example](https://docs.agentops.ai/v1/integrations/litellm)
- [Official LiteLLM documentation](https://docs.litellm.ai/docs/providers)

<details>
  <summary>Installation</summary>
  
```bash
pip install litellm
```

```python python
# Do not use LiteLLM like this
# from litellm import completion
# ...
# response = completion(model="claude-3", messages=messages)

# Use LiteLLM like this
import litellm
...
response = litellm.completion(model="claude-3", messages=messages)
# or
response = await litellm.acompletion(model="claude-3", messages=messages)
```
</details>

### LlamaIndex ü¶ô


AgentOps works seamlessly with applications built using LlamaIndex, a framework for building context-augmented generative AI applications with LLMs.

<details>
  <summary>Installation</summary>
  
```shell
pip install llama-index-instrumentation-agentops
```

To use the handler, import and set

```python
from llama_index.core import set_global_handler

# NOTE: Feel free to set your AgentOps environment variables (e.g., 'AGENTOPS_API_KEY')
# as outlined in the AgentOps documentation, or pass the equivalent keyword arguments
# anticipated by AgentOps' AOClient as **eval_params in set_global_handler.

set_global_handler("agentops")
```

Check out the [LlamaIndex docs](https://docs.llamaindex.ai/en/stable/module_guides/observability/?h=agentops#agentops) for more details.

</details>

### Llama Stack ü¶ôü•û

AgentOps provides support for Llama Stack Python Client(>=0.0.53), allowing you to monitor your Agentic applications. 

- [AgentOps integration example 1](https://github.com/AgentOps-AI/agentops/pull/530/files/65a5ab4fdcf310326f191d4b870d4f553591e3ea#diff-fdddf65549f3714f8f007ce7dfd1cde720329fe54155d54389dd50fbd81813cb)
- [AgentOps integration example 2](https://github.com/AgentOps-AI/agentops/pull/530/files/65a5ab4fdcf310326f191d4b870d4f553591e3ea#diff-6688ff4fb7ab1ce7b1cc9b8362ca27264a3060c16737fb1d850305787a6e3699)
- [Official Llama Stack Python Client](https://github.com/meta-llama/llama-stack-client-python)

### SwarmZero AI üêù

Track and analyze SwarmZero agents with full observability. Set an `AGENTOPS_API_KEY` in your environment and initialize AgentOps to get started.

- [SwarmZero](https://swarmzero.ai) - Advanced multi-agent framework
- [AgentOps integration example](https://docs.agentops.ai/v1/integrations/swarmzero)
- [SwarmZero AI integration example](https://docs.swarmzero.ai/examples/ai-agents/build-and-monitor-a-web-search-agent)
- [SwarmZero AI - AgentOps documentation](https://docs.swarmzero.ai/sdk/observability/agentops)
- [Official SwarmZero Python SDK](https://github.com/swarmzero/swarmzero)

<details>
  <summary>Installation</summary>

```bash
pip install swarmzero
pip install agentops
```

```python
from dotenv import load_dotenv
load_dotenv()

import agentops
agentops.init(<INSERT YOUR API KEY HERE>)

from swarmzero import Agent, Swarm
# ...
```
</details>

## Evaluations Roadmap üß≠

| Platform                                                                     | Dashboard                                  | Evals                                  |
| ---------------------------------------------------------------------------- | ------------------------------------------ | -------------------------------------- |
| ‚úÖ Python SDK                                                                | ‚úÖ Multi-session and Cross-session metrics | ‚úÖ Custom eval metrics                 |
| üöß Evaluation builder API                                                    | ‚úÖ Custom event tag tracking              | üîú Agent scorecards                    |
| üöß [Javascript/Typescript SDK (Alpha)](https://github.com/AgentOps-AI/agentops-node) | ‚úÖ Session replays                         | üîú Evaluation playground + leaderboard |

## Debugging Roadmap üß≠

| Performance testing                       | Environments                                                                        | LLM Testing                                 | Reasoning and execution testing                   |
| ----------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------- |
| ‚úÖ Event latency analysis                 | üîú Non-stationary environment testing                                               | üîú LLM non-deterministic function detection | üöß Infinite loops and recursive thought detection |
| ‚úÖ Agent workflow execution pricing       | üîú Multi-modal environments                                                         | üöß Token limit overflow flags               | üîú Faulty reasoning detection                     |
| üöß Success validators (external)          | üîú Execution containers                                                             | üîú Context limit overflow flags             | üîú Generative code validators                     |
| üîú Agent controllers/skill tests          | ‚úÖ Honeypot and prompt injection detection ([PromptArmor](https://promptarmor.com)) | ‚úÖ API bill tracking                        | üîú Error breakpoint analysis                      |
| üîú Information context constraint testing | üîú Anti-agent roadblocks (i.e. Captchas)                                            | üîú CI/CD integration checks                 |                                                   |
| üîú Regression testing                     | ‚úÖ Multi-agent framework visualization                                              |                                             |                                                   |

### Why AgentOps? ü§î

Without the right tools, AI agents are slow, expensive, and unreliable. Our mission is to bring your agent from prototype to production. Here's why AgentOps stands out:

- **Comprehensive Observability**: Track your AI agents' performance, user interactions, and API usage.
- **Real-Time Monitoring**: Get instant insights with session replays, metrics, and live monitoring tools.
- **Cost Control**: Monitor and manage your spend on LLM and API calls.
- **Failure Detection**: Quickly identify and respond to agent failures and multi-agent interaction issues.
- **Tool Usage Statistics**: Understand how your agents utilize external tools with detailed analytics.
- **Session-Wide Metrics**: Gain a holistic view of your agents' sessions with comprehensive statistics.

AgentOps is designed to make agent observability, testing, and monitoring easy.


## Star History

Check out our growth in the community:

<img src="https://api.star-history.com/svg?repos=AgentOps-AI/agentops&type=Date" style="max-width: 500px" width="50%" alt="Logo">

## Popular projects using AgentOps


| Repository | Stars  |
| :--------  | -----: |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/2707039?s=40&v=4" width="20" height="20" alt="">  &nbsp; [geekan](https://github.com/geekan) / [MetaGPT](https://github.com/geekan/MetaGPT) | 42787 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/130722866?s=40&v=4" width="20" height="20" alt="">  &nbsp; [run-llama](https://github.com/run-llama) / [llama_index](https://github.com/run-llama/llama_index) | 34446 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/170677839?s=40&v=4" width="20" height="20" alt="">  &nbsp; [crewAIInc](https://github.com/crewAIInc) / [crewAI](https://github.com/crewAIInc/crewAI) | 18287 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/134388954?s=40&v=4" width="20" height="20" alt="">  &nbsp; [camel-ai](https://github.com/camel-ai) / [camel](https://github.com/camel-ai/camel) | 5166 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/152537519?s=40&v=4" width="20" height="20" alt="">  &nbsp; [superagent-ai](https://github.com/superagent-ai) / [superagent](https://github.com/superagent-ai/superagent) | 5050 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/30197649?s=40&v=4" width="20" height="20" alt="">  &nbsp; [iyaja](https://github.com/iyaja) / [llama-fs](https://github.com/iyaja/llama-fs) | 4713 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/162546372?s=40&v=4" width="20" height="20" alt="">  &nbsp; [BasedHardware](https://github.com/BasedHardware) / [Omi](https://github.com/BasedHardware/Omi) | 2723 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/454862?s=40&v=4" width="20" height="20" alt="">  &nbsp; [MervinPraison](https://github.com/MervinPraison) / [PraisonAI](https://github.com/MervinPraison/PraisonAI) | 2007 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/140554352?s=40&v=4" width="20" height="20" alt="">  &nbsp; [AgentOps-AI](https://github.com/AgentOps-AI) / [Jaiqu](https://github.com/AgentOps-AI/Jaiqu) | 272 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/173542722?s=48&v=4" width="20" height="20" alt="">  &nbsp; [swarmzero](https://github.com/swarmzero) / [swarmzero](https://github.com/swarmzero/swarmzero) | 195 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/3074263?s=40&v=4" width="20" height="20" alt="">  &nbsp; [strnad](https://github.com/strnad) / [CrewAI-Studio](https://github.com/strnad/CrewAI-Studio) | 134 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/18406448?s=40&v=4" width="20" height="20" alt="">  &nbsp; [alejandro-ao](https://github.com/alejandro-ao) / [exa-crewai](https://github.com/alejandro-ao/exa-crewai) | 55 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/64493665?s=40&v=4" width="20" height="20" alt="">  &nbsp; [tonykipkemboi](https://github.com/tonykipkemboi) / [youtube_yapper_trapper](https://github.com/tonykipkemboi/youtube_yapper_trapper) | 47 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/17598928?s=40&v=4" width="20" height="20" alt="">  &nbsp; [sethcoast](https://github.com/sethcoast) / [cover-letter-builder](https://github.com/sethcoast/cover-letter-builder) | 27 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/109994880?s=40&v=4" width="20" height="20" alt="">  &nbsp; [bhancockio](https://github.com/bhancockio) / [chatgpt4o-analysis](https://github.com/bhancockio/chatgpt4o-analysis) | 19 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/14105911?s=40&v=4" width="20" height="20" alt="">  &nbsp; [breakstring](https://github.com/breakstring) / [Agentic_Story_Book_Workflow](https://github.com/breakstring/Agentic_Story_Book_Workflow) | 14 |
|<img class="avatar mr-2" src="https://avatars.githubusercontent.com/u/124134656?s=40&v=4" width="20" height="20" alt="">  &nbsp; [MULTI-ON](https://github.com/MULTI-ON) / [multion-python](https://github.com/MULTI-ON/multion-python) | 13 |


_Generated using [github-dependents-info](https://github.com/nvuillam/github-dependents-info), by [Nicolas Vuillamy](https://github.com/nvuillam)_


## TOS.md

#### Disclaimer on Error Reporting

_By implementing this SDK, you acknowledge and agree that the SDK may automatically send error reports to our servers. These reports are strictly limited to information relevant for identifying and resolving issues, such as error messages, stack traces (including file paths that may include your system username), and the SDK version. Our primary aim in collecting this data is to enhance the developer experience by identifying, diagnosing, and fixing issues more efficiently. No personal information or sensitive data will be transmitted as part of these reports without your explicit consent._

_This collection is in compliance with the California Consumer Privacy Act (CCPA). We commit to not selling your Personal Information and ensuring its confidentiality and security. California residents have the right to know about, access, and delete their Personal Information collected by us. For details on how to exercise these rights, please refer to our [Privacy Policy](https://docs.google.com/document/d/1SjWwa8nhHFLXa0ytSHv5F7osfNDclA6GU9-RvTevGDs/edit?usp=sharing). Our policy also includes information on how to opt-out of the sale of Personal Information, though we do not engage in this practice._


## CONTRIBUTING.md

# Contributing to AgentOps

Thanks for checking out AgentOps. We're building tools to help developers like you make AI agents that actually work reliably. If you've ever tried to build an agent system, you know the pain - they're a nightmare to debug, impossible to monitor, and when something goes wrong... good luck figuring out why.

We created AgentOps to solve these headaches, and we'd love your help making it even better. Our SDK hooks into all the major Python frameworks (AG2, CrewAI, LangChain) and LLM providers (OpenAI, Anthropic, Cohere, etc.) to give you visibility into what your agents are actually doing.

## How You Can Help

There are tons of ways to contribute, and we genuinely appreciate all of them:

1. **Add More Providers**: Help us support new LLM providers. Each one helps more developers monitor their agents.
2. **Improve Framework Support**: Using a framework we don't support yet? Help us add it!
3. **Make Docs Better**: Found our docs confusing? Help us fix them! Clear documentation makes everyone's life easier.
4. **Share Your Experience**: Using AgentOps? Let us know what's working and what isn't. Your feedback shapes our roadmap.

Even if you're not ready to contribute code, we'd love to hear your thoughts. Drop into our Discord, open an issue, or start a discussion. We're building this for developers like you, so your input matters.

## Table of Contents
- [Getting Started](#getting-started)
- [Development Environment](#development-environment)
- [Testing](#testing)
- [Adding LLM Providers](#adding-llm-providers)
- [Code Style](#code-style)
- [Pull Request Process](#pull-request-process)
- [Documentation](#documentation)

## Getting Started

1. **Fork and Clone**:
   First, fork the repository by clicking the 'Fork' button in the top right of the [AgentOps repository](https://github.com/AgentOps-AI/agentops). This creates your own copy of the repository where you can make changes.

   Then clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/agentops.git
   cd agentops
   ```

   Add the upstream repository to stay in sync:
   ```bash
   git remote add upstream https://github.com/AgentOps-AI/agentops.git
   git fetch upstream
   ```

   Before starting work on a new feature:
   ```bash
   git checkout main
   git pull upstream main
   git checkout -b feature/your-feature-name
   ```

2. **Install Dependencies**:
   ```bash
   pip install -e .
   ```

3. **Set Up Pre-commit Hooks**:
   ```bash
   pre-commit install
   ```

## Development Environment

1. **Environment Variables**:
   Create a `.env` file:
   ```
   AGENTOPS_API_KEY=your_api_key
   OPENAI_API_KEY=your_openai_key  # For testing
   ANTHROPIC_API_KEY=your_anthropic_key  # For testing
   # Other keys...
   ```

2. **Virtual Environment**:
   We recommend using `poetry` or `venv`:
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   .\venv\Scripts\activate   # Windows
   ```

3. **Pre-commit Setup**:
   We use pre-commit hooks to automatically format and lint code. Set them up with:
   ```bash
   pip install pre-commit
   pre-commit install
   ```

   That's it! The hooks will run automatically when you commit. To manually check all files:
   ```bash
   pre-commit run --all-files
   ```

## Testing

We use a comprehensive testing stack to ensure code quality and reliability. Our testing framework includes pytest and several specialized testing tools.

### Testing Dependencies

Install all testing dependencies:
```bash
pip install -e ".[dev]"
```

We use the following testing packages:
- `pytest==7.4.0`: Core testing framework
- `pytest-depends`: Manage test dependencies
- `pytest-asyncio`: Test async code
- `pytest-vcr`: Record and replay HTTP interactions
- `pytest-mock`: Mocking functionality
- `pyfakefs`: Mock filesystem operations
- `requests_mock==1.11.0`: Mock HTTP requests

### Using Tox

We use tox to automate and standardize testing. Tox:
- Creates isolated virtual environments for testing
- Tests against multiple Python versions (3.7-3.12)
- Runs all test suites consistently
- Ensures dependencies are correctly specified
- Verifies the package installs correctly

Run tox:
```bash
tox
```

This will:
1. Create fresh virtual environments
2. Install dependencies
3. Run pytest with our test suite
4. Generate coverage reports

### Running Tests

1. **Run All Tests**:
   ```bash
   tox
   ```

2. **Run Specific Test File**:
   ```bash
   pytest tests/llms/test_anthropic.py -v
   ```

3. **Run with Coverage**:
   ```bash
   coverage run -m pytest
   coverage report
   ```

### Writing Tests

1. **Test Structure**:
   ```python
   import pytest
   from pytest_mock import MockerFixture
   from unittest.mock import Mock, patch

   @pytest.mark.asyncio  # For async tests
   async def test_async_function():
       # Test implementation

   @pytest.mark.depends(on=['test_prerequisite'])  # Declare test dependencies
   def test_dependent_function():
       # Test implementation
   ```

2. **Recording HTTP Interactions**:
   ```python
   @pytest.mark.vcr()  # Records HTTP interactions
   def test_api_call():
       response = client.make_request()
       assert response.status_code == 200
   ```

3. **Mocking Filesystem**:
   ```python
   def test_file_operations(fs):  # fs fixture provided by pyfakefs
       fs.create_file('/fake/file.txt', contents='test')
       assert os.path.exists('/fake/file.txt')
   ```

4. **Mocking HTTP Requests**:
   ```python
   def test_http_client(requests_mock):
       requests_mock.get('http://api.example.com', json={'key': 'value'})
       response = make_request()
       assert response.json()['key'] == 'value'
   ```

### Testing Best Practices

1. **Test Categories**:
   - Unit tests: Test individual components
   - Integration tests: Test component interactions
   - End-to-end tests: Test complete workflows
   - Performance tests: Test response times and resource usage

2. **Fixtures**:
   Create reusable test fixtures in `conftest.py`:
   ```python
   @pytest.fixture
   def mock_llm_client():
       client = Mock()
       client.chat.completions.create.return_value = Mock()
       return client
   ```

3. **Test Data**:
   - Store test data in `tests/data/`
   - Use meaningful test data names
   - Document data format and purpose

4. **VCR Cassettes**:
   - Store in `tests/cassettes/`
   - Sanitize sensitive information
   - Update cassettes when API changes

### CI Testing Strategy

We use Jupyter notebooks as integration tests for LLM providers. This approach:
- Tests real-world usage patterns
- Verifies end-to-end functionality
- Ensures examples stay up-to-date
- Tests against actual LLM APIs

1. **Notebook Tests**:
   - Located in `examples/` directory
   - Each LLM provider has example notebooks
   - CI runs notebooks on PR merges to main
   - Tests run against multiple Python versions

2. **Test Workflow**:
   The `test-notebooks.yml` workflow:
   ```yaml
   name: Test Notebooks
   on:
     pull_request:
       paths:
         - "agentops/**"
         - "examples/**"
         - "tests/**"
   ```
   - Runs on PR merges and manual triggers
   - Sets up environment with provider API keys
   - Installs AgentOps from main branch
   - Executes each notebook
   - Excludes specific notebooks that require manual testing

3. **Provider Coverage**:
   Each provider should have notebooks demonstrating:
   - Basic completion calls
   - Streaming responses
   - Async operations (if supported)
   - Error handling
   - Tool usage (if applicable)

4. **Adding Provider Tests**:
   - Create notebook in `examples/provider_name/`
   - Include all provider functionality
   - Add necessary secrets to GitHub Actions
   - Update `exclude_notebooks` in workflow if manual testing needed

## Adding LLM Providers

The `agentops/llms/` directory contains provider implementations. Each provider must:

1. **Inherit from BaseProvider**:
   ```python
   @singleton
   class NewProvider(BaseProvider):
       def __init__(self, client):
           super().__init__(client)
           self._provider_name = "ProviderName"
   ```

2. **Implement Required Methods**:
   - `handle_response()`: Process LLM responses
   - `override()`: Patch the provider's methods
   - `undo_override()`: Restore original methods

3. **Handle Events**:
   Track:
   - Prompts and completions
   - Token usage
   - Timestamps
   - Errors
   - Tool usage (if applicable)

4. **Example Implementation Structure**:
   ```python
   def handle_response(self, response, kwargs, init_timestamp, session=None):
       llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
       try:
           # Process response
           llm_event.returns = response.model_dump()
           llm_event.prompt = kwargs["messages"]
           # ... additional processing
           self._safe_record(session, llm_event)
       except Exception as e:
           self._safe_record(session, ErrorEvent(trigger_event=llm_event, exception=e))
   ```

## Code Style

1. **Formatting**:
   - Use Black for Python code formatting
   - Maximum line length: 88 characters
   - Use type hints

2. **Documentation**:
   - Docstrings for all public methods
   - Clear inline comments
   - Update relevant documentation

3. **Error Handling**:
   - Use specific exception types
   - Log errors with meaningful messages
   - Include context in error messages

## Pull Request Process

1. **Branch Naming**:
   - `feature/description`
   - `fix/description`
   - `docs/description`

2. **Commit Messages**:
   - Clear and descriptive
   - Reference issues when applicable

3. **PR Requirements**:
   - Pass all tests
   - Maintain or improve code coverage
   - Include relevant documentation
   - Update CHANGELOG.md if applicable

4. **Review Process**:
   - At least one approval required
   - Address all review comments
   - Maintain PR scope

## Documentation

1. **Types of Documentation**:
   - API reference
   - Integration guides
   - Examples
   - Troubleshooting guides

2. **Documentation Location**:
   - Code documentation in docstrings
   - User guides in `docs/`
   - Examples in `examples/`

3. **Documentation Style**:
   - Clear and concise
   - Include code examples
   - Explain the why, not just the what

## Getting Help & Community

We encourage active community participation and are here to help!

### Preferred Communication Channels

1. **GitHub Issues & Discussions**:
   - Open an [issue](https://github.com/AgentOps-AI/agentops/issues) for:
     - Bug reports
     - Feature requests
     - Documentation improvements
   - Start a [discussion](https://github.com/AgentOps-AI/agentops/discussions) for:
     - Questions about usage
     - Ideas for new features
     - Community showcase
     - General feedback

2. **Discord Community**:
   - Join our [Discord server](https://discord.gg/FagdcwwXRR) for:
     - Real-time help
     - Community discussions
     - Feature announcements
     - Sharing your projects

3. **Contact Form**:
   - For private inquiries, use our [contact form](https://agentops.ai/contact)
   - Please note that public channels are preferred for technical discussions

## License

By contributing to AgentOps, you agree that your contributions will be licensed under the MIT License.


## tests/core_manual_tests/api_server/readme.md

# API server test
This is a manual test with two files. It checks to make sure that the SDK works in an API environment.

## Running the FastAPI Server
You can run FastAPI with:
1. `uvicorn main:app --reload`
2. To test, run `curl http://localhost:8000/completion` in a different terminal.

## Validate
Check in your AgentOps Dashboard 

1. two sessions are created with the `api-server-test` tag.
2. Each session should have one `LLMEvent` and one `ActionEvent`.
3. Both sessions should have an end state of `Success`
4. Neither session should have multiple agents

## tests/unit/instrumentation/openai_agents/tools/README.md

# OpenAI Agents Test Fixtures Generator

Dead simple script to grab test fixtures from OpenAI Agents API.

## Usage

```bash
# Activate venv
source .venv/bin/activate

# Run it
python -m tests.unit.instrumentation.openai_agents_tools.generate_fixtures
```

## What it does

- Makes API calls to OpenAI Agents endpoint:
  - Standard agent response
  - Agent response with tool calls
- Saves the JSON responses to `../fixtures/`
- That's it!

## Generated Fixtures

- `openai_agents_response.json` - Standard Agents API response
- `openai_agents_tool_response.json` - Agents API with tool calls

## Requirements

- OpenAI API key in env or .env file
- openai + openai-agents packages installed

## docs/README.md

# Mintlify Starter Kit

Click on `Use this template` to copy the Mintlify starter kit. The starter kit contains examples including

- Guide pages
- Navigation
- Customizations
- API Reference pages
- Use of popular components

### üë©‚Äçüíª Development

Install the [Mintlify CLI](https://www.npmjs.com/package/mintlify) to preview the documentation changes locally. To install, use the following command

```
npm i -g mintlify
```

Run the following command at the root of your documentation (where mint.json is)

```
mintlify dev
```

### üòé Publishing Changes

Changes will be deployed to production automatically after pushing to the default branch.

You can also preview changes using PRs, which generates a preview link of the docs.

#### Troubleshooting

- Mintlify dev isn't running - Run `mintlify install` it'll re-install dependencies.
- Page loads as a 404 - Make sure you are running in a folder with `mint.json`


## docs/v1/quickstart.mdx

---
title: "Quickstart"
description: "Start using AgentOps with just 2 lines of code"
---
import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Add 2 lines of code">
    <CodeTooltip />
    Get an AgentOps API key [here](https://app.agentops.ai/settings/projects)
      <CodeGroup>
        ```python python
        import agentops
        agentops.init(<INSERT YOUR API KEY HERE>)
        ```
      </CodeGroup>
    <EnvTooltip />
  </Step>
  <Step title="Run your agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è

    <Tip>After your run, AgentOps prints a clickable URL to console linking directly to your session in the Dashboard</Tip> 
     <div/>{/* Intentionally blank div for newline */}

     <Frame type="glass" caption="Clickable link to session">
        <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
    </Frame>
  </Step>
</Steps>
<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) if you liked AgentOps! (you may be our <span id="stars-text">3,000th</span> üòä)</Check>

## More basic functionality

<CardGroup cols={1}>

<Card icon="code" title="Decorate Operations">
  You can instrument functions inside your code with the `@operation` decorator, which will create spans that track function execution, parameters, and return values. These operations will be displayed in your session visualization alongside LLM calls.
  ```python python
  # Instrument a function as an operation
  from agentops.sdk.decorators import operation
  
  @operation
  def process_data(data):
      # Your function logic here
      result = data.upper()
      return result
```

</Card>
<Card icon="robot" title="Track Agents">
  If you use specific named agents within your system, you can create agent spans that contain all downstream operations using the `@agent` decorator.
  ```python python
  # Create an agent class
  from agentops.sdk.decorators import agent, operation
  
  @agent
  class MyAgent:
      def __init__(self, name):
          self.name = name
          
      @operation
      def perform_task(self, task):
          # Agent task logic here
          return f"Completed {task}"
```

</Card>
<Card icon="stop" title="Creating Sessions">
  Create a session to group all your agent operations by using the `@session` decorator. Sessions serve as the root span for all operations.
  ```python python
  # Create a session
  from agentops.sdk.decorators import session
  
  @session
  def my_workflow():
      # Your session code here
      agent = MyAgent("research-agent")
      result = agent.perform_task("data analysis")
      return result
      
  # Run the session
  my_workflow()
  ```
</Card>
</CardGroup>


## Example Code

Here is the complete code from the sections above

```python python
import agentops
from agentops.sdk.decorators import session, agent, operation

# Initialize AgentOps
agentops.init(<INSERT YOUR API KEY HERE>)

# Create an agent class
@agent
class MyAgent:
    def __init__(self, name):
        self.name = name
        
    @operation
    def perform_task(self, task):
        # Agent task logic here
        return f"Completed {task}"

# Create a session
@session
def my_workflow():
    # Your session code here
    agent = MyAgent("research-agent")
    result = agent.perform_task("data analysis")
    return result
    
# Run the session
my_workflow()
```

<Card
  title="Simple Code Example"
  icon="square-code"
  href="https://github.com/AgentOps-AI/agentops-py/blob/main/examples/openai-gpt.ipynb"
>
  Jupyter Notebook with sample code that you can run!
</Card>

<Check>
  That's all you need to get started! Check out the documentation below to see how you can record other operations. AgentOps is a lot more powerful this way!
</Check>

## Explore our more advanced functionality!

<CardGroup cols={2}>
  <Card
    title="Examples and Video Guides"
    icon="square-code"
    href="/v1/examples"
  >
    Record all of your operations the way AgentOps intends.
  </Card>
  <Card title="Tracking Agents" icon="robot" href="/v1/usage/tracking-agents">
    Associate operations with specific named agents.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/introduction.mdx

---
title: "Introduction"
description: "AgentOps is the developer favorite platform for testing, debugging, and deploying AI agents and LLM apps."
mode: "wide"
---

## Integrate with developer favorite agent frameworks

<CardGroup cols={2}>
  <Card title="OpenAI Agents SDK" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI Agents SDK" />} iconType="image" href="/v1/integrations/agents_sdk" />
  <Card title="CrewAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/v1/img/docs-icons/crew.png?raw=true" alt="CrewAI"/>} iconType="image" href="/v1/integrations/crewai" />
  <Card title="AG2" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/autogen/ag2.svg?raw=true" alt="AG2"/>} iconType="image" href="/v1/integrations/autogen" />
  <Card title="AutoGen" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/microsoft/microsoft_logo.svg?raw=true" alt="AutoGen" />} iconType="image" href="/v1/integrations/autogen" />
  <Card title="Anthropic" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/anthropic/anthropic_icon_slate.png?raw=true" alt="Anthropic" />} iconType="image" href="/v1/integrations/anthropic" />
  <Card title="Ollama" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ollama/ollama-icon.png?raw=true" alt="Ollama" />} iconType="image" href="/v1/integrations/ollama" />
  <Card title="Cohere" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/cohere/cohere-logo.svg?raw=true" alt="Cohere" />} iconType="image" href="/v1/integrations/cohere" />
  <Card title="Groq" icon="bolt" href="https://docs.agentops.ai/v1/integrations/groq" />
</CardGroup>

Observability and monitoring for your AI agents and LLM apps. And we do it all in just two lines of code...
<CodeGroup>
	```python python
	import agentops
	agentops.init(<INSERT YOUR API KEY HERE>)
	```
</CodeGroup>
... that logs everything back to your AgentOps Dashboard.

<Note>
AgentOps is also available for TypeScript/JavaScript applications. Check out our [TypeScript SDK guide](/v2/usage/typescript-sdk) for Node.js projects.
</Note>

## The AgentOps Dashboard

<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) to bookmark on GitHub, save for later üñáÔ∏è)</Check>

With just two lines of code, you can free yourself from the chains of the terminal and, instead, visualize your agents' behavior
in your AgentOps Dashboard. After setting up AgentOps, each execution of your program is recorded as a session and the above
data is automatically recorded for you.

The examples below were captured with two lines of code.

### Session Drilldown
Here you will find a list of all of your previously recorded sessions and useful data about each such as total execution time.
You also get helpful debugging info such as any SDK versions you were on if you're building on a supported agent framework like Crew or AutoGen.
LLM calls are presented as a familiar chat history view, and charts give you a breakdown of the types of events that were called and how long they took.

<Frame type="glass" caption="Session Summary">
  <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/session-summary.gif?raw=true" />
</Frame>

Find any past sessions from your Session Drawer.
<Frame type="glass" caption="Session Drawer">
  <img height="250" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/session-drawer.gif?raw=true" />
</Frame>

Most powerful of all is the Session Waterfall. On the left, a time visualization of all your LLM calls, Action events, Tool calls, and Errors.
On the right, specific details about the event you've selected on the waterfall. For instance the exact prompt and completion for a given LLM call. 
Most of which has been automatically recorded for you.

<Frame type="glass" caption="Session Waterfall">
  <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/session-waterfall.gif?raw=true" />
</Frame>


### Session Overview
View a meta-analysis of all of your sessions in a single view.
<Frame type="glass" caption="Session Overview">
  <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/overview.png?raw=true" />
</Frame>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/usage/tracking-agents.mdx

---
title: "Tracking Agents"
description: "Use the `@agent` decorator to create agent spans"
---

All operations are automatically associated with the agent that originated them. Agents are given a name which is what you will see in the dashboard.

<Frame type="glass" caption="This operation is labeled with the name of the Agent that originated it">
  <img height="200" src="/images/agent-name.png" />
</Frame>

The example below creates an agent class with a custom name:

```python
from agentops.sdk.decorators import agent

@agent(name='ResearchAgent')
class MyAgent:
    def __init__(self):
        # Agent initialization
        pass
        
    # Agent methods
```

If you don't specify a name, the agent will use the class name by default:

```python
@agent
class ResearchAgent:
    # This agent will have the name "ResearchAgent"
    pass
```

## Nesting Operations Under Agents

Operations performed by an agent should be decorated with the `@operation` decorator to ensure they're properly nested under the agent:

```python
from agentops.sdk.decorators import agent, operation

@agent
class ResearchAgent:
    @operation
    def search_web(self, query):
        # Search implementation
        return results
        
    @operation
    def analyze_data(self, data):
        # Analysis implementation
        return analysis
```

## Session Context

Agents should be created within a session context to ensure proper tracing:

```python
from agentops.sdk.decorators import session, agent, operation

@agent
class ResearchAgent:
    @operation
    def perform_research(self, topic):
        # Research implementation
        return results

@session
def research_workflow(topic):
    agent = ResearchAgent()
    return agent.perform_research(topic)
    
# Run the session
result = research_workflow("quantum computing")
```

## docs/v1/usage/dashboard-info.mdx

---
title: "Dashboard"
description: "Visualize your AgentOps analysis."
mode: "wide"
---

## Insights Dashboard

You need better insights to turn error-prone AI into stable workflows. Here's your new best friend.

### Session Drilldown
Here you will find a list of all of your previously recorded sessions and useful data about each such as total execution time.
You also get helpful debugging info such as any SDK versions you were on if you're building on a supported agent framework like Crew or AutoGen.
LLM calls are presented as a familiar chat history view, and charts give you a breakdown of the types of events that were called and how long they took.

<Frame type="glass" caption="Session Summary">
  <img height="200" src="/images/session-summary.gif" />
</Frame>

Find any past sessions from your Session Drawer.
<Frame type="glass" caption="Session Drawer">
  <img height="250" src="/images/session-drawer.gif" />
</Frame>

Most powerful of all is the Session Waterfall. On the left, a time visualization of all your LLM calls, Action events, Tool calls, and Errors.
On the right, specific details about the event you've selected on the waterfall. For instance the exact prompt and completion for a given LLM call.
Most of which has been automatically recorded for you.

<Frame type="glass" caption="Session Waterfall">
  <img height="200" src="/images/session-waterfall.gif" />
</Frame>


### Session Overview
View a meta-analysis of all of your sessions in a single view.
<Frame type="glass" caption="Session Overview">
  <img height="200" src="/images/overview.png" />
</Frame>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/intro.css"></script>

## docs/v1/usage/tracking-llm-calls.mdx

---
title: "Tracking LLM Calls"
description: "Tracking LLM Calls using the AgentOps SDK"
---

### How it works

When the AgentOps SDK detects a supported LLM provider module installed, it will automatically
start tracking its usage. No further work is required from you! üòä

### Not working? 

Try these steps:
1. Make sure you have the latest version of the AgentOps SDK installed. We are constantly updating it to support new LLM libraries and releases.
2. Make sure you are calling `agentops.init()` *after* importing the LLM module but *before* you are calling the LLM method.
3. Make sure the `instrument_llm_calls` parameter of `agentops.init()` is set to `True` (default).
4. Make sure if you have more than one concurrent session, you patch the LLM call as described [here](/v1/usage/multiple-sssions).

Still not working? Please let us know! You can find us on [Discord](https://discord.gg/DR2abmETjZ),
 [GitHub](https://github.com/AgentOps-AI/agentops), 
 or email us at [engineering@agentops.ai](mailto:engineering@agentops.ai).

To get started, just follow the quick start guide.

<CardGroup cols={2}>
  <Card title="Quick Start" icon="code" href="/v1/quickstart">
    Get started with AgentOps in under 5 minutes
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/usage/sdk-reference.mdx

---
title: "SDK Reference"
description: "All functions and classes exposed in the top layer of the SDK"
---

These are the functions and classes available with simply `import agentops`

## Functions

---

### `init()`

The first element of AgentOps is always calling .init()

**Parameters**:

- `api_key` (str, optional): API Key for AgentOps services. If not provided, the key will be read from the `AGENTOPS_API_KEY` environment variable.
- `endpoint` (str, optional): The endpoint for the AgentOps service. Defaults to 'https://api.agentops.ai'.
- `max_wait_time` (int, optional): The maximum time to wait in milliseconds before flushing the queue. Defaults to 30,000 (30 seconds).
- `max_queue_size` (int, optional): The maximum size of the event queue. Defaults to 100.
- `tags` (List[str], optional): Tags for the sessions for grouping or sorting (e.g., ["GPT-4"]).
- `override` (bool, optional): [Deprecated] Use `instrument_llm_calls` instead to decide whether to instrument LLM calls and emit LLMEvents.
- `instrument_llm_calls` (bool): Whether to instrument LLM calls and emit LLMEvents.
- `auto_start_session` (bool): Whether to start a session automatically when the client is created. You may wish to delay starting a session in order to do additional setup or starting a session on a child process.
- `inherited_session_id` (str, optional): When creating the client, passing in this value will connect the client to an existing session. This is useful when having separate processes contribute to the same session.
- `skip_auto_end_session` (bool, optional): If you are using a framework such as Crew, the framework can decide when to halt execution. Setting this parameter to true will not end your agentops session when this happens.

**Returns**:

- Session ID of the current or new session.

---

### `start_session()`

Start a new [Session](/v1/concepts/sessions) for recording events.

**Parameters**:

- `tags` (List[str], optional): Tags for the session.
- `config` ([Config](#config), optional): Client configuration object.
- `inherited_session_id` (str, optional): Session ID to continue from an existing session.

**Returns**:

- Session ID of the started session.

---

### `end_session()`

Ends the current session with the AgentOps service.

**Parameters**:

- `end_state` (str): The final state of the session. Options: Success, Fail, or Indeterminate.
- `end_state_reason` (str, optional): The reason for ending the session.
- `video` (str, optional): URL to a video recording of the session.

---

### `record()`

Record an event with the AgentOps service.

**Parameters**:

- `event` (Union[Event, ErrorEvent]): The event to record.

---


### `get_api_key()`

Retrieve the API key used by the client.

**Returns**:

- API key as a string.

---


## Types
---

### `Config`

Stores the configuration settings for AgentOps clients.

**Parameters**:

- `api_key` (str, optional): API Key for AgentOps services. If not provided, the key will be read from the `AGENTOPS_API_KEY` environment variable. If no key is found, a `ConfigurationError` is raised.
- `endpoint` (str, optional): The endpoint for the AgentOps service. If not provided, the endpoint will be read from the `AGENTOPS_API_ENDPOINT` environment variable. Defaults to 'https://api.agentops.ai'.
- `max_wait_time` (int, optional): The maximum time to wait in milliseconds before flushing the queue. Defaults to 30000.
- `max_queue_size` (int, optional): The maximum size of the event queue. Defaults to 100.

**Properties**

- **api_key** (str): Get or set the API Key for AgentOps services.
- **endpoint** (str): Get or set the endpoint for the AgentOps service.
- **max_wait_time** (int): Get or set the maximum wait time in milliseconds before flushing the queue.
- **max_queue_size** (int): Get or set the maximum size of the event queue.

---

### `LangchainCallbackHandler`

[Reference](/v1/usage/langchain-callback-handler)

This callback handler is intended to be used as an option in place of AgentOps auto-instrumenting. This is only useful
when using LangChain as your LLM calling library.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/usage/recording-operations.mdx

---
title: "Recording Operations"
description: "Instrument your code with decorators to track operations, tasks, and workflows."
---

To get the most out of AgentOps, it's important to properly instrument your code to track operations. AgentOps provides a set of decorators that make this easy.

## Using Decorators for Instrumentation

AgentOps provides several decorators that allow you to easily instrument your code:

### `@operation` / `@task` Decorator

The `@operation` and `@task` decorators are aliases that create operation/task spans for tracking specific operations.

```python
from agentops.sdk.decorators import operation

@operation
def process_data(data):
    # Process the data
    return result
```

You can also use these decorators with parameters:

```python
@operation(name="custom-operation-name", version=1)
def process_data(data):
    # Process the data
    return result
```

When used within an agent class, operations are automatically nested under the agent:

```python
from agentops.sdk.decorators import agent, operation

@agent
class MyAgent:
    @operation
    def perform_task(self, task):
        # Operation implementation
        return result
```

### `@workflow` Decorator

The `@workflow` decorator creates workflow spans for tracking workflows, which can contain multiple operations.

```python
from agentops.sdk.decorators import workflow

@workflow
def my_workflow(data):
    # Workflow implementation
    return result
```

### `@agent` Decorator

The `@agent` decorator creates an agent span for tracking agent operations.

```python
from agentops.sdk.decorators import agent

@agent
class MyAgent:
    def __init__(self, name):
        self.name = name
        
    # Agent methods here
```

### `@session` Decorator

The `@session` decorator creates a session span, which serves as the root for all other spans.

```python
from agentops.sdk.decorators import session

@session
def my_workflow():
    # Your session code here
    return result
```

## Automatic Instrumentation

AgentOps automatically instruments calls to popular LLM providers (OpenAI, Anthropic, etc.). These calls are automatically tracked as LLM spans and are properly nested within your agent and operation spans.

## Best Practices

1. **Use the Right Decorator**: Choose the appropriate decorator based on what you're instrumenting:
   - `@session` for the root of your application
   - `@agent` for agent classes
   - `@operation` for specific operations
   - `@workflow` for sequences of operations

2. **Proper Nesting**: Ensure that your spans are properly nested:
   - Session spans should be at the top
   - Agent spans should be children of session spans
   - Operation spans should be children of agent spans

3. **Meaningful Names**: Use meaningful names for your spans to make them easier to identify in the dashboard.

4. **Record Important Data**: The decorators automatically record function parameters and return values, but you can also add custom attributes to spans for additional context.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/usage/advanced-configuration.mdx

---
title: "Advanced Configuration"
mode: "wide"
---

In AgentOps fashion, you only need to add one line of "code" to your `.env` file üòä

<CodeGroup>
  ```python .env
  AGENTOPS_API_KEY=<YOUR API KEY>
  ```
</CodeGroup>

<Tip>
Find your AgentOps API Key in your Settings > [Projects & API Keys](https://app.agentops.ai/settings/projects) page.
</Tip>

#### Optional settings:

```python .env
# The AgentOps API endpoint. Defaults to https://api.agentops.ai
AGENTOPS_API_ENDPOINT=https://api.agentops.ai
# Logging level. <DEBUG, INFO, CRITICAL>. Defaults to INFO
AGENTOPS_LOG_LEVEL=INFO
# Write logs to file <TRUE, FALSE>. Defaults to TRUE
AGENTOPS_LOGGING_TO_FILE=TRUE
# Whether to opt out of recording environment data. <FALSE, TRUE>. Defaults to FALSE
AGENTOPS_ENV_DATA_OPT_OUT=FALSE
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/usage/multiple-sessions.mdx

---
title: "Concurrent Traces"
description: "Managing multiple concurrent traces and sessions"
---

# Session Management in AgentOps

AgentOps supports running multiple concurrent traces (sessions) without any special mode switching or restrictions. The modern approach uses the trace-based API with `start_trace()` and `end_trace()`, while legacy session functions remain available for backwards compatibility.

## Modern Trace-Based Approach

The recommended way to manage sessions is using the trace-based API:

<CodeGroup>
```python single trace
import agentops

agentops.init()
trace_context = agentops.start_trace("my_workflow")
# Your agent logic here
agentops.end_trace(trace_context, "Success")
```

```python multiple concurrent traces
import agentops

agentops.init(auto_start_session=False)
trace_1 = agentops.start_trace("workflow_1", tags=["experiment_a"])
trace_2 = agentops.start_trace("workflow_2", tags=["experiment_b"])

# Work with both traces concurrently
agentops.end_trace(trace_1, "Success")
agentops.end_trace(trace_2, "Success")
```

```python context manager
import agentops

agentops.init(auto_start_session=False)
with agentops.start_trace("my_workflow") as trace:
    # Your agent logic here
    # Trace automatically ends when exiting the context
    pass
```
</CodeGroup>

## Legacy Session API

For backwards compatibility, the legacy session functions are still available:

<CodeGroup>
```python legacy single session
import agentops
agentops.init()
agentops.end_session(end_state='Success')
```

```python legacy multiple sessions
import agentops
agentops.init(auto_start_session=False)
session_1 = agentops.start_session(tags=["session_1"])
session_2 = agentops.start_session(tags=["session_2"])

session_1.end_session(end_state='Success')
session_2.end_session(end_state='Success')
```
</CodeGroup>

## Managing Multiple Traces

### Starting Traces
You can start multiple traces concurrently without any restrictions:

```python
agentops.init(auto_start_session=False)
trace_1 = agentops.start_trace("user_query_1")
trace_2 = agentops.start_trace("user_query_2")
trace_3 = agentops.start_trace("background_task")
```

### Ending Traces
End traces individually or all at once:

```python
# End specific trace
agentops.end_trace(trace_1, "Success")

# End all active traces
agentops.end_trace(end_state="Success")
```

### Using Decorators
The modern approach also supports decorators for automatic trace management:

```python
import agentops

@agentops.trace
def my_workflow():
    # Your agent logic here
    return "result"

@agentops.agent
class MyAgent:
    def run(self):
        # Agent logic here
        pass
```

## LLM Call Tracking

LLM calls are automatically tracked when using the modern instrumentation. No special handling is needed for multiple concurrent traces:

```python
import agentops
import openai

agentops.init()
client = openai.OpenAI()

trace_1 = agentops.start_trace("query_1")
response_1 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello from trace 1"}]
)

trace_2 = agentops.start_trace("query_2")
response_2 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello from trace 2"}]
)

agentops.end_trace(trace_1, "Success")
agentops.end_trace(trace_2, "Success")
```

## Migration from Legacy Multi-Session Mode

If you're migrating from older AgentOps versions that had multi-session mode restrictions:

1. **Remove multi-session mode checks** - These are no longer needed
2. **Update to trace-based API** - Use `start_trace()` and `end_trace()` for new code
3. **Simplify LLM tracking** - Automatic instrumentation handles LLM calls without special session assignment
4. **Use decorators** - Consider using `@trace`, `@agent`, and `@tool` decorators for cleaner code

### Examples
<CardGroup cols={2}>
	<Card title="Concurrent Traces Example" icon="scroll" iconType="solid" color="green" href="/v1/examples/multi_session">
        Create multiple concurrent traces and manage them independently
	</Card>
	<Card title="REST API" icon="computer" iconType="regular" color="purple" href="/v1/examples/fastapi">
        Create a REST server with FastAPI and manage traces per request
	</Card>
</CardGroup>


## docs/v1/usage/langchain-callback-handler.mdx

---
title: 'LangChain Callback Handler'
description: 'How to use AgentOps with LangChain'
---

By default, AgentOps is compatible with agents using LangChain with our LLM Instrumentor as long as they're using
supported models.

As an alternative to instrumenting, the LangChain Callback Handler is available.

## Constructor

- `api_key` (Optional, string): API Key for AgentOps services. If not provided, the key will be read from the `AGENTOPS_API_KEY` environment variable.
- `endpoint` (Optional, string): The endpoint for the AgentOps service. Defaults to 'https://api.agentops.ai'.
- `max_wait_time` (Optional, int): The maximum time to wait in milliseconds before flushing the queue. Defaults to 30,000 (30 seconds).
- `max_queue_size` (Optional, int): The maximum size of the event queue. Defaults to 100.
- `tags` (Optional, List[string]): Tags for the sessions for grouping or sorting (e.g., ["GPT-4"]).

## Usage

### Install Dependencies
<CodeGroup>
```bash pip 
pip install agentops[langchain]
```
```bash poetry
poetry add agentops[langchain]
```
</CodeGroup>

### Disable Instrumentation

The Handler and our automatic instrumentation both accomplish the same tasks. To use the Handler, first disable instrumentation.

When calling `.init()`, pass in the proper parameter.
`agentops.init(instrument_llm_calls=False)`

If you are building an Agent framework or other SDK and adding support for AgentOps, you cannot guarantee that your end user will
properly include `instrument_llm_calls=False`.

### Implement Callback Handler

Initialize the handler with its constructor and pass it into the callbacks array from LangChain.
```python
from agentops.integration.callbacks.langchain import LangchainCallbackHandler
ChatOpenAI(callbacks=[LangchainCallbackHandler()])
```

Example:
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from agentops import LangchainCallbackHandler

prompt = ChatPromptTemplate.from_messages(["Tell me a joke about {animal}"])

model = ChatOpenAI(callbacks=[LangchainCallbackHandler()])

chain = prompt | model
response = chain.invoke({"animal": "bears"})
```

## Why use the handler?

If your project uses LangChain for Agents, Events and Tools, it may be easier to use the callback Handler for observability.

If your project uses models with LangChain that are not yet supported by AgentOps, they can be supported by the Handler.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/concepts/core-concepts.mdx

---
title: Core Concepts
description: An in-depth exploration of the foundational elements and operational dynamics of AgentOps, highlighting its architecture and constituent components.
---

## The AgentOps SDK Client

### Principles

The AgentOps SDK works to provide as much functionality with as little developer implementation as possible. We accomplish this with a few principles.
- Auto-Instrumenting
  - After calling `agentops.init()` we automatically look for installed LLM providers and auto-instrument their calls. This allows us to capture calls between your code and the provider to collect data for your dashboard.
- Decorators
  - With [Decorators](/v1/concepts/decorators), the SDK can add tracing to your existing functions and classes to create hierarchical spans for monitoring and analysis.
- Process monitoring
  - The SDK establishes a couple process monitors that allow us to understand the state and health of your agents.

### Sessions

A **Session** encapsulates a singular execution instance of your workflow, bringing together all agents, LLMs, actions, etc., under one umbrella. Consequently, it is imperative for each event to be associated with a session. The AgentOps dashboard provides detailed insights at the session level, including costs, token counts, errors, and more.

Sessions possess the following attributes:
- **ID**: A unique identifier for the session.
- **Project ID**: Identifies the project associated with the session, determined by the API Key used.
- **Starting Timestamp**: Marks the beginning of the session.
- **Ending Timestamp**: Indicates when the session concludes.
- **End State**: Signifies the success or failure of the session.

Optionally, sessions may include:
- **End State Reason**: Explains why the session ended, whether due to an error or a user-triggered interrupt (SIGINT).
- **Tags**: Tags allow for the categorization and later retrieval of sessions.
- **Host Environment**: Automatically gathers basic information about the system on which the session ran.
- **Video**: If applicable, an optional video recording of the session.

### Session Management
AgentOps can exist in one of two states:
<CardGroup cols={2}>
	<Card title="Single Session" icon="hand-point-up" iconType="solid" color="#2bd600">
		- ‚Ä¢ Only one session exists at a time. All agent usage is synchronous.
		- ‚Ä¢ Use cases: Scripting, development, local machine use (browser extensions, web client, etc)
	</Card>
	<Card title="Concurrent Traces" icon="hand-peace" iconType="solid" color="#2bd600">
		- ‚Ä¢ REST server
		- ‚Ä¢ Asynchronous agents
		- ‚Ä¢ Multiple concurrent workflows
	</Card>
</CardGroup>

AgentOps supports both single and multiple concurrent traces (sessions) seamlessly. You can use either the modern trace-based API or the legacy session functions for backwards compatibility.

The modern approach uses `start_trace()` and `end_trace()` with automatic instrumentation, while legacy session functions remain available. Multiple concurrent traces work without any special mode switching or restrictions.
<CodeGroup>
```python single trace
import agentops
agentops.init()
trace_context = agentops.start_trace("my_workflow")
# Your agent logic here
agentops.end_trace(trace_context, "Success")
```

```python concurrent traces
import agentops
agentops.init(auto_start_session=False)
trace_1 = agentops.start_trace("workflow_1")
trace_2 = agentops.start_trace("workflow_2")

# Work with both traces concurrently
agentops.end_trace(trace_1, "Success")
agentops.end_trace(trace_2, "Success")
```

```python using decorators
import agentops

@agentops.trace
def my_workflow():
    # Your agent logic here
    pass

my_workflow()
```
</CodeGroup>

For more documentation on using multiple concurrent traces, please see [Concurrent Traces](/v1/usage/multiple-sessions) and [FastAPI Example](/v1/examples/fastapi).

### LLMs, Tools, and Operations (Spans)

Within AgentOps, **LLMs**, **Tools**, and **Operations** are categorized as **Spans**, executed by Agents. Agents primarily initiate LLM calls, potentially leading to API/Tool calls, while Operations encompass any other significant procedures, such as executing functions, taking screenshots, etc.

All spans share the following characteristics:
- **ID**: A unique identifier.
- **Session ID**: The session to which the span belongs.
- **Agent ID**: Identifies the agent responsible for the span.
- **Parameters**: The inputs provided to the span.
- **Returns**: The outputs or results of the span.
- **Starting Timestamp**: The time at which the span began.
- **Ending Timestamp**: The time at which the span concluded.

Additionally, each span type has its own specific properties:

**LLMs**:
- **Model**: The specific LLM model used.
- **Prompt Messages**: The initial prompts sent to the model.
- **Completion Messages**: The responses received from the model.
- **Prompt Tokens**: The number of tokens used in the prompts.
- **Completion Tokens**: The number of tokens in the model's responses.
- **Cost**: The cost incurred by the span.
- **Thread ID**: Associates the span with a specific thread for execution tracking.

**Tools**:
- **Logs**: Records of the tool's operation, including any outputs and errors.

**Operations**:
- **Operation Type**: Specifies the nature of the operation (e.g., function execution, screenshot).
- **Logs**: Detailed records of the operation's execution, including parameters and outcomes.

### Errors

Errors are an inevitable aspect of operational processes. AgentOps comprehensively documents errors related to Spans, providing a wealth of information for troubleshooting.

- **Error Type**: The span type (LLM, Tool, or Operation) the error is associated with.
- **Error Code**: A specific code identifying the error.
- **Details**: A detailed description of the error.
- **Logs/Stack Trace**: Logs or stack traces capturing the error context.
- **Timestamp**: The exact time the error occurred.

### Agents

An **Agent** is an autonomous entity with its own memory and capabilities. In multi-agent frameworks, agents act as team members, each with specialized skills and responsibilities, such as project management, software development, and quality assurance, coordinating and communicating to achieve collective goals.

Agents are characterized by:
- **ID**: A unique identifier for the agent.
- **Session ID**: Links the agent to its associated session.
- **Name**: A user-defined name for easy identification of the agent.

Optionally, agents may also have:
- **Logs**: Textual records of the tasks performed by the agent.

**Note**: For spans not specifically assigned to an agent, AgentOps will automatically create and assign a default agent.

### Threads

*Details coming soon.*

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/concepts/tags.mdx

---
title: Tags
description: 'Tags are a fantastic way to organize your Sessions based on data known by the developer but not by AgentOps.
Ex: `paid-user`, `v3`, `recruiter-agent`'
---



# How to use Tags

Tags belong to [Sessions](/v1/concepts/sessions) and are a fantastic way to organize your Sessions.
## On `init()`

Passing `tags` into the `init` function will assign these tags to the all subsequent Sessions.

<CodeGroup>
```python python
agentops.init(tags=['test tag'])
```
</CodeGroup>

`tags` can also be passed into the `start_session` function for better organization.

<CodeGroup>
```python python
agentops.start_session(tags=['test tag'])
```
</CodeGroup>

# Sorting by tags

## Overview
1. Select the filter selector in the top bar
2. Choose "Tags"
3. Select the tag(s) to filter by

## Session Drill-down
1. Click the session selector button
2. In the session selector, click "Tags"
3. Choose what tags to search by

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v1/concepts/sessions.mdx

---
title: Sessions
description: Detailed breakdown of initializing AgentOps and managing sessions.
---

A **Session** encapsulates a singular execution instance of your workflow, bringing together all agents, LLMs, 
actions, etc., under one umbrella. Consequently, it is imperative for each event to be associated with a session. 
The AgentOps dashboard provides detailed insights at the session level, including costs, token counts, errors, and more.

**There must be an active session in order to use AgentOps.**

---

## `Session`

### Properties

Sessions possess the following attributes:
- **ID**: A unique identifier for the session.
- **Project ID**: Identifies the project associated with the session, determined by the API Key used.
- **Starting Timestamp**: Marks the beginning of the session.
- **Ending Timestamp**: Indicates when the session concludes.
- **End State**: Signifies the success or failure of the session.

Optionally, sessions may include:
- **End State Reason**: Explains why the session ended, whether due to an error or a user-triggered interrupt (SIGINT).
- **Tags**: Tags allow for the categorization and later retrieval of sessions.
- **Host Environment**: Automatically gathers basic information about the system on which the session ran.
- **Video**: If applicable, an optional video recording of the session.


### Methods
#### `end_session`
**Params**
- **end_state** (str, enum): Success|Failure|Indeterminate
- **end_state_reason** (optional, str): additional notes on end state

**Returns** (str): Total cost of session in USD

#### `record`
**Params**
- **event** ([Event](/v1/concepts/events#event-class)): The Event to record as part of the session

#### `get_analytics`
**Returns** (dict): A dictionary containing various analytics metrics for the session.


## Starting a Session
When you call `agentops.init()`, a session is automatically started.
Calling `agentops.init(auto_start_session=False)` will initialize the AgentOps SDK but not start a session.

To start a session later, call `agentops.start_session()` [(reference)](/v1/usage/sdk-reference/#start-session)

Both `agentops.init()` and `agentops.start_session()` work as a factory pattern and return a `Session` object. The above methods can all be called on this session object.

## Ending a Session
If a process ends without any call to agentops, it will show in the dashboard as `Indeterminate`.
To end with a state, call either `agentops.end_session(...)` [(reference)](/v1/usage/sdk-reference/#end-session) if only one session is in use. Otherwise use `session.end_session(...)`.

## Inherited Sessions
When working with multiple agents running in different processes, it's possible to initialize AgentOps or start a session
with an existing session_id.

`agentops.init(inherited_session_id=<id>)`
`agentops.start_session(inherited_session_id=<id>)`

You can retrieve the current `session_id` by assigning the returned value from `init()` or `start_session()`.

<CodeGroup>

```python
import agentops
session = agentops.init()
# pass session.session_id to the other process
```

```python
# -- other process --
session_id = retrieve_session_id() # <-- your function
agentops.init(inherited_session_id=<id>)
```

</CodeGroup>

Both processes will now contribute data to the same session.

## Session Data Export
AgentOps provides REST endpoints to export your session data and statistics. These endpoints allow you to retrieve detailed information about your sessions programmatically.

### Authentication
All data export requests require a single header:
- `X-Agentops-Api-Key`: Your AgentOps API key

### Available Endpoints

#### Get Session Statistics
```http
GET /v2/sessions/<session_id>/stats
```

Returns statistics for the specified session including:
- Event counts
- Duration
- Costs
- Token usage
- Other session metrics

#### Export Complete Session Data
```http
GET /v2/sessions/<session_id>/export
```

Returns comprehensive session data including:
- Session metadata
- Statistics
- All recorded events:
  - Actions
  - LLM calls
  - Tool usage
  - Errors

### Example Usage
```python
import requests

# Your AgentOps API key
api_key = "your-api-key"
session_id = "your-session-id"

headers = {
    "X-Agentops-Api-Key": api_key
}

# Get session stats
stats_url = f"https://api.agentops.ai/v2/sessions/{session_id}/stats"
stats_response = requests.get(stats_url, headers=headers)
stats = stats_response.json()

# Export complete session data
export_url = f"https://api.agentops.ai/v2/sessions/{session_id}/export"
export_response = requests.get(export_url, headers=headers)
session_data = export_response.json()
```

## Session Analytics
You can retrieve the analytics for a session by calling `session.get_analytics()`.

The example below shows how to record events and retrieve analytics.

<CodeGroup>

```python
import agentops
session = agentops.init()
session.record(ActionEvent("llms"))
session.record(ActionEvent("tools"))
analytics = session.get_analytics()
print(analytics)
session.end_session("Success")
```

The output will look like this -

```bash
{'LLM calls': 0, 'Tool calls': 0, 'Actions': 0, 'Errors': 0, 'Duration': '0.9s', 'Cost': '0.00'}
```

</CodeGroup>

## The AgentOps SDK Client
_More info for the curious_

Under the hood, `agentops.init()` creates a `Client` object with various configuration options. Whenever you start a new session, these configuration options will automatically
be applied. You can also apply different configuration options when you start a new session by passing in a
[Config](/v1/usage/sdk-reference/#config) object.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/concepts/host-env.mdx

---
title: Host Environment Data
description: How AgentOps collects environment data
---

## What Data is Collected?

The AgentOps SDK uses `psutil` to gather certain information about the machine that it is being run on. This data
includes information about:
- CPU
- Memory
- Storage
- Operating System

## How is it used?

This data is used only for determining issues with the host environment and reporting this on your dashboard.

> Example: Your agent performed slower because of a reduction in available memory

This data is used in accordance with our [Privacy Policy](https://app.agentops.ai/privacy-policy)
and is never sold or shared.

## Opting-Out

AgentOps users can opt out of certain environment data collection by setting the following environment variable.

```
AGENTOPS_ENV_DATA_OPT_OUT=True
```

For more questions on privacy and data usage, please [send us an email](mailto:alex@agentops.ai)

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/concepts/events.mdx

---
title: "Spans"
description: "AgentOps uses spans to track different types of operations in your agent workflows."
---

## Spans Overview

In AgentOps v0.4, the concept of "Events" has been replaced with "Spans" for all operation tracking. Spans represent different types of operations and are organized hierarchically.

A span is a unit of work or operation in your agent workflow. Spans are created using decorators and are automatically nested to create a hierarchical trace of your agent's execution.

## Span Types

AgentOps supports several types of spans, each representing a different kind of operation:

### Session Spans

Session spans serve as the root for all other spans. They represent a complete execution of your agent workflow.

```python
from agentops.sdk.decorators import session

@session
def my_workflow():
    # Your session code here
    return result
```

### Agent Spans

Agent spans represent operations performed by a specific agent. They are typically children of session spans.

```python
from agentops.sdk.decorators import agent

@agent
class MyAgent:
    def __init__(self, name):
        self.name = name
```

### Operation/Task Spans

Operation spans represent specific tasks or operations performed by an agent. They are typically children of agent spans.

```python
from agentops.sdk.decorators import operation

@operation
def process_data(data):
    # Process the data
    return result
```

### Workflow Spans

Workflow spans represent a sequence of operations that form a workflow. They can contain multiple operations.

```python
from agentops.sdk.decorators import workflow

@workflow
def my_workflow(data):
    # Workflow implementation
    return result
```

### LLM Spans

LLM spans are automatically created when you make calls to supported LLM providers. They represent interactions with language models.

## Span Attributes

All spans share a set of common attributes:

| Attribute | Description |
|-----------|-------------|
| span_id | A unique identifier for the span |
| trace_id | The ID of the trace this span belongs to |
| parent_id | The ID of the parent span |
| name | The name of the span |
| kind | The kind of span (session, agent, operation, etc.) |
| start_time | When the span started |
| end_time | When the span ended |
| attributes | Custom attributes for the span |

Additionally, spans automatically capture:

- Input parameters to the decorated function
- Return values from the decorated function
- Exceptions that occur during execution

## Span Hierarchy

Spans are organized hierarchically to create a trace of your agent's execution. The typical hierarchy is:

1. Session (root)
2. Agent
3. Operation/Task
4. Nested Operations

This hierarchy allows you to visualize the flow of your agent's execution and understand how different operations relate to each other.

## Error Handling

When an exception occurs within a decorated function, it's automatically recorded in the span. This allows you to easily identify and debug errors in your agent workflow.

```python
from agentops.sdk.decorators import operation

@operation
def risky_operation():
    # This exception will be recorded in the span
    raise ValueError("Something went wrong")
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v1/concepts/decorators.mdx

---
title: "Decorators"
description: "Seemingly magic tools that can be added to functions and classes for easier instrumenting."
---

> Decorators work by wrapping functions or classes that they are placed above. You've probably seen this before.
> Using decorators allows us to add a lot of functionality to your code with minimal work on your part.
> ```python python
> @example_decorator()
> def hello_world():
>    ...
> ```

AgentOps provides a set of decorators that allow you to easily instrument your code for tracing and monitoring AI agent workflows. These decorators create spans (units of work) that are organized hierarchically to track different types of operations.

## `@session`

The `@session` decorator creates a session span, which serves as the root for all other spans. No spans can exist without a session at the top.

```python
from agentops.sdk.decorators import session

@session
def my_workflow():
    # Your session code here
    return result
```

You can also use the decorator with parameters:

```python
@session(name="custom-session-name", version=1)
def my_workflow():
    # Your session code here
    return result
```

## `@agent`

The `@agent` decorator creates an agent span for tracking agent operations. Agent spans are typically children of session spans and parents of operation spans.

```python
from agentops.sdk.decorators import agent

@agent
class MyAgent:
    def __init__(self, name):
        self.name = name
        
    # Agent methods here
```

You can also specify a custom name for the agent:

```python
@agent(name="research-assistant")
class MyAgent:
    # Agent implementation
```

## `@operation` / `@task`

The `@operation` and `@task` decorators are aliases that create operation/task spans for tracking specific operations. These spans are typically children of agent spans.

```python
from agentops.sdk.decorators import agent, operation

@agent
class MyAgent:
    @operation
    def perform_task(self, task):
        # Operation implementation
        return result
```

Operations can also be used outside of agent classes:

```python
from agentops.sdk.decorators import operation

@operation
def process_data(data):
    # Process the data
    return result
```

## `@workflow`

The `@workflow` decorator creates workflow spans for tracking workflows, which can contain multiple operations.

```python
from agentops.sdk.decorators import workflow

@workflow
def my_workflow(data):
    # Workflow implementation
    return result
```

## Nesting and Hierarchy

The decorators automatically manage the context propagation, ensuring that spans are properly nested within their parent spans. The typical hierarchy is:

1. Session (root)
2. Agent
3. Operation/Task
4. Nested Operations

Example of proper nesting:

```python
from agentops.sdk.decorators import session, agent, operation

@agent
class MyAgent:
    @operation
    def nested_operation(self, message):
        return f"Processed: {message}"
        
    @operation
    def main_operation(self):
        result = self.nested_operation("test message")
        return result

@session
def my_session():
    agent = MyAgent()
    return agent.main_operation()
    
# Run the session
result = my_session()
```

## Additional Features

The decorators provide several additional features:

1. **Input/Output Recording**: The decorators automatically record the input arguments and output results of the decorated functions.

2. **Exception Handling**: If an exception occurs within a decorated function, it's recorded in the span.

3. **Support for Different Function Types**: The decorators handle different types of functions:
   - Regular synchronous functions
   - Asynchronous functions (using `async/await`)
   - Generator functions (using `yield`)
   - Asynchronous generator functions (using `async` and `yield`)

4. **Custom Attributes**: You can add custom attributes to spans using the decorator parameters.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v1/integrations/groq.mdx

---
title: Groq
description: "AgentOps provides first class support for Groq"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Groq](https://groq.com) accelerates LLM inference with its ultra-fast Language Processing Unit (LPU) that powers Groq Cloud. Explore the [Groq docs](https://console.groq.com/docs/overview) to get started with their developer console.

## Steps to Integrate Groq with AgentOps

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Groq SDK">
    <CodeGroup>
      ```bash pip
      pip install groq
      ```
      ```bash poetry
      poetry add groq
      ```
    </CodeGroup>
  </Step>
  <Step title="Initialize AgentOps and develop with Groq">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      from groq import Groq
      import agentops

      agentops.init(<INSERT YOUR API KEY HERE>)
      client = Groq(api_key="your_api_key")
      
      # Your code here...

      agentops.end_session('Success')
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        GROQ_API_KEY=<YOUR OPENAI API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/groq/groq_session.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

<CodeGroup>

```python sync
from groq import Groq
import agentops

agentops.init(<INSERT YOUR API KEY HERE>)
client = Groq(api_key="your_api_key")

response = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Explain the importance of low latency LLMs",
        }
    ],
    model="llama3-8b-8192",
)

print(response.choices[0].message.content)
agentops.end_session('Success')
```

```python async
from groq import AsyncGroq
import agentops
import asyncio

async def main() -> None:
    agentops.init(<INSERT YOUR API KEY HERE>)
    client = AsyncGroq(api_key="your_api_key")

    response = await client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "Explain the importance of low latency LLMs",
            }
        ],
        model="llama3-8b-8192",
    )

    print(response.choices[0].message.content)
    agentops.end_session('Success')

asyncio.run(main())
```

</CodeGroup>

<Card title="Multi Agent Example" icon="bolt" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/multi_agent_groq_example.ipynb">
    This notebook demonstrates how to use AgentOps with a multi-agent system using Groq.
</Card>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/ai21.mdx

---
title: AI21
description: "Observe AI21's latest Jamba family of models with AgentOps "
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps has first party support for [AI21](https://www.ai21.com) via its Python SDK. Explore development with AI21 by visiting their [docs](https://docs.ai21.com/docs).

## Steps to integrate AI21 with AgentOps

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the AI21 SDK">
    <Note>
      We currently support v2.x.x of the AI21 SDK and plan to support v3.x.x in the future.
    </Note>
    <CodeGroup>
      ```bash pip
      pip install "ai21<3.0.0"
      ```
      ```bash poetry
      poetry add "ai21<3.0.0"
      ```
    </CodeGroup>
  </Step>
  <Step title="Add AgentOps to your code">
    <CodeTooltip/>
      <CodeGroup>
        ```python python
        from ai21 import AI21Client
        from ai21.models.chat import ChatMessage
        import agentops

        # Initialize clients
        agentops.init(<INSERT YOUR API KEY HERE>)
        client = AI21Client(api_key="your-api-key")

        # Your AI21 code here...

        agentops.end_session("Success")
        ```
      </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/ai21/ai21-session.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python sync
  from ai21 import AI21Client
  from ai21.models.chat import ChatMessage
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = AI21Client(api_key="your-api-key")

  messages = [
      ChatMessage(
          content="You are a world renowned poet in the style of Edgar Allan Poe.",
          role="system",
      ),
      ChatMessage(
          content="Write me a short poem about the AI agents co-existing within the human brain.",
          role="user",
      ),
  ]

  response = client.chat.completions.create(
      messages=messages,
      model="jamba-1.5-mini",
  )

  print(response.choices[0].message.content)
  agentops.end_session('Success')
  ```

  ```python async
  from ai21 import AsyncAI21Client
  import agentops
  import asyncio

  async def main():
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = AsyncAI21Client(api_key="your-api-key")

      messages = [
          ChatMessage(
              content="You are a world renowned poet in the style of Edgar Allan Poe.",
              role="system",
          ),
          ChatMessage(
              content="Write me a short poem about the AI agents co-existing within the human brain.",
              role="user",
          ),
      ]

      response = await client.chat.completions.create(
          messages=messages,
          model="jamba-1.5-mini",
      )

      print(response.choices[0].message.content)
      agentops.end_session('Success')

  asyncio.run(main())
  ```
</CodeGroup>

### Streaming Examples

<CodeGroup>
  ```python sync
  from ai21 import AI21Client
  from ai21.models.chat import ChatMessage
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = AI21Client(api_key="your-api-key")

  messages = [
      ChatMessage(
          content="You are a world renowned poet in the style of Edgar Allan Poe.",
          role="system",
      ),
      ChatMessage(
          content="Write me a short poem about the AI agents co-existing within the human brain.",
          role="user",
      ),
  ]

  complete_response = ""

  response = client.chat.completions.create(
      messages=messages,
      model="jamba-1.5-mini",
      stream=True,
  )

  for chunk in response:
    complete_response += str(chunk.choices[0].delta.content)

  print(complete_response)
  agentops.end_session('Success')
  ```

  ```python async
  from ai21 import AsyncAI21Client
  import agentops
  import asyncio

  async def main():
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = AsyncAI21Client(api_key="your-api-key")

      messages = [
          ChatMessage(
              content="You are a world renowned poet in the style of Edgar Allan Poe.",
              role="system",
          ),
          ChatMessage(
              content="Write me a short poem about the AI agents co-existing within the human brain.",
              role="user",
          ),
      ]

      complete_response = ""

      response = await client.chat.completions.create(
          messages=messages,
          model="jamba-1.5-mini",
          stream=True,
      )

      async for chunk in response:
        complete_response += str(chunk.choices[0].delta.content)

      print(complete_response)
      agentops.end_session('Success')

  asyncio.run(main())
  ```
</CodeGroup>

### Task-Specific Models

AI21 v2.x.x provides specialized models called Task Specific Models for specific tasks.

Here's an example using the contextual answers endpoint:

<CodeGroup>
  ```python
  from ai21 import AI21Client
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = AI21Client(api_key="your-api-key")

  CONTEXT = """
  In 2020 and 2021, enormous QE ‚Äî approximately $4.4 trillion, or 18%, of 2021 gross
  domestic product (GDP) ‚Äî and enormous fiscal stimulus (which has been and
  always will be inflationary) ‚Äî approximately $5 trillion, or 21%, of 2021 GDP
  ‚Äî stabilized markets and allowed companies to raise enormous amounts of
  capital. In addition, this infusion of capital saved many small businesses and
  put more than $2.5 trillion in the hands of consumers and almost $1 trillion into
  state and local coffers. These actions led to a rapid decline in unemployment, 
  dropping from 15% to under 4% in 20 months ‚Äî the magnitude and speed of which were both
  unprecedented. Additionally, the economy grew 7% in 2021 despite the arrival of
  the Delta and Omicron variants and the global supply chain shortages, which were
  largely fueled by the dramatic upswing in consumer spending and the shift in
  that spend from services to goods.
  """

  response = client.answer.create(
      context=CONTEXT,
      question="Did the economy shrink after the Omicron variant arrived?",
  )

  print(response.answer)
  agentops.end_session('Success')
  ```

</CodeGroup>

<Note>
  If you want to stream the response, you can use the stream=True flag. The streaming response is handled by the AI21 SDK.
</Note>

All of these examples can be found in  this [notebook](https://github.com/AgentOps-AI/agentops/blob/main/examples/ai21_examples/ai21_examples.ipynb).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script> 

## docs/v1/integrations/litellm.mdx

---
title: LiteLLM
description: "Call the latest models using the OpenAI format including:  
Llama, Mistral, Claude, Gemini, Gemma, DALL-E, Whisper"
---

import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[LiteLLM](https://www.litellm.ai) allows tracking 100+ LLMs with AgentOps. Checkout the [providers](https://docs.litellm.ai/docs/providers) supported by LiteLLM.
Visit the [LiteLLM docs](https://docs.litellm.ai/docs/) for more information.

## Using AgentOps with LiteLLM

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install LiteLLM">
    <Note>
      AgentOps supports `litellm>=1.3.1`
    </Note>
    <CodeGroup>
      ```bash pip
      pip install litellm
      ```
      ```bash poetry
      poetry add litellm
      ```
    </CodeGroup>
  </Step>
  <Step title="Initialize AgentOps and use LiteLLM in your code">
    <Warning>
      AgentOps requires a minor adjustment to how you call LiteLLM. This is because of other existing integrations that use the `completion` function.
    </Warning>
    <CodeGroup>
      ```python python
      import agentops
      import litellm

      agentops.init(<INSERT YOUR API KEY HERE>)

      # Some code here...

      response = litellm.completion(model="gpt-4o", messages=messages)
      
      # Some code here...

      agentops.end_session('Success')
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://raw.githubusercontent.com/AgentOps-AI/agentops/refs/heads/main/docs/images/external/app_screenshots/session-replay.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

An example notebook is available [here](https://github.com/AgentOps-AI/agentops/blob/main/examples/litellm_examples/litellm_example.ipynb).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/xai.mdx

---
title: xAI
description: "Observe the power of Grok and Grok Vision with AgentOps"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[xAI](https://x.ai) develops the Grok and Grok Vision models. Explore their developer docs [here](https://docs.x.ai/docs).

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install XAI libraries">
    <Note>
      xAI models can be accessed via OpenAI or Anthropic clients.
    </Note>
    To install the OpenAI client, run:
    <CodeGroup>
      ```bash pip
      pip install openai
      ```
      ```bash poetry
      poetry add openai
      ```
    </CodeGroup>
    To install the Anthropic client, run:
    <CodeGroup>
      ```bash pip
      pip install anthropic
      ```
      ```bash poetry
      poetry add anthropic
      ```
    </CodeGroup>
  </Step>
  <Step title="Add AgentOps to your code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      
      agentops.init(<INSERT YOUR API KEY HERE>)

      # Your code here...

      agentops.end_session("Success")
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Analysis">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your XAI analysis! üîç
    <Tip>
      AgentOps automatically tracks your XAI metrics and visualizations in the Dashboard
    </Tip>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
```python openai
from openai import OpenAI

XAI_API_KEY = "you xai api key"
client = OpenAI(
    api_key=XAI_API_KEY,
    base_url="https://api.x.ai/v1",
)

completion = client.chat.completions.create(
    model="grok-beta",
    messages=[
        {"role": "system", "content": "You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy."},
        {"role": "user", "content": "What is the meaning of life, the universe, and everything?"},
    ],
)

print(completion.choices[0].message)
```

```python anthropic
from anthropic import Anthropic

XAI_API_KEY = "you xai api key"
client = Anthropic(
    api_key=XAI_API_KEY,
    base_url="https://api.x.ai",
)
message = client.messages.create(
    model="grok-beta",
    max_tokens=128,
    system="You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.",
    messages=[
        {
            "role": "user",
            "content": "What is the meaning of life, the universe, and everything?",
        },
    ],
)
print(message.content)
```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/llamaindex.mdx

---
title: 'LlamaIndex'
description: 'AgentOps works seamlessly with LlamaIndex, a framework for building context-augmented generative AI applications with LLMs.'
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[LlamaIndex](https://www.llamaindex.ai/) is a framework for building context-augmented generative AI applications with LLMs. AgentOps provides observability into your LlamaIndex applications through automatic instrumentation.

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip 
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install LlamaIndex AgentOps Instrumentation">
		<CodeGroup>
			```bash pip 
			pip install llama-index-instrumentation-agentops
			```
			```bash poetry
			poetry add llama-index-instrumentation-agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Add 2 lines of code">
		<CodeTooltip/>
			<CodeGroup>
				```python python
				from llama_index.core import set_global_handler

				# NOTE: Feel free to set your AgentOps environment variables (e.g., 'AGENTOPS_API_KEY')
				# as outlined in the AgentOps documentation, or pass the equivalent keyword arguments
				# anticipated by AgentOps' AOClient as **eval_params in set_global_handler.

				set_global_handler("agentops")
				```
			</CodeGroup>
		<EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
	<Step title="Run your LlamaIndex application">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your LlamaIndex application! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip> 
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Usage Pattern

Here's a simple example of how to use AgentOps with LlamaIndex:

```python
from llama_index.core import set_global_handler
import llama_index.core

# Set the global handler to AgentOps
set_global_handler("agentops")

# Your LlamaIndex application code here
# AgentOps will automatically track LLM calls and other operations
```

## Additional Resources

For more detailed information about LlamaIndex's observability features and AgentOps integration, check out the [LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/module_guides/observability/#agentops).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/smolagents.mdx

---
title: smolagents
description: "Track HuggingFace's smolagents with AgentOps seamlessly"
---

import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[smolagents](https://github.com/huggingface/smolagents) is a framework for building and running AI agents from [HuggingFace](https://huggingface.co/).
Checkout their [docs](https://huggingface.co/docs/smolagents/index) to get started with creating your own agents.

## Integrating AgentOps with smolagents

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install smolagents">
    <CodeGroup>
      ```bash pip
      pip install smolagents
      ```
      ```bash poetry
      poetry add smolagents
      ```
    </CodeGroup>
  </Step>
  <Step title="Initialize AgentOps and use LiteLLM in your code">
    <Note>
      AgentOps tracks calls from smolagents via LiteLLM currently.
    </Note>
    <CodeGroup>
      ```python python
      import agentops
      from smolagents import LiteLLMModel

      agentops.init(<INSERT YOUR API KEY HERE>)
      model = LiteLLMModel(<model_name>)
      
      # Your code here...

      agentops.end_session('Success')
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://raw.githubusercontent.com/AgentOps-AI/agentops/refs/heads/main/docs/images/external/app_screenshots/session-replay.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

You can refer to the following examples -

- [Text to SQL](https://github.com/AgentOps-AI/agentops/blob/main/examples/smolagents_examples/text_to_sql.ipynb)
- [Multi-Agent System](https://github.com/AgentOps-AI/agentops/blob/main/examples/smolagents_examples/multi_smolagents_system.ipynb)

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/mistral.mdx

---
title: Mistral
description: "AgentOps provides first class support for Mistral AI's models"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Mistral](https://mistral.ai) publishes open-weight AI models that can be used for a variety of tasks. To develop with Mistral, visit their developer docs [here](https://docs.mistral.ai).

## Steps to Integrate Mistral with AgentOps

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Mistral SDK">
    <CodeGroup>
      ```bash pip
      pip install mistralai
      ```
      ```bash poetry
      poetry add mistralai
      ```
    </CodeGroup>
  </Step>
  <Step title="Initialize AgentOps and develop with Mistral">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      from mistralai import Mistral
      import agentops

      agentops.init(<INSERT YOUR API KEY HERE>)
      client = Mistral(api_key="your_api_key")
      
      # Your code here...

      agentops.end_session('Success')
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        MISTRAL_API_KEY=<YOUR MISTRAL API KEY>
        ```
      </CodeGroup>
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/mistral/mistral_session.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

A notebook demonstrating how to use AgentOps with Mistral can be found [here](https://github.com/AgentOps-AI/agentops/blob/main/examples/mistral_examples/mistral_example.ipynb).

<CodeGroup>

```python sync
from mistralai import Mistral
import agentops

agentops.init(<INSERT YOUR API KEY HERE>)
client = Mistral(api_key="your_api_key")

response = client.chat.complete(
    model="mistral-small-latest",
    messages=[
        {
            "role": "user",
            "content": "Explain the history of the French Revolution."
        }
    ],
)

print(response.choices[0].message.content)
agentops.end_session('Success')
```

```python async
import asyncio
from mistralai import Mistral
import agentops

async def main():
    agentops.init(<INSERT YOUR API KEY HERE>)
    client = Mistral(api_key="your_api_key")

    response = await client.chat.complete_async(
        model="mistral-small-latest",
        messages=[
            {
                "role": "user",
                "content": "Write a short summary about the poem La Belle Dame sans Merci.",
            },
        ],
    )

    print(response.choices[0].message.content)
    agentops.end_session('Success')

asyncio.run(main())
```

</CodeGroup>

### Streaming Examples

<CodeGroup>

```python sync
from mistralai import Mistral
import agentops

agentops.init(<INSERT YOUR API KEY HERE>)
client = Mistral(api_key="your_api_key")

complete_response = ""

response = client.chat.stream(
    model="mistral-small-latest",
    messages=[
        {
            "role": "user",
            "content": "Who was Joan of Arc?"
        }
    ],
)

for chunk in response:
    if chunk.data.choices[0].finish_reason == "stop":
        print(complete_response)
    else:
        complete_response += chunk.data.choices[0].delta.content

agentops.end_session('Success')
```

```python async
import asyncio
from mistralai import Mistral
import agentops

async def main():
    agentops.init(<INSERT YOUR API KEY HERE>)
    client = Mistral(api_key="your_api_key")

    complete_response = ""

    response = await client.chat.stream_async(
        model="mistral-small-latest",
        messages=[
            {
                "role": "user",
                "content": "Write a short summary about the poem La Belle Dame sans Merci.",
            },
        ],
    )

    async for chunk in response:
        if chunk.data.choices[0].finish_reason == "stop":
            print(complete_response)
        else:
            complete_response += chunk.data.choices[0].delta.content

    agentops.end_session('Success')

asyncio.run(main())
```

</CodeGroup>


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/rest.mdx

---
title: "REST API"
description: "Direct integration with AgentOps API"
---

The AgentOps REST API allows you to integrate agent monitoring directly into your application without using our Python SDK. This is useful for:
- Non-Python applications
- Custom integrations
- Direct API access

For a complete API reference, you can view our [OpenAPI specification](https://api.agentops.ai/v2/openapi.yaml).

## Authentication

The AgentOps API uses a two-step authentication process:
1. API Key: Used to create sessions and get JWT tokens
2. JWT: Used for all operations within a session

### Initial Authentication

When you create a session, you'll use your API key and receive a JWT token in response:

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/create_session \
  -H "Content-Type: application/json" \
  -H "X-Agentops-Api-Key: your_api_key" \
  -d '{
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "init_timestamp": "2024-03-14T12:00:00Z"
  }'
```

```json Success Response
{
  "status": "success",
  "jwt": "eyJhbGciOiJIUzI1NiIs...",
  "session_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

```bash Error Response
HTTP/1.1 401 Unauthorized
{
  "error": "Invalid API key",
  "message": "Please check your API key and try again"
}
```
</CodeGroup>

### Using JWT Tokens

Use the JWT token in the Authorization header for all subsequent requests:

<CodeGroup>
```bash Example Request
curl -X POST https://api.agentops.ai/create_events \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..." \
  -H "Content-Type: application/json" \
  -d '{
    "events": [{
      "type": "llm",
      "init_timestamp": "2024-03-14T12:01:00Z"
    }]
  }'
```

```bash Error Response
HTTP/1.1 401 Unauthorized
{
  "error": "Invalid or expired JWT",
  "message": "Please reauthorize using /reauthorize_jwt"
}
```
</CodeGroup>

### Refreshing Tokens

JWTs expire after 24 hours. When a token expires, use your API key to get a new one:

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/reauthorize_jwt \
  -H "Content-Type: application/json" \
  -H "X-Agentops-Api-Key: your_api_key" \
  -d '{
    "session_id": "550e8400-e29b-41d4-a716-446655440000"
  }'
```

```json Success Response
{
  "status": "success",
  "jwt": "eyJhbGciOiJIUzI1NiIs..."
}
```
</CodeGroup>

## Session Management

Sessions require a unique identifier that you generate client-side. While any unique string will work, we recommend using UUIDs for consistency. Here's how to generate one in Python:

<CodeGroup>
```python Python
import uuid
session_id = str(uuid.uuid4())
# "550e8400-e29b-41d4-a716-446655440000"
```
</CodeGroup>

### Create Session

Start a new monitoring session using your generated session ID:

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/create_session \
  -H "Content-Type: application/json" \
  -H "X-Agentops-Api-Key: your_api_key" \
  -d '{
    "session": {
      "id": "550e8400-e29b-41d4-a716-446655440000",
      "init_timestamp": "2024-03-14T12:00:00Z",
      "tags": ["production", "customer-service"],
      "host_env": {
        "OS": {
          "OS": "Windows",
          "OS Release": "11",
          "OS Version": "10.0.22631"
        },
        "CPU": {
          "CPU Usage": "5.9%",
          "Total cores": 12
        },
        "RAM": {
          "Used": "14.49 GB",
          "Total": "31.75 GB"
        },
        "SDK": {
          "Python Version": "3.12.0",
          "System Packages": {
            "agentops": "0.3.17",
            "openai": "1.2.3"
          }
        }
      }
    }
  }'
```

```bash Request
POST https://api.agentops.ai/v2/create_session
Content-Type: application/json
X-Agentops-Api-Key: your_api_key

{
  "session": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "init_timestamp": "2024-03-14T12:00:00Z",
    "tags": ["production", "customer-service"],
    "host_env": {
      "OS": {
        "OS": "Windows",
        "OS Release": "11",
        "OS Version": "10.0.22631"
      },
      "CPU": {
        "CPU Usage": "5.9%",
        "Total cores": 12
      },
      "RAM": {
        "Used": "14.49 GB",
        "Total": "31.75 GB"
      },
      "SDK": {
        "Python Version": "3.12.0",
        "System Packages": {
          "agentops": "0.3.17",
          "openai": "1.2.3"
        }
      }
    }
  }
}
```
</CodeGroup>

### Update Session

Update an existing session (e.g., when it ends):

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/update_session \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_jwt_token" \
  -d '{
    "session": {
      "id": "550e8400-e29b-41d4-a716-446655440000",
      "end_timestamp": "2024-03-14T12:05:00Z",
      "end_state": "Success",
      "end_state_reason": "Task successfully completed",
      "tags": ["production", "updated-tag"]
    }
  }'
```

```bash Request
POST https://api.agentops.ai/v2/update_session
Content-Type: application/json
Authorization: Bearer your_jwt_token

{
  "session": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "end_timestamp": "2024-03-14T12:05:00Z",
    "end_state": "Success",
    "end_state_reason": "Task successfully completed",
    "tags": ["production", "updated-tag"]
  }
}
```
</CodeGroup>

## Event Tracking

### Create Events

Track LLM calls, tool usage, or other events:

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/create_events \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_jwt_token" \
  -d '{
    "events": [
      {
        "type": "llm",
        "init_timestamp": "2024-03-14T12:01:00Z",
        "end_timestamp": "2024-03-14T12:01:02Z",
        "model": "gpt-4",
        "prompt": [
          {"role": "system", "content": "You are a helpful assistant"},
          {"role": "user", "content": "Analyze this data..."}
        ],
        "completion": {
          "role": "assistant",
          "content": "Based on the data..."
        },
        "prompt_tokens": 150,
        "completion_tokens": 80
      },
      {
        "type": "tool",
        "name": "database_query",
        "init_timestamp": "2024-03-14T12:01:03Z",
        "end_timestamp": "2024-03-14T12:01:04Z",
        "input": "SELECT * FROM users",
        "output": "Retrieved 5 users"
      }
    ]
  }'
```

```bash Request
POST https://api.agentops.ai/v2/create_events
Content-Type: application/json
Authorization: Bearer your_jwt_token

{
  "events": [
    {
      "type": "llm",
      "init_timestamp": "2024-03-14T12:01:00Z",
      "end_timestamp": "2024-03-14T12:01:02Z",
      "model": "gpt-4",
      "prompt": [
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Analyze this data..."}
      ],
      "completion": {
        "role": "assistant",
        "content": "Based on the data..."
      },
      "prompt_tokens": 150,
      "completion_tokens": 80
    },
    {
      "type": "tool",
      "name": "database_query",
      "init_timestamp": "2024-03-14T12:01:03Z",
      "end_timestamp": "2024-03-14T12:01:04Z",
      "input": "SELECT * FROM users",
      "output": "Retrieved 5 users"
    }
  ]
}
```
</CodeGroup>

### Update Events

Update existing events (e.g., adding completion information):

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/update_events \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_jwt_token" \
  -d '{
    "events": [
      {
        "event_id": "event-id-123",
        "end_timestamp": "2024-03-14T12:01:02Z",
        "completion": "Updated completion text",
        "completion_tokens": 100
      }
    ]
  }'
```

```bash Request
POST https://api.agentops.ai/v2/update_events
Content-Type: application/json
Authorization: Bearer your_jwt_token

{
  "events": [
    {
      "event_id": "event-id-123",
      "end_timestamp": "2024-03-14T12:01:02Z",
      "completion": "Updated completion text",
      "completion_tokens": 100
    }
  ]
}
```
</CodeGroup>

## Agent Management

### Create Agent

Register a new agent in a session:

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/v2/create_agent \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_jwt_token" \
  -d '{
    "id": "agent-123",
    "name": "Research Assistant"
  }'
```

```bash Request
POST https://api.agentops.ai/v2/create_agent
Content-Type: application/json
Authorization: Bearer your_jwt_token

{
  "id": "agent-123",
  "name": "Research Assistant"
}
```
</CodeGroup>

## Example Integration

Here's a complete example using Python's requests library:

<CodeGroup>
```python Complete Example
import requests
import uuid
from datetime import datetime, timezone

# Configuration
API_KEY = "your_api_key"
BASE_URL = "https://api.agentops.ai"

# Create session
session_id = str(uuid.uuid4())
response = requests.post(
    f"{BASE_URL}/v2/create_session",
    headers={"X-Agentops-Api-Key": API_KEY},
    json={
        "session": {
            "id": session_id,
            "init_timestamp": datetime.now(timezone.utc).isoformat(),
            "tags": ["example"]
        }
    }
)
jwt_token = response.json()["jwt"]

# Track LLM call
requests.post(
    f"{BASE_URL}/v2/create_events",
    headers={"Authorization": f"Bearer {jwt_token}"},
    json={
        "events": [{
            "type": "llm",
            "init_timestamp": datetime.now(timezone.utc).isoformat(),
            "model": "gpt-4",
            "prompt": "Hello, world!",
            "completion": "Hi there!",
            "prompt_tokens": 3,
            "completion_tokens": 2
        }]
    }
)

# End session
requests.post(
    f"{BASE_URL}/v2/update_session",
    headers={"Authorization": f"Bearer {jwt_token}"},
    json={
        "session": {
            "id": session_id,
            "end_timestamp": datetime.now(timezone.utc).isoformat(),
            "end_state": "completed"
        }
    }
)
```
</CodeGroup>


## docs/v1/integrations/openai.mdx

---
title: OpenAI
description: "AgentOps provides first class support for OpenAI's GPT family of models"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[OpenAI](https://www.openai.com) is a leading provider of AI tools and services.
Explore the [OpenAI API](https://platform.openai.com/) for more information.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/qlotcZSh5_0"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the OpenAI SDK">
    <Note>
      `openai<1.0.0` has limited support while `openai>=1.0.0` is continuously supported.
    </Note>
    <CodeGroup>
      ```bash pip
      pip install openai
      ```
      ```bash poetry
      poetry add openai
      ```
    </CodeGroup>

    To install `openai<1.0.0`, use the following:
    <CodeGroup>
      ```bash pip
      pip install "openai<1.0.0"
      ```
      ```bash poetry
      poetry add "openai<1.0.0"
      ```
    </CodeGroup>
  </Step>
  <Step title="Add 3 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      from openai import OpenAI
      
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = OpenAI()
      ...
      # End of program (e.g. main.py)
      agentops.end_session("Success")
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        OPENAI_API_KEY=<YOUR OPENAI API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://raw.githubusercontent.com/AgentOps-AI/agentops/refs/heads/main/docs/images/external/app_screenshots/session-replay.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python sync
  from openai import OpenAI
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = OpenAI()

  response = client.chat.completions.create(
      model="gpt-4o-mini",
      messages=[{
          "role": "user",
          "content": "Write a haiku about AI and humans working together"
      }]
  )

  print(response.choices[0].message.content)
  agentops.end_session('Success')
  ```

  ```python async
  from openai import AsyncOpenAI
  import agentops
  import asyncio

  async def main():
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = AsyncOpenAI()

      response = await client.chat.completions.create(
          model="gpt-4o-mini",
          messages=[{
              "role": "user",
              "content": "Write a haiku about AI and humans working together"
          }]
      )

      print(response.choices[0].message.content)
      agentops.end_session('Success')

  asyncio.run(main())
  ```

</CodeGroup>

### Streaming examples

<CodeGroup>
  ```python sync
  from openai import OpenAI
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = OpenAI()

  stream = client.chat.completions.create(
      model="gpt-4o-mini",
      stream=True,
      messages=[{
            "role": "user",
            "content": "Write a haiku about AI and humans working together"
      }],
  )

  for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")

  agentops.end_session('Success')
  ```

  ```python async
  from openai import AsyncOpenAI
  import agentops
  import asyncio

  async def main():
    agentops.init(<INSERT YOUR API KEY HERE>)
    client = AsyncOpenAI()

    stream = await client.chat.completions.create(
        model="gpt-4o-mini",
        stream=True,
        messages=[{
            "role": "user",
            "content": "Write a haiku about AI and humans working together"
        }],
    )

    async for chunk in stream:
        print(chunk.choices[0].delta.content or "", end="")
    
    agentops.end_session('Success')

  asyncio.run(main())
  ```

### Assistants example

You can find the example in the [Assistants](/v1/examples/openai_assistants) section.

</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/agentssdk.mdx

---
title: 'OpenAI Agents SDK'
description: 'AgentOps and OpenAI Agents SDK integration for powerful multi-agent workflow monitoring.'
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) to bookmark on GitHub, save for later üñáÔ∏è)</Check>

[OpenAI Agents SDK](https://github.com/openai/agentsdk_prototype) is a lightweight yet powerful framework for building multi-agent workflows. The SDK provides a comprehensive set of tools for creating, managing, and monitoring agent-based applications.

{/* <Frame type="glass" caption="OpenAI Agents Tracing UI">
  <img height="400" src="/images/openai-agents-tracing-ui.png" />
</Frame> */}

## Core Concepts

- **Agents**: LLMs configured with instructions, tools, guardrails, and handoffs
- **Handoffs**: Allow agents to transfer control to other agents for specific tasks
- **Guardrails**: Configurable safety checks for input and output validation
- **Tracing**: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install OpenAI Agents SDK">
    <CodeGroup>
      ```bash pip 
      pip install openai-agents
      ```
      ```bash poetry
      poetry add openai-agents
      ```
    </CodeGroup>
    <Tip>
      This will be updated to a PyPI link when the package is officially released.
    </Tip>
  </Step>
  <Step title="Add 2 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      agentops.init(<INSERT YOUR API KEY HERE>)
      ```
    </CodeGroup>
    <EnvTooltip />
    <CodeGroup>
      ```python .env
      AGENTOPS_API_KEY=<YOUR API KEY>
      OPENAI_API_KEY=<YOUR OPENAI API KEY>
      ```
    </CodeGroup>
    Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your agents">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agents! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip> 
    <div/>{/* Intentionally blank div for newline */}
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
    </Frame>
  </Step>
</Steps>

## Hello World Example

```python
from agents import Agent, Runner
import agentops

# Initialize AgentOps
agentops.init()

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Output:
# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

## Handoffs Example

```python
from agents import Agent, Runner
import asyncio
import agentops

# Initialize AgentOps
agentops.init()

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, ¬øc√≥mo est√°s?")
    print(result.final_output)
    # ¬°Hola! Estoy bien, gracias por preguntar. ¬øY t√∫, c√≥mo est√°s?


if __name__ == "__main__":
    asyncio.run(main())
```

## Functions Example

```python
import asyncio
from agents import Agent, Runner, function_tool
import agentops

# Initialize AgentOps
agentops.init()

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Hello world",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
```

## The Agent Loop

When you call `Runner.run()`, the SDK runs a loop until it gets a final output:

1. The LLM is called using the model and settings on the agent, along with the message history.
2. The LLM returns a response, which may include tool calls.
3. If the response has a final output, the loop ends and returns it.
4. If the response has a handoff, the agent is set to the new agent and the loop continues from step 1.
5. Tool calls are processed (if any) and tool response messages are appended. Then the loop continues from step 1.

You can use the `max_turns` parameter to limit the number of loop executions.

## Final Output

Final output is the last thing the agent produces in the loop:

- If you set an `output_type` on the agent, the final output is when the LLM returns something of that type using structured outputs.
- If there's no `output_type` (i.e., plain text responses), then the first LLM response without any tool calls or handoffs is considered the final output.

<CardGroup cols={3}>
    <Card title="Basic Agent" icon="robot" href="https://github.com/openai/agentsdk_prototype/tree/main/examples/basic" />
    <Card title="Multi-Agent" icon="users" href="https://github.com/openai/agentsdk_prototype/tree/main/examples/multi_agent" />
    <Card title="Tool Usage" icon="toolbox" href="https://github.com/openai/agentsdk_prototype/tree/main/examples/tools" />
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script> 

## docs/v1/integrations/llama_stack.mdx

---
title: 'Llama Stack'
description: 'Llama Stack is a framework from Meta AI for building Agentic applications.'
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps integrates with Llama Stack via its python [client](https://github.com/meta-llama/llama-stack-client-python) to provide observability into applications that leverage it.

Llama Stack has comprehensive [documentation](https://llama-stack.readthedocs.io/) available as well as a great [quickstart](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) guide. You can use this guide to setup the Llama Stack server and client or alternatively use our Docker [compose](https://github.com/AgentOps-AI/agentops/blob/main/examples/llama_stack_client_examples/docker-compose.yml) file.

<iframe
  width="560"
  height="315"
	src="https://www.youtube.com/embed/OZgTN4SdQ2Y"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

## Adding AgentOps to Llama Stack applications

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip 
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install the Llama Stack Client">
		<CodeGroup>
			```bash pip
			pip install llama-stack-client
			```
			```bash poetry
			poetry add llama-stack-client
			```
		</CodeGroup>
	</Step>
	<Step title="Add 3 lines of code">
		<CodeTooltip/>
		<span className="api-key-container">
			<CodeGroup>
				```python python
				import agentops
				agentops.init(<INSERT YOUR API KEY HERE>)
				```
			</CodeGroup>
		</span>
		<EnvTooltip />
    <span className="api-key-container">
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
    </span>
	</Step>
	<Step title="Run your ü¶ôü•û application">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your waterfall! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip> 
	</Step>
</Steps>

## Examples

An example notebook is available [here](https://github.com/AgentOps-AI/agentops/blob/main/examples/llama_stack_client_examples/notebook.ipynb) to showcase how to use the Llama Stack client with AgentOps.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/link_to_api_button.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/autogen.mdx

---
title: AG2
description: "Track your AG2 agents with AgentOps"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps and AG2 (Formerly AutoGen) teamed up to make monitoring AG2 agents dead simple. AG2 has comprehensive [documentation](https://docs.ag2.ai/docs/Home) available as well as a great [quickstart](https://docs.ag2.ai/docs/Getting-Started).

## Adding AgentOps to AG2 agents

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install AG2">
		<CodeGroup>
			```bash pip
			pip install ag2
			```
			```bash poetry
			poetry add ag2
			```
		</CodeGroup>
	</Step>
	<Step title="Add 3 lines of code">
		<CodeTooltip/>
			<CodeGroup>
				```python python
				import agentops
				agentops.init(<INSERT YOUR API KEY HERE>)
				...
				# MUST END SESSION at end of program (e.g. main.py)
				agentops.end_session("Success") # Success|Fail|Indeterminate
				```
			</CodeGroup>
		<EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
	<Step title="Run your agent">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Autogen Agent! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip>
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/haystack.mdx

---
title: Haystack
description: "Monitor your Haystack agents with AgentOps"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps makes monitoring your Haystack agents seamless. Haystack, much like AutoGen, is a flexible framework that supports multiple models. For detailed installation instructions and feature overviews, refer to the [Haystack documentation](https://docs.haystack.deepset.ai/docs/installation).

## Integrating AgentOps with Haystack

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install Haystack">
    <Warning>
      We currently only support Haystack 2.x.
    </Warning>
    <CodeGroup>
      ```bash pip
      pip install haystack-ai
      ```
      ```bash poetry
      poetry add haystack-ai
      ```
    </CodeGroup>
  </Step>
  <Step title="Configure your environment">
    <EnvTooltip />
    <CodeGroup>
      ```python .env
      AGENTOPS_API_KEY=<YOUR API KEY>
      ```
    </CodeGroup>
    Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Add 3 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      agentops.init(<INSERT YOUR API KEY HERE>)
      ...
      # End of program (e.g. main.py)
      agentops.end_session("Success") # Success|Fail|Indeterminate
      ```
    </CodeGroup>
  </Step>
  <Step title="Run your agent">
    Execute your script and then visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to monitor your Haystack agent in action. üïµÔ∏è
    <Tip>
      After execution, AgentOps outputs a clickable URL in the console that links directly to your session in the Dashboard.
    </Tip>
    <div/>{/* Intentionally blank div for newline */}
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

You can refer to the following examples -

- [Philosopher Agent](https://github.com/AgentOps-AI/agentops/blob/main/examples/haystack_examples/haystack_anthropic_example.ipynb)
- [Mathematician Agent](https://github.com/AgentOps-AI/agentops/blob/main/examples/haystack_examples/haystack_openai_example.ipynb)


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/swarmzero.mdx

---
title: SwarmZero
description: "Build powerful web search agents with SwarmZero and AgentOps monitoring"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps has first party support for [SwarmZero](https://swarmzero.ai) via its Python SDK. Explore development with SwarmZero by visiting their [docs](https://docs.swarmzero.ai).

## Steps to integrate SwarmZero with AgentOps

<Steps>
  <Step title="Install SwarmZero">
    <CodeGroup>
      ```bash pip 
      pip install swarmzero
      ```
      ```bash poetry
      poetry add swarmzero
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install additional dependencies">
    <CodeGroup>
      ```bash pip
      pip install python-dotenv tavily-python
      ```
      ```bash poetry
      poetry add python-dotenv tavily-python
      ```
    </CodeGroup>
  </Step>
  <Step title="Add AgentOps to your code">
    <CodeTooltip/>
      <CodeGroup>
        ```python python
        import os
        import agentops
        from dotenv import load_dotenv
        from swarmzero import Agent
        from tavily import TavilyClient

        # Load environment variables
        load_dotenv()

        # Initialize clients
        agentops.init(os.getenv("AGENTOPS_API_KEY"))
        tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

        # Your SwarmZero agent code here...
        ```
      </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        TAVILY_API_KEY=<YOUR API KEY>
        OPENAI_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/swarmzero/swarmzero_session.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Example

<CodeGroup>
  ```python web_search_agent.py
  import os
  import agentops
  from dotenv import load_dotenv
  from swarmzero import Agent
  from tavily import TavilyClient

  load_dotenv()
  agentops.init(os.getenv("AGENTOPS_API_KEY"))
  tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

  async def web_search(query: str) -> dict:
      response = tavily_client.search(query)
      results = []
      for result in response["results"][:3]:
          results.append({"title": result["title"], "url": result["url"], "content": result["content"]})
      return results

  async def extract_from_urls(urls: list[str]) -> dict:
      response = tavily_client.extract(urls=urls)

      if response["failed_results"]:
          print(f"Failed to extract from {response['failed_results']}")

      results = []
      for result in response["results"]:
          results.append({"url": result["url"], "raw_content": result["raw_content"]})

      return results

  my_agent = Agent(
      name="workflow-assistant",
      functions=[
          web_search,
          extract_from_urls,
      ],
      config_path="./swarmzero_config.toml", # see https://github.com/swarmzero/swarmzero/blob/main/swarmzero_config_example.toml
      instruction="You are a helpful assistant that can search the web and extract information from a given URL.",
      # chat_only_mode=True  # remove comment only if using `my_agent.chat()`
  )

  my_agent.run()  # see agent API at localhost:8000/docs

  """
  # chat directly without starting the agent's server
  import asyncio
  
  response = asyncio.run(my_agent.chat(prompt="what is Decentralized-AI about about?"))
  print(response)
  """

  ```
</CodeGroup>

### Using the Agent

Once your agent is running, you can interact with it using HTTP requests:

<CodeGroup>
  ```bash curl
  curl -X 'POST' \
    'http://localhost:8000/api/v1/chat' \
    -H 'accept: application/json' \
    -H 'Content-Type: multipart/form-data' \
    -F 'user_id=test_user' \
    -F 'session_id=test_web_search_agent' \
    -F 'chat_data={"messages":[{"role":"user","content":"what is Decentralized-AI about about?"}]}'
  ```
</CodeGroup>

This example can be found in this [notebook](https://github.com/AgentOps-AI/agentops/blob/main/examples/swarmzero_examples/web_search_agent.ipynb)
This full code for this example can be found in this [repository](https://github.com/swarmzero/examples/tree/main/agents/web_search_agent).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/langchain.mdx

---
title: LangChain
description: "AgentOps provides first class support for LangChain applications"
---

import EnvTooltip from '/snippets/add-env-tooltip.mdx'

AgentOps works seamlessly with applications built using LangChain.

## Adding AgentOps to LangChain applications

<Steps>
	<Step title="Install the AgentOps SDK and the additional LangChain dependency">
		<CodeGroup>
			```bash pip
			pip install agentops
			pip install agentops[langchain]
			```
			```bash poetry
			poetry add agentops
			poetry add agentops[langchain]
			```
		</CodeGroup>
		<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) on GitHub while you're at it (you may be our <span id="stars-text">3,000th</span> üòä)</Check>
	</Step>
	<Step title="Set up your import statements">
		Import the following LangChain and AgentOps dependencies
		<CodeGroup>
			```python python
			import os
			from langchain.chat_models import ChatOpenAI
			from langchain.agents import initialize_agent, AgentType
			from agentops.integration.callbacks.langchain import LangchainCallbackHandler
			```
		</CodeGroup>
	</Step>
	<Step title="Set up your LangChain handler to make the calls">
		<Tip>
			Note that you don't need to set up a separate agentops.init() call, as the LangChain callback handler will automatically initialize the AgentOps client for you.
		</Tip>
		Set up your LangChain agent with the AgentOps callback handler, and AgentOps will automatically record your LangChain sessions.
			<CodeGroup>
				```python python
				handler = LangchainCallbackHandler(api_key=AGENTOPS_API_KEY, tags=['LangChain Example'])



				llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
					callbacks=[handler],
					model='gpt-3.5-turbo')

				agent = initialize_agent(tools,
					llm,
					agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
					verbose=True,
					callbacks=[handler], # You must pass in a callback handler to record your agent
					handle_parsing_errors=True)
				```
			</CodeGroup>
			<EnvTooltip />
			<CodeGroup>
				```python .env
				AGENTOPS_API_KEY=<YOUR API KEY>
				```
			</CodeGroup>
			Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
	<Step title="Run your agent">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your LangChain Agent! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable URL to the console linking directly to your session in the Dashboard
		</Tip>
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Full Examples

<CodeGroup>
	```python python
	import os
	from langchain.chat_models import ChatOpenAI
	from langchain.agents import initialize_agent, AgentType
	from agentops.integration.callbacks.langchain import LangchainCallbackHandler

	handler = LangchainCallbackHandler(api_key=AGENTOPS_API_KEY, tags=['LangChain Example'])

	llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
		callbacks=[handler],
		model='gpt-3.5-turbo')

	agent = initialize_agent(tools,
		llm,
		agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
		verbose=True,
		callbacks=[handler], # You must pass in a callback handler to record your agent
		handle_parsing_errors=True)
	```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/cohere.mdx

---
title: Cohere
description: "AgentOps provides first class support for Cohere"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Cohere](https://cohere.com) models can now be tracked with AgentOps. Explore the [Cohere API](https://docs.cohere.com) for more information.

<Note>
  Cohere's [LLM University](https://cohere.com/llmu) is a great resource for learning about LLMs with a lot of hands on examples.
</Note>

<Card title="Cohere" icon="hill-rockslide" href="https://www.loom.com/share/679d65cc06f148299e8dffc459d053ab">
    First class support for Command-R-Plus
    ![thumbnail](https://cdn.loom.com/sessions/thumbnails/679d65cc06f148299e8dffc459d053ab-with-play.gif)
</Card>

# Steps to integrate Cohere with AgentOps
<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Cohere SDK">
    <Note>
      `cohere>=5.4.0` is currently supported.
    </Note>
    <CodeGroup>
      ```bash pip
      pip install cohere
      ```
      ```bash poetry
      poetry add cohere
      ```
    </CodeGroup>
  </Step>
	<Step title="Add 3 lines of code">
    <CodeTooltip/>
		<CodeGroup>
			```python python
			import agentops
			agentops.init(<INSERT YOUR API KEY HERE>)
      co = cohere.Client()
			...
			# End of program (e.g. main.py)
			agentops.end_session("Success") # Success|Fail|Indeterminate
			```
		</CodeGroup>
    <Warning>
      Requires `cohere>=5.4.0`
    </Warning>
		<EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
  <Step title="Run your Agent">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip> 
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python chat()
  import cohere
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)

  co = cohere.Client()

  chat = co.chat(
      message="Is it pronounced ceaux-hear or co-hehray?"
  )

  print(chat)

  agentops.end_session('Success')
  ```

  ```python chat_stream()
  import cohere
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)

  co = cohere.Client()

  stream = co.chat_stream(
      message="Write me a haiku about the synergies between Cohere and AgentOps"
  )

  for event in stream:
      if event.event_type == "text-generation":
          print(event.text, end='')

  agentops.end_session('Success')
  ```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/taskweaver.mdx

---
title: TaskWeaver
description: "First class support for Microsoft TaskWeaver"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[TaskWeaver](https://microsoft.github.io/TaskWeaver/) is a code-first agent framework for seamlessly planning and executing data analytics tasks. Explore TaskWeaver's comprehensive [documentation](https://microsoft.github.io/TaskWeaver/docs/overview) for more information.

## Steps to integrate TaskWeaver with AgentOps

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install TaskWeaver and configure your project directory">
    Follow the instructions on the TaskWeaver [Quick Start](https://microsoft.github.io/TaskWeaver/docs/quickstart) section in the documentation.
  </Step>
  <Step title="Import the TaskWeaver handler from AgentOps and initialize the AgentOps client">
    AgentOps uses the TaskWeaver handler to handle its events. Additionally, AgentOps tracks the LLM calls made by TaskWeaver via its inbuilt module. These are used together to observe everything in the TaskWeaver session.
    <Note>
      TaskWeaver provides different [usage options](https://microsoft.github.io/TaskWeaver/docs/usage). By default, it's used as a CLI app via the `taskweaver` command but can also be used as a library.
    </Note>
    <CodeGroup>
      ```python python
      import agentops
      from agentops.partners.taskweaver_event_handler import TaskWeaverEventHandler
      from taskweaver.app.app import TaskWeaverApp
      
      agentops.init(<INSERT YOUR API KEY HERE>)

      # Your TaskWeaver code here

      agentops.end_session("Success")
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your TaskWeaver Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/microsoft/taskweaver_session.png?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

This is a basic example of how to use TaskWeaver as a library and observe it with AgentOps.

Ensure you have configured your project directory in the TaskWeaverApp by following the documentation mentioned in step 2.

<Warning>
  When registering the TaskWeaverEventHandler, you should either use the `session.event_emitter.register` method or pass the handler object to the `event_handler` parameter in the `send_message` method. If you use both, the events will be recorded twice, resulting in a "stuttering" effect in the recorded messages.
</Warning>

<CodeGroup>
```python python
import agentops

from taskweaver.app.app import TaskWeaverApp
from agentops.partners.taskweaver_event_handler import TaskWeaverEventHandler

agentops.init(<INSERT YOUR API KEY HERE>)

app_dir = "<path to your configured project directory>"
app = TaskWeaverApp(app_dir=app_dir)
session = app.get_session()
handler = TaskWeaverEventHandler()
session.event_emitter.register(handler)

user_query = "Hello, what are the capabilities of TaskWeaver?"
response_round = session.send_message(user_query)

user_query = "Write me a simple calculator program in python"
response_round = session.send_message(user_query)

agentops.end_session("Success")
```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/camel.mdx

---
title: 'Camel AI'
description: 'Track and analyze CAMEL agents with AgentOps'
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[CAMEL-AI](https://www.camel-ai.org/) is the first large language model (LLM) multi-agent framework and an open-source community dedicated to finding the scaling law of agents. Checkout their comprehensive documentation [here](https://docs.camel-ai.org/).

## Adding AgentOps to CAMEL agents

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install CAMEL-AI with all dependencies">
		<CodeGroup>
			```bash pip
			pip install "camel-ai[all]==0.2.11"
			```
			```bash poetry
			poetry add "camel-ai[all]==0.2.11"
			```
		</CodeGroup>
	</Step>
	<Step title="Add AgentOps code to your code">
		<CodeTooltip/>
			<CodeGroup>
				```python python
				import agentops
				agentops.init(<INSERT YOUR API KEY HERE>)

				# your code here

				agentops.end_session("Success") # Success|Fail|Indeterminate
				```
			</CodeGroup>
		<EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
	<Step title="Run your agent">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your CAMEL Agent! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip>
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Full Examples
### Single Agent Example with Tools

Here's a simple example of tracking a single CAMEL agent with tools using AgentOps:

```python
import agentops
import os
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Initialize AgentOps
agentops.init(os.getenv("AGENTOPS_API_KEY"))

# Import toolkits after AgentOps init for tracking
from camel.toolkits import SearchToolkit

# Set up the agent with search tools
sys_msg = BaseMessage.make_assistant_message(
    role_name='Tools calling operator',
    content='You are a helpful assistant'
)

# Configure tools and model
tools = [*SearchToolkit().get_tools()]
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
)

# Create the agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

# Run the agent
user_msg = 'What is CAMEL-AI.org?'
response = camel_agent.step(user_msg)
print(response)

# End the session
agentops.end_session("Success")
```

### Multi-Agent Example

Check out the example notebook [here](https://github.com/AgentOps-AI/agentops/tree/main/examples/camel_examples/camelai-multi-agent-example.ipynb) to see how to track multi-agent setups.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/gemini.mdx

---
title: Gemini
description: "Explore Google DeepMind's Gemini with observation via AgentOps"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Gemini (Google Generative AI)](https://ai.google.dev/gemini-api/docs/quickstart) is a leading provider of AI tools and services.
Explore the [Gemini API](https://ai.google.dev/docs) for more information.

<Note>
  `google-generativeai>=0.1.0` is currently supported.
</Note>

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Gemini SDK">
    <Note>
      `google-generativeai>=0.1.0` is required for Gemini integration.
    </Note>
    <CodeGroup>
      ```bash pip
      pip install google-generativeai
      ```
      ```bash poetry
      poetry add google-generativeai
      ```
    </CodeGroup>
  </Step>
  <Step title="Add 3 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import google.generativeai as genai
      import agentops
      
      agentops.init(<INSERT YOUR API KEY HERE>)
      model = genai.GenerativeModel("gemini-1.5-flash")
      ...
      # End of program (e.g. main.py)
      agentops.end_session("Success") # Success|Fail|Indeterminate
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        GEMINI_API_KEY=<YOUR GEMINI API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python sync
  import google.generativeai as genai
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  model = genai.GenerativeModel("gemini-1.5-flash")

  response = model.generate_content(
      "Write a haiku about AI and humans working together"
  )

  print(response.text)
  agentops.end_session('Success')
  ```

  ```python stream
  import google.generativeai as genai
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  model = genai.GenerativeModel("gemini-1.5-flash")

  response = model.generate_content(
      "Write a haiku about AI and humans working together",
      stream=True
  )

  for chunk in response:
      print(chunk.text, end="")

  agentops.end_session('Success')
  ```
</CodeGroup>

You can find more examples in the [Gemini Examples](/v1/examples/gemini_examples) section.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/ollama.mdx

---
title: Ollama
description: "AgentOps provides first class support for Ollama"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Ollama](https://ollama.com) is a lightweight, open-source tool for running and managing LLM models. Track your Ollama model calls with AgentOps.

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops ollama
      ```
      ```bash poetry
      poetry add agentops ollama
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Ollama SDK">
    <CodeGroup>
      ```bash pip
      pip install ollama
      ```
      ```bash poetry
      poetry add ollama
      ```
    </CodeGroup>
  </Step>
  <Step title="Add 3 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      import ollama
      
      agentops.init(<INSERT YOUR API KEY HERE>)
      agentops.start_session()

      ollama.pull("<MODEL NAME>")

      response = ollama.chat(model='mistral',
        messages=[{
            'role': 'user',
            'content': 'What are the benefits of using AgentOps for monitoring LLMs?',
        }]
      )
      print(response['message']['content'])
      ...
      # End of program (e.g. main.py)
      agentops.end_session("Success")
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        # Alternatively, you can set the API key as an environment variable
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python basic completion
  import ollama
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)

  ollama.pull("<MODEL NAME>")
  response = ollama.chat(
      model="<MODEL NAME>",
      max_tokens=1024,
      messages=[{
          "role": "user",
          "content": "Write a haiku about AI and humans working together"
      }]
  )

  print(response['message']['content'])
  agentops.end_session('Success')
  ```

  ```python streaming
  import agentops
  import ollama

  async def main():
      agentops.init(<INSERT YOUR API KEY HERE>)
      ollama.pull("<MODEL NAME>")

      stream = ollama.chat(
        model="<MODEL NAME>",
          messages=[{
              'role': 'user',
              'content': 'Write a haiku about monitoring AI agents',
          }], 
          stream=True
      )

      for chunk in stream:
          print(chunk['message']['content'], end='')

      agentops.end_session('Success')
  ```

  ```python conversation
  import ollama
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  ollama.pull("<MODEL NAME>")

  messages = [
    {
        'role': 'user',
        'content': 'What is AgentOps?'
    },
    {
        'role': 'assistant',
        'content': 'AgentOps is a monitoring and observability platform for LLM applications.'
    },
    {
        'role': 'user',
        'content': 'Can you give me 3 key features?'
    }
]

  response = ollama.chat(
      model="<MODEL NAME>",
      messages=messages
  )
  print(response['message']['content'])   
  agentops.end_session('Success')
  ```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v1/integrations/anthropic.mdx

---
title: Anthropic
description: "AgentOps provides first class support for Anthropic's latest models"
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[Anthropic AI](https://www.anthropic.com) is a leading provider of AI tools and services, including the Claude, Haiku, and Sonnet series of models.
Explore the [Anthropic API](https://docs.anthropic.com/en/home) for more information.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/Yx2Crpbms7I"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

<Steps>
  <Step title="Install the AgentOps SDK">
    <CodeGroup>
      ```bash pip 
      pip install agentops
      ```
      ```bash poetry
      poetry add agentops
      ```
    </CodeGroup>
  </Step>
  <Step title="Install the Anthropic SDK">
    <Note>
      `anthropic>=0.32.0` is currently supported with additional support for the Computer Use tool.
    </Note>
    <CodeGroup>
      ```bash pip
      pip install anthropic
      ```
      ```bash poetry
      poetry add anthropic
      ```
    </CodeGroup>
  </Step>
  <Step title="Add 3 lines of code">
    <CodeTooltip/>
    <CodeGroup>
      ```python python
      import agentops
      from anthropic import Anthropic
      
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = Anthropic()
      ...
      # End of program (e.g. main.py)
      agentops.end_session("Success") # Success|Fail|Indeterminate
      ```
    </CodeGroup>
    <EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ANTHROPIC_API_KEY=<YOUR ANTHROPIC API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
  </Step>
  <Step title="Run your Agent">
    Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Agent! üïµÔ∏è
    <Tip>
      After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
    </Tip>
    <div/>
    <Frame type="glass" caption="Clickable link to session">
      <img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
    </Frame>
  </Step>
</Steps>

## Full Examples

<CodeGroup>
  ```python sync
  from anthropic import Anthropic
  import agentops

  agentops.init(<INSERT YOUR API KEY HERE>)
  client = Anthropic()

  message = client.messages.create(
      model="claude-3-opus-20240229",
      max_tokens=1024,
      messages=[{
          "role": "user",
          "content": "Write a haiku about AI and humans working together"
      }]
  )

  print(message.content)
  agentops.end_session('Success')
  ```

  ```python async
  from anthropic import AsyncAnthropic
  import agentops
  import asyncio

  async def main():
      agentops.init(<INSERT YOUR API KEY HERE>)
      client = AsyncAnthropic()

      message = await client.messages.create(
          model="claude-3-opus-20240229",
          max_tokens=1024,
          messages=[{
              "role": "user",
              "content": "Write a haiku about AI and humans working together"
          }]
      )

      print(message.content)
      agentops.end_session('Success')

  asyncio.run(main())
  ```
</CodeGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/multion.mdx

---
title: MultiOn
description: "AgentOps and MultiOn have collaborated to simplify the monitoring of autonomous browser agents."
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[MultiOn](https://www.multion.ai) is a platform for building autonomous agents.

## Adding AgentOps to MultiOn agents

<Steps>
	<Step title="Install MultiOn">
		<CodeGroup>
			```bash pip
			pip install -U multion
			```
			```bash poetry
			poetry add multion
			```
		</CodeGroup>
	</Step>
	<Step title="Create your API keys">
		* Create an [AgentOps API key](https://app.agentops.ai/settings/projects)
		* Create a [MultiOn API key](https://app.multion.ai/api-keys)
	</Step>
	<Step title="Create a MultiOn client with an AgentOps key">
		<CodeTooltip/>
			<CodeGroup>
				```python python
				import multion
				from multion.client import MultiOn
				
				# Initialize MultiOn with both a MultiOn API key and an AgentOps API key
				multion = MultiOn(
					api_key=<INSERT YOUR MULTION API KEY HERE>,
					agentops_api_key=<INSERT YOUR AGENTOPS API KEY HERE>
				)

				browse_response = multion.browse(
					cmd="what three things do i get with agentops",
					url="https://www.agentops.ai/",
					max_steps=4,
					include_screenshot=True
				)

					print(browse_response.message)
					```
			</CodeGroup>		
		<EnvTooltip />
	</Step>
	<Step title="Run your agent">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your MultiOn Agent! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip>
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/agentops-multion-browse.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Full Examples

You can view several example notebooks [here](https://github.com/AgentOps-AI/agentops/tree/main/examples/multion):
* [Sample web browser agent using only MultiOn](https://github.com/AgentOps-AI/agentops/blob/main/examples/multion_examples/Autonomous_web_browsing.ipynb)
* [Combining a MultiOn agent with other LLMs calls](https://github.com/AgentOps-AI/agentops/blob/main/examples/multion_examples/Autonomous_web_browsing.ipynb)
* [Step-by-step tracking of browse events](https://github.com/AgentOps-AI/agentops/blob/main/examples/multion_examples/Step_by_step_web_browsing.ipynb)
* [Collecting data from a web page using the retrieve API](https://github.com/AgentOps-AI/agentops/blob/main/examples/multion_examples/Webpage_data_retrieval.ipynb)

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/integrations/crewai.mdx

---
title: 'CrewAI'
description: 'AgentOps and CrewAI teamed up to make monitoring Crew agents dead simple.'
---

import CodeTooltip from '/snippets/add-code-tooltip.mdx'
import EnvTooltip from '/snippets/add-env-tooltip.mdx'

[CrewAI](https://www.crewai.com/) is a framework for easily building multi-agent applications. Crew has comprehensive [documentation](https://docs.crewai.com) available as well as a great [quickstart](https://docs.crewai.com/how-to/Creating-a-Crew-and-kick-it-off/) guide.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/sF_bgRuRa3s"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip 
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install Crew with AgentOps">
		<CodeGroup>
			```bash pip 
			pip install 'crewai[agentops]'
			```
			```bash poetry
			poetry add 'crewai[agentops]'
			```
		</CodeGroup>
	</Step>
	<Step title="Add 2 lines of code">
		<CodeTooltip/>
			<CodeGroup>
				```python python
				import agentops
				agentops.init(<INSERT YOUR API KEY HERE>)
				```
			</CodeGroup>
		<EnvTooltip />
      <CodeGroup>
        ```python .env
        AGENTOPS_API_KEY=<YOUR API KEY>
        ```
      </CodeGroup>
      Read more about environment variables in [Advanced Configuration](/v1/usage/advanced-configuration)
	</Step>
	<Step title="Run your crew">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Crew! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip> 
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Special Considerations with Crew
The Crew framework is capable of determining when all tasks have been accomplished and to halt execution. AgentOps will automatically end your active session
when this determination is made. If you don't want your AgentOps session to end at this time, add an optional parameter to your `agentops.init()` call.

```python
agentops.init(skip_auto_end_session=True)
```

## Crew + AgentOps Examples

<CardGroup cols={3}>
    <Card title="Job Posting" icon="briefcase" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/job-posting" />
	<Card title="Instagram Post" icon="instagram" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/instagram_post" />
    <Card title="Markdown Validator" icon="markdown" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/markdown_validator" />
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v1/examples/examples.mdx

---
title: 'Examples'
description: 'Sample code that you can run!'
mode: "wide"
---

## Explore our example code to see AgentOps in action!

### Agent Examples

<CardGroup cols={2}>
  <Card title="Simple Agent" icon="microchip-ai" iconType="solid" href="/v1/examples/simple_agent">
    Jupyter Notebook with sample code that you can run! Start here!
  </Card>

  <Card title="Multi-Agent" icon="people-arrows" iconType="duotone" href="/v1/examples/multi_agent">
    Jupyter Notebook with a simple multi-agent design
  </Card>

  <Card title="Concurrent Traces" icon="computer" href="/v1/examples/multi_session">
    Manage multiple concurrent traces and sessions
  </Card>

  <Card title="OpenAI Assistants" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI Assistants" />} iconType="image" href="/v1/integrations/openai" href="/v1/examples/openai_assistants">
    Observe OpenAI Assistants
  </Card>
</CardGroup>

### Integration Examples

<CardGroup cols={2}>
  <Card title="AI21" icon="brain" href="/v1/integrations/ai21">
    Integration with AI21's language models
  </Card>

  <Card title="Anthropic" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/anthropic/anthropic_icon_slate.png?raw=true" alt="Anthropic" />} iconType="image" href="/v1/integrations/anthropic">
    Track observations from Claude, Haiku and Sonnet series of models
  </Card>

  <Card title="AG2" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/autogen/ag2.svg?raw=true" alt="AG2"/>} iconType="image" href="/v1/integrations/autogen">
    AG2 (Formerly AutoGen) multi-agent conversible workflow with tool usage
  </Card>

  <Card title="CamelAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/camel/camel.png?raw=true" alt="CamelAI" />} iconType="image" href="/v1/examples/camel">
    Track and analyze CAMEL agents
  </Card>

  <Card title="Cohere" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/cohere/cohere-logo.svg?raw=true" alt="Cohere" />} iconType="image" href="/v1/integrations/cohere">
    First class support for Command-R-Plus and chat streaming
  </Card>

  <Card title="Crew.ai" icon="ship" href="/v1/integrations/crewai">
    CrewAI multi-agent framework with AgentOps support
  </Card>

  <Card title="Groq" icon="bolt" href="/v1/integrations/groq">
    Ultra-fast LLM inference with Groq Cloud
  </Card>

  <Card title="Gemini" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/deepmind/gemini-logo.png?raw=true" alt="Gemini" />} iconType="image" href="/v1/integrations/gemini">
    Explore Google DeepMind's Gemini with observation via AgentOps
  </Card>

  <Card title="Haystack" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/haystack/haystack-logo.png?raw=true" alt="Haystack" />} iconType="image" href="/v1/integrations/haystack">
    Monitor your Haystack agents with AgentOps
  </Card>

  <Card title="LangChain" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/langchain/langchain-logo.svg?raw=true" alt="LangChain" />} iconType="image" href="/v1/examples/langchain">
    Jupyter Notebook with a sample LangChain integration
  </Card>

  <Card title="Llama Stack" icon="pancakes" href="/v1/integrations/llama_stack">
    Create an agent to search the web using Brave Search and find the winner of NBA western conference semifinals 2014
  </Card>

  <Card title="LiteLLM" icon="train-tram" href="/v1/integrations/litellm">
    Unified interface for multiple LLM providers
  </Card>

  <Card title="Mistral" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/mistral/mistral-ai-icon.png?raw=true" alt="Mistral" />} iconType="image" href="/v1/integrations/mistral">
    Support for Mistral AI's open-weight models
  </Card>

  <Card title="MultiOn" icon="atom" href="/v1/examples/multion">
    Create an autonomous browser agent capable of navigating the web and extracting information
  </Card>

  <Card title="Ollama" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ollama/ollama-icon.png?raw=true" alt="Ollama" />} iconType="image" href="/v1/examples/ollama">
    Simple Ollama integration with AgentOps
  </Card>

  <Card title="OpenAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI" />} iconType="image" href="/v1/integrations/openai">
    First class support for GPT family of models
  </Card>

  <Card title="REST API" icon="bolt-lightning" href="/v1/examples/restapi">
    Create a REST server that performs and observes agent tasks
  </Card>

  <Card title="smolagents" icon={<img src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/huggingface/hf-logo.png?raw=true" alt="smolagents" />} iconType="image" iconType="solid" href="/v1/integrations/smolagents">
    Track HuggingFace's smolagents with AgentOps seamlessly
  </Card>

  <Card title="SwarmZero" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/swarmzero/swarmzero_logo.png?raw=true" alt="SwarmZero" />} iconType="image" href="/v1/integrations/swarmzero">
    SwarmZero multi-agent framework for AI Agents and AI Swarms with AgentOps support
  </Card>

  <Card title="TaskWeaver" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/microsoft/microsoft_logo.svg?raw=true" alt="TaskWeaver" />} iconType="image" href="/v1/integrations/taskweaver">
    First class support for Microsoft TaskWeaver
  </Card>

  <Card title="xAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/xai/xai-logo.png?raw=true" alt="xAI" />} iconType="image" href="/v1/integrations/xai">
    Observe the power of Grok and Grok Vision with AgentOps
  </Card>
</CardGroup>

## Video Guides

<CardGroup cols={2}>

  <Card title="AgentOps x CrewAI" icon="film" iconType="solid">
    Learn how to monitor CrewAI applications with AgentOps
    <iframe
      width="280"
      height="158"
      src="https://www.youtube.com/embed/sF_bgRuRa3s"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen
    ></iframe>
  </Card>

  <Card title="AgentOps x Anthropic" icon="film" iconType="solid">
    Learn how to monitor applications that use the Anthropic SDK directly with AgentOps
    <iframe
      width="280"
      height="158"
      src="https://www.youtube.com/embed/Yx2Crpbms7I"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen
    ></iframe>
  </Card>

  <Card title="AgentOps x OpenAI" icon="film" iconType="solid">
    Learn how to monitor applications that use the OpenAI SDK directly with AgentOps
    <iframe
      width="280"
      height="158"
      src="https://www.youtube.com/embed/R0K9Qi2dBwM"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen
    ></iframe>
  </Card>

  <Card title="AgentOps x Llama Stack" icon="film" iconType="solid">
    Learn how to monitor Llama Stack applications that use the llama-stack-client-python SDK with AgentOps
    <iframe
      width="280"
      height="158"
      src="https://www.youtube.com/embed/OZgTN4SdQ2Y"
      title="YouTube video player"
      frameBorder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowFullScreen
    ></iframe>
  </Card>

  <Card title="Simple Agent" icon="person-carry-box" iconType="solid" href="https://www.loom.com/share/0e0d2986f3d644a58d1e186dc81cd8b1">
    Add AgentOps to a simple chat completion
    ![thumbnail](https://cdn.loom.com/sessions/thumbnails/0e0d2986f3d644a58d1e186dc81cd8b1-with-play.gif)
  </Card>

  <Card title="Multi-Agent" icon="people-arrows" iconType="duotone" href="https://www.loom.com/share/2a62fd70efe8423fb495946b09c2d8ac">
    Jupyter Notebook with a sample LangChain integration
    ![thumbnail](https://cdn.loom.com/sessions/thumbnails/2a62fd70efe8423fb495946b09c2d8ac-with-play.gif)
  </Card>

  <Card title="Crew.ai" icon="ship" href="https://www.loom.com/share/cfcaaef8d4a14cc7a974843bda1076bf">
    CrewAI multi-agent framework with AgentOps support
    ![thumbnail](https://cdn.loom.com/sessions/thumbnails/cfcaaef8d4a14cc7a974843bda1076bf-1713568618224-with-play.gif)
  </Card>

  <Card title="Cohere" icon="hill-rockslide" href="https://www.loom.com/share/679d65cc06f148299e8dffc459d053ab">
    First class support for Command-R-Plus
    ![thumbnail](https://cdn.loom.com/sessions/thumbnails/679d65cc06f148299e8dffc459d053ab-with-play.gif)
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js" />

<script type="module" src="/scripts/scroll-img-fadein-animation.js" />

<script type="module" src="/scripts/button_heartbeat_animation.js" />

<script type="module" src="/scripts/adjust_api_dynamically.js" />


## docs/v1/examples/openai_assistants.mdx

---
title: 'OpenAI Assistants'
description: 'Learn how to use OpenAI Assistants API with AgentOps'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/openai_assistants_example.ipynb'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/openai_examples/openai_assistants_example.ipynb  */}

# Assistants API Overview with AgentOps

This notebook has been adapted from this OpenAI Cookbook [example](https://cookbook.openai.com/examples/assistants_api_overview_python).

The new [Assistants API](https://platform.openai.com/docs/assistants/overview) is a stateful evolution of our [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) meant to simplify the creation of assistant-like experiences, and enable developer access to powerful tools like Code Interpreter and Retrieval.

![Assistants API Diagram](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_diagram.png)

## Chat Completions API vs Assistants API

The primitives of the **Chat Completions API** are `Messages`, on which you perform a `Completion` with a `Model` (`gpt-3.5-turbo`, `gpt-4`, etc). It is lightweight and powerful, but inherently stateless, which means you have to manage conversation state, tool definitions, retrieval documents, and code execution manually.

The primitives of the **Assistants API** are

- `Assistants`, which encapsulate a base model, instructions, tools, and (context) documents,
- `Threads`, which represent the state of a conversation, and
- `Runs`, which power the execution of an `Assistant` on a `Thread`, including textual responses and multi-step tool use.

We'll take a look at how these can be used to create powerful, stateful experiences.


## Setup

> **Note**
> The Assistants API is currently in beta so the latest [Python SDK](https://github.com/openai/openai-python) is needed (`1.58.1` at time of writing) for this example.



```python
%pip install -U openai
%pip install -U agentops
%pip install -U python-dotenv
```

### Pretty Printing Helper



```python
import json

def show_json(obj):
    display(json.loads(obj.model_dump_json()))
```

## Complete Example with Assistants API


### Assistants


The easiest way to get started with the Assistants API is through the [Assistants Playground](https://platform.openai.com/playground).


![Assistants Playground](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_assistants_playground.png)


Let's begin by creating an assistant! We'll create a Math Tutor just like in our [docs](https://platform.openai.com/docs/assistants/overview).


![Creating New Assistant](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_new_assistant.png)


You can view Assistants you've created in the [Assistants Dashboard](https://platform.openai.com/assistants).


![Assistants Dashboard](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_assistants_dashboard.png)


You can also create Assistants directly through the Assistants API. But we need to have the AgentOps and OpenAI API keys first.

You can get your OpenAI API key from the [OpenAI Dashboard](https://platform.openai.com/api-keys).

To obtain the AgentOps API key, signup for an account on [AgentOps](https://agentops.ai/) and create a project. After creating the project, you can now create an API key in the [Project Settings](https://app.agentops.ai/settings/projects).

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!

Now we are all set! Let's import the necessary libraries and initialize the AgentOps and OpenAI clients.


```python
from openai import OpenAI
import agentops
from dotenv import load_dotenv
import os

load_dotenv()
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
```


```python
agentops.init(api_key=AGENTOPS_API_KEY, tags=["openai", "beta-assistants"])
client = OpenAI(api_key=OPENAI_API_KEY)
```

Next, we'll create an Assistant which will be our Math Tutor.


```python
assistant = client.beta.assistants.create(
    name="Math Tutor",
    instructions="You are a personal math tutor. Answer questions briefly, in a sentence or less.",
    model="gpt-4o-mini",
)
show_json(assistant)
```

Regardless of whether you create your Assistant through the Dashboard or with the API, you'll want to keep track of the Assistant ID. This is how you'll refer to your Assistant throughout Threads and Runs.


Next, we'll create a new Thread and add a Message to it. This will hold the state of our conversation, so we don't have re-send the entire message history each time.


### Threads


Create a new thread:



```python
thread = client.beta.threads.create()
show_json(thread)
```

Then add the Message to the thread:



```python
message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="I need to solve the equation `3x + 11 = 14`. Can you help me?",
)
show_json(message)
```

> **Note**
> Even though you're no longer sending the entire history each time, you will still be charged for the tokens of the entire conversation history with each Run.


### Runs

Notice how the Thread we created is **not** associated with the Assistant we created earlier! Threads exist independently from Assistants, which may be different from what you'd expect if you've used ChatGPT (where a thread is tied to a model/GPT).

To get a completion from an Assistant for a given Thread, we must create a Run. Creating a Run will indicate to an Assistant it should look at the messages in the Thread and take action: either by adding a single response, or using tools.

> **Note**
> Runs are a key difference between the Assistants API and Chat Completions API. While in Chat Completions the model will only ever respond with a single message, in the Assistants API a Run may result in an Assistant using one or multiple tools, and potentially adding multiple messages to the Thread.

To get our Assistant to respond to the user, let's create the Run. As mentioned earlier, you must specify _both_ the Assistant and the Thread.



```python
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
)
show_json(run)
```

Unlike creating a completion in the Chat Completions API, **creating a Run is an asynchronous operation**. It will return immediately with the Run's metadata, which includes a `status` that will initially be set to `queued`. The `status` will be updated as the Assistant performs operations (like using tools and adding messages).

To know when the Assistant has completed processing, we can poll the Run in a loop. (Support for streaming is coming soon!) While here we are only checking for a `queued` or `in_progress` status, in practice a Run may undergo a [variety of status changes](https://platform.openai.com/docs/api-reference/runs/object#runs/object-status) which you can choose to surface to the user. (These are called Steps, and will be covered later.)



```python
import time

def wait_on_run(run, thread):
    while run.status == "queued" or run.status == "in_progress":
        run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id,
        )
        time.sleep(0.5)
    return run
```


```python
run = wait_on_run(run, thread)
show_json(run)
```

### Messages


Now that the Run has completed, we can list the Messages in the Thread to see what got added by the Assistant.



```python
messages = client.beta.threads.messages.list(thread_id=thread.id)
show_json(messages)
```

As you can see, Messages are ordered in reverse-chronological order ‚Äì this was done so the most recent results are always on the first `page` (since results can be paginated). Do keep a look out for this, since this is the opposite order to messages in the Chat Completions API.


Let's ask our Assistant to explain the result a bit further!



```python
# Create a message to append to our thread
message = client.beta.threads.messages.create(
    thread_id=thread.id, role="user", content="Could you explain this to me?"
)

# Execute our run
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
)

# Wait for completion
wait_on_run(run, thread)

# Retrieve all the messages added after our last user message
messages = client.beta.threads.messages.list(
    thread_id=thread.id, order="asc", after=message.id
)
show_json(messages)
```

This may feel like a lot of steps to get a response back, especially for this simple example. However, you'll soon see how we can add very powerful functionality to our Assistant without changing much code at all!


### Example


Let's take a look at how we could potentially put all of this together. Below is all the code you need to use an Assistant you've created.

Since we've already created our Math Assistant, I've saved its ID in `MATH_ASSISTANT_ID`. I then defined two functions:

- `submit_message`: create a Message on a Thread, then start (and return) a new Run
- `get_response`: returns the list of Messages in a Thread



```python
from openai import OpenAI

MATH_ASSISTANT_ID = assistant.id  # or a hard-coded ID like "asst-..."

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))

def submit_message(assistant_id, thread, user_message):
    client.beta.threads.messages.create(
        thread_id=thread.id, role="user", content=user_message
    )
    return client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant_id,
    )


def get_response(thread):
    return client.beta.threads.messages.list(thread_id=thread.id, order="asc")
```

I've also defined a `create_thread_and_run` function that I can re-use (which is actually almost identical to the [`client.beta.threads.create_and_run`](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun) compound function in our API ;) ). Finally, we can submit our mock user requests each to a new Thread.

Notice how all of these API calls are asynchronous operations; this means we actually get async behavior in our code without the use of async libraries! (e.g. `asyncio`)



```python
def create_thread_and_run(user_input):
    thread = client.beta.threads.create()
    run = submit_message(MATH_ASSISTANT_ID, thread, user_input)
    return thread, run


# Emulating concurrent user requests
thread1, run1 = create_thread_and_run(
    "I need to solve the equation `3x + 11 = 14`. Can you help me?"
)
thread2, run2 = create_thread_and_run("Could you explain linear algebra to me?")
thread3, run3 = create_thread_and_run("I don't like math. What can I do?")

# Now all Runs are executing...
```

Once all Runs are going, we can wait on each and get the responses.



```python
import time

# Pretty printing helper
def pretty_print(messages):
    print("# Messages")
    for m in messages:
        print(f"{m.role}: {m.content[0].text.value}")
    print()


# Waiting in a loop
def wait_on_run(run, thread):
    while run.status == "queued" or run.status == "in_progress":
        run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id,
        )
        time.sleep(0.5)
    return run


# Wait for Run 1
run1 = wait_on_run(run1, thread1)
pretty_print(get_response(thread1))

# Wait for Run 2
run2 = wait_on_run(run2, thread2)
pretty_print(get_response(thread2))

# Wait for Run 3
run3 = wait_on_run(run3, thread3)
pretty_print(get_response(thread3))

# Thank our assistant on Thread 3 :)
run4 = submit_message(MATH_ASSISTANT_ID, thread3, "Thank you!")
run4 = wait_on_run(run4, thread3)
pretty_print(get_response(thread3))
```

Et voil√†!

You may have noticed that this code is not actually specific to our math Assistant at all... this code will work for any new Assistant you create simply by changing the Assistant ID! That is the power of the Assistants API.


## Tools

A key feature of the Assistants API is the ability to equip our Assistants with Tools, like Code Interpreter, Retrieval, and custom Functions. Let's take a look at each.

### Code Interpreter

Let's equip our Math Tutor with the [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter) tool, which we can do from the Dashboard...


![Enabling code interpreter](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_enable_code_interpreter.png)


...or the API, using the Assistant ID.



```python
assistant = client.beta.assistants.update(
    MATH_ASSISTANT_ID,
    tools=[{"type": "code_interpreter"}],
)
show_json(assistant)
```

Now, let's ask the Assistant to use its new tool.



```python
thread, run = create_thread_and_run(
    "Generate the first 20 fibbonaci numbers with code."
)
run = wait_on_run(run, thread)
pretty_print(get_response(thread))
```

And that's it! The Assistant used Code Interpreter in the background, and gave us a final response.

For some use cases this may be enough ‚Äì¬†however, if we want more details on what precisely an Assistant is doing we can take a look at a Run's Steps.

### Steps


A Run is composed of one or more Steps. Like a Run, each Step has a `status` that you can query. This is useful for surfacing the progress of a Step to a user (e.g. a spinner while the Assistant is writing code or performing retrieval).



```python
run_steps = client.beta.threads.runs.steps.list(
    thread_id=thread.id, run_id=run.id, order="asc"
)
```

Let's take a look at each Step's `step_details`.



```python
for step in run_steps.data:
    step_details = step.step_details
    print(json.dumps(show_json(step_details), indent=4))
```

We can see the `step_details` for two Steps:

1. `tool_calls` (plural, since it could be more than one in a single Step)
2. `message_creation`

The first Step is a `tool_calls`, specifically using the `code_interpreter` which contains:

- `input`, which was the Python code generated before the tool was called, and
- `output`, which was the result of running the Code Interpreter.

The second Step is a `message_creation`, which contains the `message` that was added to the Thread to communicate the results to the user.


### Retrieval

Another powerful tool in the Assistants API is [Retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval): the ability to upload files that the Assistant will use as a knowledge base when answering questions. This can also be enabled from the Dashboard or the API, where we can upload files we want to be used.


![Enabling retrieval](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_enable_retrieval.png)



```python
# Upload the file
file = client.files.create(
    file=open(
        "language_models_are_unsupervised_multitask_learners.pdf",
        "rb",
    ),
    purpose="assistants",
)
# Update Assistant
assistant = client.beta.assistants.update(
    MATH_ASSISTANT_ID,
    tools=[{"type": "code_interpreter"}],
    tool_resources={"code_interpreter": {"file_ids": [file.id]}},
)
show_json(assistant)
```


```python
thread, run = create_thread_and_run(
    "What are some cool math concepts behind this ML paper pdf? Explain in two sentences."
)
run = wait_on_run(run, thread)
pretty_print(get_response(thread))
```

> **Note**
> There are more intricacies in Retrieval, like [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages), which may be covered in another cookbook.


### Functions

As a final powerful tool for your Assistant, you can specify custom [Functions](https://platform.openai.com/docs/assistants/tools/function-calling) (much like the [Function Calling](https://platform.openai.com/docs/guides/function-calling) in the Chat Completions API). During a Run, the Assistant can then indicate it wants to call one or more functions you specified. You are then responsible for calling the Function, and providing the output back to the Assistant.

Let's take a look at an example by defining a `display_quiz()` Function for our Math Tutor.

This function will take a `title` and an array of `question`s, display the quiz, and get input from the user for each:

- `title`
- `questions`
  - `question_text`
  - `question_type`: [`MULTIPLE_CHOICE`, `FREE_RESPONSE`]
  - `choices`: ["choice 1", "choice 2", ...]

Unfortunately I don't know how to get user input within a Python Notebook, so I'll be mocking out responses with `get_mock_response...`. This is where you'd get the user's actual input.



```python
def get_mock_response_from_user_multiple_choice():
    return "a"


def get_mock_response_from_user_free_response():
    return "I don't know."


def display_quiz(title, questions):
    print("Quiz:", title)
    print()
    responses = []

    for q in questions:
        print(q["question_text"])
        response = ""

        # If multiple choice, print options
        if q["question_type"] == "MULTIPLE_CHOICE":
            for i, choice in enumerate(q["choices"]):
                print(f"{i}. {choice}")
            response = get_mock_response_from_user_multiple_choice()

        # Otherwise, just get response
        elif q["question_type"] == "FREE_RESPONSE":
            response = get_mock_response_from_user_free_response()

        responses.append(response)
        print()

    return responses
```

Here's what a sample quiz would look like:



```python
responses = display_quiz(
    "Sample Quiz",
    [
        {"question_text": "What is your name?", "question_type": "FREE_RESPONSE"},
        {
            "question_text": "What is your favorite color?",
            "question_type": "MULTIPLE_CHOICE",
            "choices": ["Red", "Blue", "Green", "Yellow"],
        },
    ],
)
print("Responses:", responses)
```

Now, let's define the interface of this function in JSON format, so our Assistant can call it:



```python
function_json = {
    "name": "display_quiz",
    "description": "Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.",
    "parameters": {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "questions": {
                "type": "array",
                "description": "An array of questions, each with a title and potentially options (if multiple choice).",
                "items": {
                    "type": "object",
                    "properties": {
                        "question_text": {"type": "string"},
                        "question_type": {
                            "type": "string",
                            "enum": ["MULTIPLE_CHOICE", "FREE_RESPONSE"],
                        },
                        "choices": {"type": "array", "items": {"type": "string"}},
                    },
                    "required": ["question_text"],
                },
            },
        },
        "required": ["title", "questions"],
    },
}
```

Once again, let's update our Assistant either through the Dashboard or the API.


![Enabling custom function](https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_examples/images/assistants_overview_enable_function.png)

> **Note**
> Pasting the function JSON into the Dashboard was a bit finicky due to indentation, etc. I just asked ChatGPT to format my function the same as one of the examples on the Dashboard :).



```python
assistant = client.beta.assistants.update(
    MATH_ASSISTANT_ID,
    tools=[
        {"type": "code_interpreter"},
        {"type": "function", "function": function_json},
    ],
)
show_json(assistant)
```

And now, we ask for a quiz.



```python
thread, run = create_thread_and_run(
    "Make a quiz with 2 questions: One open ended, one multiple choice. Then, give me feedback for the responses."
)
run = wait_on_run(run, thread)
run.status
```

Now, however, when we check the Run's `status` we see `requires_action`! Let's take a closer look.



```python
show_json(run)
```

The `required_action` field indicates a Tool is waiting for us to run it and submit its output back to the Assistant. Specifically, the `display_quiz` function! Let's start by parsing the `name` and `arguments`.

> **Note**
> While in this case we know there is only one Tool call, in practice the Assistant may choose to call multiple tools.



```python
# Extract single tool call
tool_call = run.required_action.submit_tool_outputs.tool_calls[0]
name = tool_call.function.name
arguments = json.loads(tool_call.function.arguments)

print("Function Name:", name)
print("Function Arguments:")
arguments
```

Now let's actually call our `display_quiz` function with the arguments provided by the Assistant:



```python
responses = display_quiz(arguments["title"], arguments["questions"])
print("Responses:", responses)
```

Great! (Remember these responses are the one's we mocked earlier. In reality, we'd be getting input from the back from this function call.)

Now that we have our responses, let's submit them back to the Assistant. We'll need the `tool_call` ID, found in the `tool_call` we parsed out earlier. We'll also need to encode our `list`of responses into a `str`.



```python
tool_outputs = []
tool_calls = run.required_action.submit_tool_outputs.tool_calls

for tool_call in tool_calls:
    arguments = json.loads(tool_call.function.arguments)
    responses = display_quiz(arguments["title"], arguments["questions"])
    tool_outputs.append({
        "tool_call_id": tool_call.id,
        "output": json.dumps(responses),
    })
```


```python
run = client.beta.threads.runs.submit_tool_outputs(
    thread_id=thread.id,
    run_id=run.id,
    tool_outputs=tool_outputs
)
show_json(run)
```

We can now wait for the Run to complete once again, and check our Thread!



```python
run = wait_on_run(run, thread)
pretty_print(get_response(thread))
```

Now let's end the AgentOps session. By default, AgentOps will end the session in the "Intedeterminate" state. You can also end the session in the "Success" or "Failure" state.

We will end the session in the "Success" state.


```python
agentops.end_session(end_state="Success")
```

Woohoo üéâ

## Conclusion

We covered the basics of the Assistants API using OpenAI's Python SDK and AgentOps for observability.

For more information, check out the Assistants API deep [deep dive](https://platform.openai.com/docs/assistants/deep-dive) guide and its [documentation](https://platform.openai.com/docs/api-reference/assistants).


## docs/v1/examples/restapi.mdx

---
title: 'Fast API'
description: 'Observing agents in a server environment'
mode: "wide"
---

[//]: # (Card for video tutorial)
[//]: # (<Card title="REST Server + AgentOps" icon="computer" href="https://www.loom.com/share/cfcaaef8d4a14cc7a974843bda1076bf">)

[//]: # (	Using agents in a REST server and observing)

[//]: # (	![thumbnail]&#40;https://cdn.loom.com/sessions/thumbnails/cfcaaef8d4a14cc7a974843bda1076bf-1713568618224-with-play.gif&#41;)

[//]: # (</Card>)

## Adding AgentOps to Crew agents

<Steps>
	<Step title="Install the AgentOps SDK">
		<CodeGroup>
			```bash pip
			pip install agentops
			```
			```bash poetry
			poetry add agentops
			```
		</CodeGroup>
	</Step>
	<Step title="Install Crew with AgentOps">
		<CodeGroup>
			```bash pip
			pip install 'crewai[agentops]'
			```
			```bash poetry
			poetry add 'crewai[agentops]'
			```
		</CodeGroup>
	</Step>
	<Step title="Add 3 lines of code">
		1. Before calling the `Crew()` constructor in your code, call `agentops.init()`
		2. At the end of your Crew run, call `agentops.end_session("Success")`
		<CodeGroup>
			```python python
			import agentops

			# Beginning of program (i.e. main.py, __init__.py)
			# IMPORTANT: Must be before calling the `Crew()` constructor
			agentops.init(<INSERT YOUR API KEY HERE>)
			...
			# End of program (e.g. main.py)
			agentops.end_session("Success") # Success|Fail|Indeterminate
			```
		</CodeGroup>
		<Check>
			Instantiating the AgentOps client will automatically instrument Crew, meaning you will be able to see all
			of your sessions on the AgentOps Dashboard along with the full LLM chat histories, cost, token counts, etc.
		</Check>
		<Tip>
			For more features see our [Usage](/v1/usage) section.
		</Tip>
	</Step>
	<Step title="Set your API key">
		Retrieve an API Key from your Settings > [Projects & API Keys](https://app.agentops.ai/settings/projects) page.
		<Frame type="glass" caption="Settings > Projects & API Keys">
			<img height="200" src="/images/api-keys.png" />
		</Frame>
		<Info>
			API keys are tied to individual projects.<br></br>
			A Default Project has been created for you, so just click Copy API Key
		</Info>
		Set this API Key in your [environment variables](/v1/usage/advanced-configuration)
		```python .env
		AGENTOPS_API_KEY=<YOUR API KEY>
		```
	</Step>
    <Step title="Run your crew">
		Execute your program and visit [app.agentops.ai/drilldown](https://app.agentops.ai/drilldown) to observe your Crew! üïµÔ∏è
		<Tip>
			After your run, AgentOps prints a clickable url to console linking directly to your session in the Dashboard
		</Tip>
		<div/>{/* Intentionally blank div for newline */}
		<Frame type="glass" caption="Clickable link to session">
			<img height="200" src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/link-to-session.gif?raw=true" />
		</Frame>
	</Step>
</Steps>

## Special Considerations with Crew
The Crew framework is capable of determining when all tasks have been accomplished and to halt execution. AgentOps will automatically end your active session
when this determination is made. If you don't want your AgentOps session to end at this time, add an optional parameter to your `agentops.init()` call.

```python
agentops.init(skip_auto_end_session=True)
```

## Crew + AgentOps Examples

<CardGroup cols={3}>
    <Card title="Job Posting" icon="briefcase" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/job-posting" />
	<Card title="Instagram Post" icon="instagram" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/instagram_post" />
    <Card title="Markdown Validator" icon="markdown" href="https://github.com/Agentops-AI/crewAI-examples/tree/main/markdown_validator" />
</CardGroup>

## docs/v1/examples/recording_events.mdx

---
title: 'Recording Events'
description: 'How to record events using spans in your session'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/recording-operations.ipynb'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/recording-operations.ipynb  */}

# Recording Operations with Spans
AgentOps v0.4 uses spans to track different types of operations in your agent workflows.

We automatically instrument your LLM Calls from OpenAI, LiteLLM, Cohere, and more. Just make sure their SDKs are imported before initializing AgentOps like we see below.

First let's install the required packages


```python
%pip install -U openai
%pip install -U agentops
%pip install -U python-dotenv
```

Then import them


```python
from openai import OpenAI
import agentops
from agentops.sdk.decorators import session, agent, operation
import os
from dotenv import load_dotenv
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
```


```python
# Initialize the client
agentops.init()

# Optionally, we can add default tags to all sessions
# agentops.init(tags=['Hello Tracker'])

openai_client = OpenAI()

# Create a session to track all operations
@session
def my_session():
    messages = [{"role": "user", "content": "Hello"}]
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo", messages=messages, temperature=0.5
    )
    print(response.choices[0].message.content)
    
    # Call our operation
    result = add(2, 4)
    print(f"Addition result: {result}")
    
    # Call our agent
    agent = MyAgent()
    agent.process_data("test data")
    
# Run the session
my_session()
```

Click the AgentOps link above to see your session!

## Operations

AgentOps allows you to record operations using the `@operation` decorator:


```python
from agentops.sdk.decorators import operation

@operation
def add(x, y):
    return x + y
```

## Agents

You can create agent spans that contain operations using the `@agent` decorator:


```python
from agentops.sdk.decorators import agent, operation

@agent
class MyAgent:
    @operation
    def process_data(self, data):
        print(f"Processing data: {data}")
        return f"Processed: {data.upper()}"
```

## Error Handling

Errors are automatically captured by the spans. When an exception occurs within a decorated function, it's recorded in the span:


```python
from agentops.sdk.decorators import operation

@operation
def risky_operation():
    # This exception will be recorded in the span
    try:
        1 / 0  # Ooops! Something went wrong
    except Exception as e:
        print(f"Error occurred: {e}")
        raise

# Create a session that includes the error
@session
def error_session():
    try:
        risky_operation()
    except Exception:
        print("Caught the error, but it's still recorded in the span")

# Run the error session
error_session()
```

## Custom Span Attributes

You can add custom attributes to spans for additional context:


```python
from agentops.sdk.decorators import operation

@operation(name="custom-operation")
def custom_operation(data):
    # Your operation logic here
    return f"Custom: {data}"

# Create a session with custom operation
@session(name="custom-session")
def custom_session():
    result = custom_operation("test")
    print(result)

# Run the custom session
custom_session()
```


## docs/v1/examples/multi_session.mdx

---
title: 'Concurrent Traces Example'
description: 'Managing multiple concurrent traces and sessions'
mode: "wide"
---

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/multi_session_llm.ipynb'} target={'_blank'}>Github</a>_

# Multiple Concurrent Traces
This example demonstrates how to run multiple traces (sessions) concurrently using both the modern trace-based API and the legacy session API for backwards compatibility.

First let's install the required packages:

```bash
pip install -U openai
pip install -U agentops
pip install -U python-dotenv
```

Then import them:

```python
from openai import OpenAI
import agentops
import os
from dotenv import load_dotenv
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this example. It accounts for both users who use environment variables and those who just want to set the API Key here.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!

```python
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
```

Initialize AgentOps. We'll disable auto-start to manually create our traces:

```python
agentops.init(AGENTOPS_API_KEY, auto_start_session=False)
client = OpenAI()
```

## Modern Trace-Based Approach

The recommended approach uses `start_trace()` and `end_trace()`:

```python
# Create multiple concurrent traces
trace_1 = agentops.start_trace("user_query_1", tags=["experiment_a"])
trace_2 = agentops.start_trace("user_query_2", tags=["experiment_b"])

print(f"Trace 1 ID: {trace_1.span.get_span_context().trace_id}")
print(f"Trace 2 ID: {trace_2.span.get_span_context().trace_id}")
```

## LLM Calls with Automatic Tracking

With the modern implementation, LLM calls are automatically tracked without needing special session assignment:

```python
# LLM calls are automatically tracked and associated with the current context
messages_1 = [{"role": "user", "content": "Hello from trace 1"}]
response_1 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages_1,
    temperature=0.5,
)

messages_2 = [{"role": "user", "content": "Hello from trace 2"}]
response_2 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages_2,
    temperature=0.5,
)
```

## Using Context Managers

You can also use traces as context managers for automatic cleanup:

```python
with agentops.start_trace("context_managed_trace") as trace:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Hello from context manager"}],
        temperature=0.5,
    )
    # Trace automatically ends when exiting the context
```

## Using Decorators

For even cleaner code, use decorators:

```python
@agentops.trace
def process_user_query(query: str):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": query}],
        temperature=0.5,
    )
    return response.choices[0].message.content

# Each function call creates its own trace
result_1 = process_user_query("What is the weather like?")
result_2 = process_user_query("Tell me a joke")
```

## Legacy Session API (Backwards Compatibility)

For backwards compatibility, the legacy session API is still available:

```python
# Legacy approach - still works but not recommended for new code
session_1 = agentops.start_session(tags=["legacy-session-1"])
session_2 = agentops.start_session(tags=["legacy-session-2"])

# Legacy sessions work the same way as before
session_1.end_session(end_state="Success")
session_2.end_session(end_state="Success")
```

## Ending Traces

End traces individually or all at once:

```python
# End specific traces
agentops.end_trace(trace_1, "Success")
agentops.end_trace(trace_2, "Success")

# Or end all active traces at once
# agentops.end_trace(end_state="Success")
```

## Key Differences from Legacy Multi-Session Mode

1. **No mode switching**: You can create multiple traces without entering a special "multi-session mode"
2. **Automatic LLM tracking**: LLM calls are automatically associated with the current execution context
3. **No exceptions**: No `MultiSessionException` or similar restrictions
4. **Cleaner API**: Use decorators and context managers for better code organization
5. **Backwards compatibility**: Legacy session functions still work for existing code

If you look in the AgentOps dashboard, you will see multiple unique traces, each with their respective LLM calls and events properly tracked.



## docs/v1/examples/multi_agent.mdx

---
title: 'Multi-Agent Example'
description: 'How to track operations from multiple different agents'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/multi_agent_example.ipynb'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/multi_agent_example.ipynb  */}

# Multi-Agent Support
This is an example implementation of tracking operations from two separate agents

First let's install the required packages


```python
%pip install -U openai
%pip install -U agentops
%pip install -U python-dotenv
```

Then import them


```python
import agentops
from agentops.sdk.decorators import agent, operation
from openai import OpenAI
import os
from dotenv import load_dotenv
import logging
from IPython.display import display, Markdown
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
logging.basicConfig(
    level=logging.DEBUG
)  # this will let us see that calls are assigned to an agent
```


```python
agentops.init(AGENTOPS_API_KEY, tags=["multi-agent-notebook"])
openai_client = OpenAI(api_key=OPENAI_API_KEY)
```

Now lets create a few agents!


```python
@agent(name="qa")
class QaAgent:
    def __init__(self):
        pass
        
    @operation
    def completion(self, prompt: str):
        res = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "You are a qa engineer and only output python code, no markdown tags.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.5,
        )
        return res.choices[0].message.content


@agent(name="engineer")
class EngineerAgent:
    def __init__(self):
        pass
        
    @operation
    def completion(self, prompt: str):
        res = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "You are a software engineer and only output python code, no markdown tags.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.5,
        )
        return res.choices[0].message.content
```


```python
qa = QaAgent()
engineer = EngineerAgent()
```

Now we have our agents and we tagged them with the `@agent` decorator. Any LLM calls that go through this class will now be tagged as agent calls in AgentOps.

Let's use these agents!


```python
generated_func = engineer.completion("python function to test prime number")
```


```python
display(Markdown("```python\n" + generated_func + "\n```"))
```


```python
generated_test = qa.completion(
    "Write a python unit test that test the following function: \n " + generated_func
)
```


```python
display(Markdown("```python\n" + generated_test + "\n```"))
```

Perfect! It generated the code as expected, and in the DEBUG logs, you can see that the calls were made by agents named "engineer" and "qa"!

Let's verify one more thing! If we make an LLM call outside of the context of a tracked agent, we want to make sure it gets assigned to the Default Agent.


```python
res = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are not a tracked agent"},
        {"role": "user", "content": "Say hello"},
    ],
)
res.choices[0].message.content
```

You'll notice that we didn't log an agent name, so the AgentOps backend will assign it to the Default Agent for the session!


## docs/v1/examples/langchain.mdx

---
title: 'LangChain Example'
description: 'Using the LangChain Callback Handler'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/langchain_examples.ipynb'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/langchain_examples/langchain_examples.ipynb  */}

# AgentOps Langchain Agent Implementation

Using AgentOps monitoring with Langchain is simple. We've created a LangchainCallbackHandler that will do all of the heavy lifting!

First let's install the required packages


```python
%pip install langchain==0.2.9
%pip install langchain_openai
%pip install -U agentops
%pip install -U python-dotenv
```

Then import them


```python
import os
from langchain_openai import ChatOpenAI
from langchain.agents import tool, AgentExecutor, create_openai_tools_agent
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
```

The only difference with using AgentOps is that we'll also import this special Callback Handler


```python
from agentops.partners.langchain_callback_handler import (
    LangchainCallbackHandler as AgentOpsLangchainCallbackHandler,
)
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
AGENTOPS_API_KEY = os.environ.get("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
```

This is where AgentOps comes into play. Before creating our LLM instance via Langchain, first we'll create an instance of the AO LangchainCallbackHandler. After the handler is initialized, a session will be recorded automatically.

Pass in your API key, and optionally any tags to describe this session for easier lookup in the AO dashboard.



```python
agentops_handler = AgentOpsLangchainCallbackHandler(
    api_key=AGENTOPS_API_KEY, tags=["Langchain Example"]
)

llm = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY, callbacks=[agentops_handler], model="gpt-3.5-turbo"
)

# You must pass in a callback handler to record your agent
llm.callbacks = [agentops_handler]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Respond only in Spanish."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
        # ("tool_names", "find_movie")
    ]
)
```

You can also retrieve the `session_id` of the newly created session.


```python
print("Agent Ops session ID: " + str(agentops_handler.current_session_ids))
```

Agents generally use tools. Let's define a simple tool here. Tool usage is also recorded.


```python
@tool
def find_movie(genre: str) -> str:
    """Find available movies"""
    if genre == "drama":
        return "Dune 2"
    else:
        return "Pineapple Express"


tools = [find_movie]
```

For each tool, you need to also add the callback handler


```python
for t in tools:
    t.callbacks = [agentops_handler]
```

Add the tools to our LLM


```python
llm_with_tools = llm.bind_tools([find_movie])
```

Finally, let's create our agent! Pass in the callback handler to the agent, and all the actions will be recorded in the AO Dashboard


```python
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
```


```python
agent_executor.invoke(
    {"input": "What comedies are playing?"}, config={"callback": [agentops_handler]}
)
```

## Check your session
Finally, check your run on [AgentOps](https://app.agentops.ai)

Now if we look in the AgentOps dashboard, you will see a session recorded with the LLM calls and tool usage.

## Langchain v0 Example

This langchain version is out of date and support is being deprecated. You can find the example notebook [here](https://github.com/agentops-ai/agentops/blob/main/examples/langchain_examples/langchain_v0_example.ipynb).


## docs/v1/examples/camel.mdx

---
title: 'CAMEL AI Example'
description: 'Using CAMEL AI for Multi-Agent Simulation'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/camel_examples/camelai-multi-agent-example.ipynb'} target={'_blank'}>Github</a>_

{/*  SOURCE_FILE: examples/camel_examples/camelai-multi-agent-example.ipynb  */}

# CAMEL AI Multi Agent Example

In this example, we will use CamelAI to simulate tools! In this case, we will best determine how many shots it takes to beat an enemy with a blue shield in Apex Legeneds using melee only. The character "Pathfinder" from Apex Legends will answer.

First let's install the required packages for this example.


```python
%pip install camel-ai[all]
%pip install agentops
```

Next we import the necessary libraries


```python
import agentops
import os
from getpass import getpass
from dotenv import load_dotenv
from typing import List
from colorama import Fore

# Camel imports
from camel.agents.chat_agent import FunctionCallingRecord
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated
from camel.toolkits import SearchToolkit, MathToolkit
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your openai key here>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your agentops key here>"
```

Now we will initialize our AgentOps client.


```python
agentops.init(tags=["camel", "multi-agent", "example"])
```

Let's start with setting our task prompt and setting our tools.

You can look at the link below to see all available tools:
https://docs.camel-ai.org/key_modules/tools.html


```python
task_prompt = (
    "We are in the Apex Legends Games. Determine the amount of"
    "meele hits it will take to beat someone with a blue shield."
    "You should answer as if you are Pathfinder from the Apex Games."
)

tools = [
    *MathToolkit().get_tools(),
    *SearchToolkit().get_tools(),
]
```

We will now create our Camel AI session which is of [`RolePlaying`](https://docs.camel-ai.org/key_modules/society.html#roleplaying) type. Here we will set the assistant and user role names, as well as the model and tools for each agent.


```python
search_session = RolePlaying(
    assistant_role_name="Searcher",
    user_role_name="Pathfinder",
    assistant_agent_kwargs=dict(
        model=ModelFactory.create(
            model_platform=ModelPlatformType.OPENAI,
            model_type=ModelType.GPT_4O_MINI,
        ),
        tools=tools,
    ),
    user_agent_kwargs=dict(
        model=ModelFactory.create(
            model_platform=ModelPlatformType.OPENAI,
            model_type=ModelType.GPT_4O_MINI,
        ),
    ),
    task_prompt=task_prompt,
    with_task_specify=False,
)
```

Let's print out the Assistant System Message and User Task Prompt.


```python
print(
    Fore.GREEN
    + f"AI Assistant System Message:\n{search_session.assistant_sys_msg}\n"
)
print(Fore.BLUE + f"AI User System Message:\n{search_session.user_sys_msg}\n")

print(Fore.YELLOW + f"Original Task Prompt:\n{task_prompt}\n")
print(
    Fore.CYAN
    + "Specified Task Prompt:"
    + f"\n{search_session.specified_task_prompt}\n"
)
print(Fore.RED + f"Final Task Prompt:\n{search_session.task_prompt}\n")
```

Now we will initiate our Camel AI session and begin the chat loop. You can see that we have set the number of messages to 50. This is to prevent the session from running indefinitely.


```python
n = 0
input_msg = search_session.init_chat()
while n < 50:
    n += 1
    assistant_response, user_response = search_session.step(input_msg)

    if assistant_response.terminated:
        print(
            Fore.GREEN
            + (
                "AI Assistant terminated. Reason: "
                f"{assistant_response.info['termination_reasons']}."
            )
        )
        break
    if user_response.terminated:
        print(
            Fore.GREEN
            + (
                "AI User terminated. "
                f"Reason: {user_response.info['termination_reasons']}."
            )
        )
        break

    # Print output from the user
    print_text_animated(
        Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
    )

    # Print output from the assistant, including any function execution information
    print_text_animated(Fore.GREEN + "AI Assistant:")
    tool_calls: List[FunctionCallingRecord] = assistant_response.info[
        'tool_calls'
    ]
    for func_record in tool_calls:
        print_text_animated(f"{func_record}")
    print_text_animated(f"{assistant_response.msg.content}\n")

    if "CAMEL_TASK_DONE" in user_response.msg.content:
        break

    input_msg = assistant_response.msg
```

Awesome! We've successfully completed our session.

Now we will end the session with a success message. We can also end the session with a failure or indeterminate status. By default, the session will be marked as indeterminate.


```python
agentops.end_session("Success")
```

## Check your session
Finally, check your run on [AgentOps](https://app.agentops.ai)

Now if we look in the AgentOps dashboard, you will see a session recorded with the LLM calls and tool usage.


## docs/v1/examples/ollama.mdx

---
title: 'Ollama Example'
description: 'Using Ollama with AgentOps'
mode: "wide"
---

{/*  SOURCE_FILE: examples/ollama_examples/ollama_examples.ipynb  */}

# AgentOps Ollama Integration

This example demonstrates how to use AgentOps to monitor your Ollama LLM calls.

First let's install the required packages

> ‚ö†Ô∏è **Important**: Make sure you have Ollama installed and running locally before running this notebook. You can install it from [ollama.ai](https://ollama.com).


```python
%pip install -U ollama
%pip install -U agentops
%pip install -U python-dotenv
```

Then import them


```python
import ollama
import agentops
import os
from dotenv import load_dotenv

```

Next, we'll set our API keys. For Ollama, we'll need to make sure Ollama is running locally.
[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...
2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
# Let's load our environment variables
load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
```


```python
# Initialize AgentOps with some default tags
agentops.init(AGENTOPS_API_KEY, tags=["ollama-example"])
```

Now let's make some basic calls to Ollama. Make sure you have pulled the model first, use the following or replace with whichever model you want to use.


```python
ollama.pull("mistral")
```


```python
# Basic completion,
response = ollama.chat(model='mistral',
    messages=[{
        'role': 'user',
        'content': 'What are the benefits of using AgentOps for monitoring LLMs?',
    }]
)
print(response['message']['content'])
```

Let's try streaming responses as well


```python
# Streaming Example
stream = ollama.chat(
    model='mistral',
    messages=[{
        'role': 'user',
        'content': 'Write a haiku about monitoring AI agents',
    }],
    stream=True
)

for chunk in stream:
    print(chunk['message']['content'], end='')

```


```python
# Conversation Example
messages = [
    {
        'role': 'user',
        'content': 'What is AgentOps?'
    },
    {
        'role': 'assistant',
        'content': 'AgentOps is a monitoring and observability platform for LLM applications.'
    },
    {
        'role': 'user',
        'content': 'Can you give me 3 key features?'
    }
]

response = ollama.chat(
    model='mistral',
    messages=messages
)
print(response['message']['content'])
```

> üí° **Note**: In production environments, you should add proper error handling around the Ollama calls and use `agentops.end_session("Error")` when exceptions occur.

Finally, let's end our AgentOps session


```python
agentops.end_session("Success")
```


## docs/v1/examples/simple_agent.mdx

---
title: 'Simple Agent'
description: 'Most basic usage with OpenAI API'
mode: "wide"
---
_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/openai-gpt.ipynb'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/openai-gpt.ipynb  */}

# AgentOps Basic Monitoring 
This is an example of how to use the AgentOps library for basic Agent monitoring with OpenAI's GPT

First let's install the required packages


```python
%pip install -U openai
%pip install -U agentops
%pip install -U python-dotenv
```

Then import them


```python
from openai import OpenAI
import agentops
import os
from dotenv import load_dotenv
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
```

The AgentOps library is designed to be a plug-and-play replacement for the OpenAI Client, maximizing use with minimal install effort.


```python
openai = OpenAI(api_key=OPENAI_API_KEY)
agentops.init(AGENTOPS_API_KEY, tags=["openai-gpt-notebook"])
```

Now just use OpenAI as you would normally!

## Single Session with ChatCompletion


```python
message = [{"role": "user", "content": "Write a 12 word poem about secret agents."}]
response = openai.chat.completions.create(
    model="gpt-3.5-turbo", messages=message, temperature=0.5, stream=False
)
print(response.choices[0].message.content)
```

Make sure to end your session with a `Result` (Success|Fail|Indeterminate) for better tracking


```python
agentops.end_session("Success")
```

Now if you check the AgentOps dashboard, you should see information related to this run!

# Events
Additionally, you can track custom events via AgentOps.
Let's start a new session and record some events 


```python
# Create new session
agentops.start_session(tags=["openai-gpt-notebook-events"])
```

The easiest way to record actions is through the use of AgentOps' decorators


```python
from agentops import record_action


@record_action("add numbers")
def add(x, y):
    return x + y


add(2, 4)
```

We can also manually craft an event exactly the way we want


```python
from agentops import ActionEvent

message = ({"role": "user", "content": "Hello"},)
response = openai.chat.completions.create(
    model="gpt-3.5-turbo", messages=message, temperature=0.5
)

if "hello" in str(response.choices[0].message.content).lower():
    agentops.record(
        ActionEvent(
            action_type="Agent says hello",
            logs=str(message),
            returns=str(response.choices[0].message.content),
        )
    )
```


```python
agentops.end_session("Success")
```


## docs/v1/examples/multion.mdx

---
title: 'Multion Examples'
description: 'Tracking Multion usage with AgentOps'
mode: "wide"
---

_View All Notebooks on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/multion_examples/'} target={'_blank'}>Github</a>_


{/*  SOURCE_FILE: examples/multion_examples/Autonomous_web_browsing.ipynb  */}

# MultiOn Tracking Web Browse Actions
<img src="https://github.com/AgentOps-AI/agentops/blob/main/docs/logo/multion_integration.png?raw=true" width="250px" style="max-width: 100%; height: auto;"/>
    
Agents using MultiOn can launch and control remote or local web browsers to perform actions and retrieve context using natural language commands. With AgentOps, MultiOn evens such as browse, retrieve, and step are automatically tracked.


![AgentOps MultiOn Browse](https://github.com/AgentOps-AI/agentops/blob/main/docs/images/agentops-multion-browse.gif?raw=true)

Furthermore, events and LLM calls in your Python program will be tracked as well.

First let's install the required packages


```python
%pip install -U multion
%pip install -U agentops
%pip install -U openai
%pip install -U python-dotenv
```

Then import them


```python
from multion.client import MultiOn
from multion.core.request_options import RequestOptions
import openai
import agentops
import os
from dotenv import load_dotenv
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
MULTION_API_KEY = os.getenv("MULTION_API_KEY") or "<your_multion_key>"
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "<your_agentops_key>"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or "<your_openai_key>"
```

### Tracking MultiOn events with AgentOps

When an `agentops_api_key` is provided, MultiOn will automatically start an AgentOps session and record events.


```python
multion = MultiOn(
    api_key=MULTION_API_KEY,
    agentops_api_key=AGENTOPS_API_KEY,
)
cmd = "what three things do i get with agentops"
request_options = RequestOptions(
    timeout_in_seconds=60, max_retries=4, additional_headers={"test": "ing"}
)

browse_response = multion.browse(
    cmd="what three things do i get with agentops",
    url="https://www.agentops.ai/",
    max_steps=4,
    include_screenshot=True,
    request_options=request_options,
)

print(browse_response.message)

# End session to see your dashboard
agentops.end_session("Success")
```

### Linking MultiOn events to an existing AgentOps session
When running `agentops.init()`, be sure to set `auto_start_session=False`. MultiOn will automatically launch AgentOps sessions by default, but by setting auto start to false, you can configure your AgentOps client independently.


```python
agentops.init(
    AGENTOPS_API_KEY, auto_start_session=False, tags=["MultiOn browse example"]
)
```

Now, we can launch a MultiOn browse event. This event will automatically get added to your AgentOps session.


```python
multion = MultiOn(
    api_key=MULTION_API_KEY,
    agentops_api_key=AGENTOPS_API_KEY,
)
cmd = "what three things do i get with agentops"
request_options = RequestOptions(
    timeout_in_seconds=60, max_retries=4, additional_headers={"test": "ing"}
)

browse_response = multion.browse(
    cmd="what three things do i get with agentops",
    url="https://www.agentops.ai/",
    max_steps=4,
    include_screenshot=True,
    request_options=request_options,
)

print(browse_response.message)
```

Let's use OpenAI to summarize our output


```python
messages = [
    {
        "role": "user",
        "content": f"Format this data as a markdown table: {browse_response.message}",
    }
]
client = openai.OpenAI()
response = client.chat.completions.create(messages=messages, model="gpt-3.5-turbo")

print(response.choices[0].message.content)
```


```python
agentops.end_session("Success")
```

## Check your session
Check your session on [AgentOps](https://app.agentops.ai). This session should include the MultiOn browse action and the OpenAI call.

![AgentOps MultiOn Browse](https://github.com/AgentOps-AI/agentops/blob/main/docs/images/agentops-multion-browse.gif?raw=true)


## docs/v0/tracking-llm-calls.mdx

---
title: "Tracking LLM Calls"
description: "Tracking LLM Calls using the AgentOps SDK"
---

When the AgentOps SDK detects the `openai` as an install module, it will automatically
start tracking OpenAI calls by instrumenting chat completions to track their usage.

To get started, just follow the quick start guide.

<CardGroup cols={2}>
  <Card title="Quick Start" icon="code" href="/v0/quickstart">
    Get started with AgentOps in under 5 mins
  </Card>
</CardGroup>


## docs/v0/logger.mdx

---
title: 'AgentOps Logger'
description: 'This document explains the use of the AgentOpsLogger
 and AgentOpsHandler classes, which are used for creating loggers 
 and handlers configured to work with the AgentOps service'
---

## AgentOps logger

The AgentOps logger is quick and simple way of integrating your existing 
agent codebase with AgentOps. It allows you to use your current logs as events 
by extending the built-in Python logging system to emit events to AgentOps.

There are main ways of using it: 
* Initializing a new Logger with AgentOps functionality
* Adding a log handler to your existing Logger

But first, we must import AgentOps classes and initilize the AgentOps client.

```python python
from agentops import Client, AgentOpsLogger

ao_client = Client(<INSERT YOUR API KEY HERE>)
```

### Initializing a new Logger

If you don't already have a logger or want to simply initialize your logger
via AgentOps, do the following: 

We can initilize a new logger with the following:
```python python
logger = AgentOpsLogger.get_agentops_logger(ao_client, "my_logger")
```
Now whenever you call get a logger with the same name, any logs written to the 
logger will be sent to AgentOps.

`get_agentops_logger` also takes the optional parameter of `level` to limit
the amount of logs that get sent to AgentOps.

### Adding a log handler

If you already have complex set ups for your logging and you find it more convinient, 
you can get a log handler from Agent Ops and attach it to your existing logger

Then we can initilize a new logger with the client
```python python
handler = AgentOpsLogger.get_agentops_handler(agentops, "my_logger")
<YOUR LOGGER>.addHandler(handler)
```

## Details

The AgentOpsLogger class is a utility class for creating loggers 
and handlers configured to work with the AgentOps service.

```python python
@staticmethod
def get_agentops_logger(client: Client, name: str, level=logging.DEBUG)
```
This static method creates and returns a logger with an AgentOpsHandler.

Arguments

   * client (Client): The AgentOps client to which the logs will be sent.
   * name (str): The name for the logger and handler.
   * level (int, optional): The minimum severity level to log. Defaults to logging.DEBUG.

Returns
* A logger (logging.Logger) configured with an AgentOpsHandler.

```python python
@staticmethod
def get_agentops_handler(client: AgentOps, name: str)
```

This static method creates and returns an AgentOpsHandler.

Arguments
    * client (Client): The AgentOps client to which the logs will be sent.
    * name (str): The name for the handler.

Returns
* A new AgentOpsHandler with the given client and name.

The AgentOpsHandler class is a custom logging handler for sending logs 
to the AgentOps service. This handler extends the built-in logging.Handler
 class to send log records to AgentOps. It also removes ANSI color codes 
 from log messages before sending them.

This method initializes the handler with a specific AgentOps client and name.

Arguments
    * client (Client): The AgentOps client to which the logs will be sent.
    * name (str): The name for this handler.

```python python
@staticmethod
def remove_color_codes(s: str) -> str
```
This static method removes ANSI color codes from a string.

Arguments
    * s (str): The string from which color codes will be removed.

Returns
*The same string, but without any color codes.

###emit

```python python
def emit(self, record)
```
This method processes a log record and sends it to the AgentOps client. It is called whenever a log record needs to be processed.

Arguments
    * record (logging.LogRecord): The log record to process.

## docs/v0/quickstart.mdx

---
title: "Quickstart"
description: "Start using AgentOps in under 5 minutes"
---

## Analytics in under 5 minutes:

4 simple steps:

1. Import AgentOps
2. Initialize AgentOps
3. Patch APIs/Decorate functions
4. End Session

<CodeGroup>
```python python
import openai # Make sure openai is imported before instantiating an AgentOps client.
import agentops

# Beginning of program (i.e. main.py, **init**.py)

ao_client = agentops.Client(<INSERT YOUR API KEY HERE>)

...

# (optional: record specific functions)

@ao_client.record_action('sample function being record')
def sample_function(...):
...

...

# End of program

ao_client.end_session('Success')

# Woohoo You're done üéâ

````

</CodeGroup>

AgentOps is now set and ready to use. Every OpenAI call you make will now be recorded to the dashboard. Finally, before ending your agent you will need to close your session. `end_session` can be `Success`, `Fail`, or `Indeterminate`. We suggest setting session state depending on how your agent exits or whether your agent succeeded or not.

We have 2 additional mechanisms for recording data. Namely, function decorators and discrete function calls. We suggest liberal usage of the `record_action` decorator to get the most out of your sessions.

### Get an API key

You can retrieve an API from [your account page](https://app.agentops.ai/account).

### Install the SDK

<CodeGroup>
```bash pip 
pip install agentops
```
```bash poetry
poetry add agentops
```
</CodeGroup>

### Add the AgentOps SDK to your agent in 3 easy steps

<AccordionGroup>

<Accordion icon="code" title="Add AgentOps to your logger">
  The quickest way to integrate AgentOps into your agent is to use
  the AgentOpsLogger. You can record all of your log events
  as events. Add these imports to your code:
  ```python python
  import agentops
  ```
  Then initialize your AgentOps client:
  ```python python
  ao_client = agentops.Client(<INSERT YOUR API KEY HERE>)
  ```
  We recommend you store your `API_Key` as an environmental variable
  and read it with `os.getenv('AGENT_OPS_KEY')`
</Accordion>
<Accordion icon="stop" title="Close the Session when your agent's run ends">
  AgentOps records your agent's runs as `Sessions`, grouping all of 
  events of a run together. It also includes the result of the run.
  When your agent is about to finish it's run, add this line to your code to 
  wrap up the session:
  ```python python
  ao_client.end_session(end_state="Success")
  ```
  An `end_state` of `Success`, `Fail`, or `Indeterminate` is required.
  Furthermore, an `end_state_reason` can be provided to add additional context (i.e. `Could not find element on page`).
</Accordion>
<Accordion icon="browser" title="Check out your dashboard">
  Run your agent and then visit [`app.agentops.ai`](https://app.agentops.ai). 
  From the "Session Drill-Down" you should be able to see all of your OpenAI calls! 
  Note: this will only track your OpenAI calls and you must be using the `openai` package.
  In order to tracker other events, explore our more advanced functionality.
</Accordion>
</AccordionGroup>

That's all you need to get started! Check out the documentation below to see
how you can record other events. AgentOps is a lot more powerful this way!

## Explore our more advanced functionality!

<CardGroup cols={2}>
  <Card title="Recording Events" icon="square-code" href="/v0/recording-events">
    Record all of your other events the way AgentOps intends.
  </Card>
</CardGroup>


## docs/v0/introduction.mdx

---
title: Introduction
description: "Build your next agent with evals, observability, and replays."
---

## Get Started

<CardGroup cols={2}>
  <Card title="Quick Start" icon="rocket" href="/v0/quickstart">
    Get started with AgentOps in under 5 mins
  </Card>
</CardGroup>

## Request an API key

AgentOps is in maintenance mode, but you can sign up for an account [here](https://legacy.agentops.ai)

## Integrate with your agent

<CardGroup cols={2}>
  <Card title="Quick Start" icon="rocket" href="/v0/quickstart">
    Get started with AgentOps in under 5 mins
  </Card>
  <Card title="Recording Events" icon="square-code" href="/v0/recording-events">
    Record events the way AgentOps is intended to work.
  </Card>
</CardGroup>


## docs/v0/recording-events.mdx

---
title: 'Recording Events'
description: 'Record Events using the AgentOps SDK'
---

To get the most out of AgentOps, it is best to carefully consider what events to record - 
not simply record whatever your agent is logging. AgentOps offers two ways to record events 
outside of logs.


## `record_action` Decorator
To make AgentOps easier to integrate, we also provide a function decorator to automatically creates 
and records an event for your function.

```python python
@ao_client.record_action('sample function being record')
def sample_function(...):
    ...
```

The decorator will record the function's parameters, returns, and the time duration. We suggest using this on functions that take a long time and contain nested functions. For example, if you decorate a function that makes several OpenAI calls, then each openai call will show in the replay graph as a child of the decorated function.


record_action:

Arguments
        * event_type (str): Type of the event
        * tags (List[str], optional): Any tags associated with the event. Defaults to None.


## `record()` Method
From this point, simply call the .record() method in the AgentOps client:

```python python
ao_client.record(Event("event_type1"))
```

In AgentOps, each session is associated with a number of "Events". Events have 
must have an "event_type" which is any arbitrary string of your choice. It might be something
like "OpenAI Call". Events can also have other information such as the parameters of the operation,
the returned data, alongside tags, etc.


Events:
Represents a discrete event to be recorded.

Args:
        event_type (str): Type of the event, e.g., "API Call". Required.
        params (Optional[Dict[str, Any]], optional): The parameters passed to the operation. Defaults to None.
        returns (str, optional): The output of the operation. Defaults to None.
        result (str, optional): Result of the operation, e.g., "Success", "Fail", "Indeterminate". Defaults to "Indeterminate".
        action_type (str, optional): Type of action of the event e.g. 'action', 'llm', 'api', 'screenshot'. Defaults to 'action'.
        model (Models, optional): The model used during the event if an LLM is used (i.e. GPT-4).
            For models, see the types available in the Models enum. 
            If a model is set but an action_type is not, the action_type will be coerced to 'llm'. 
Arguments
        * event_type (str): Type of the event, e.g., "API Call". Required.
        * params (str, optional): The parameters passed to the operation.
        * returns (str, optional): The output of the operation.
        * result (EventState, optional): Result of the operation, Enum: "Success", "Fail", "Indeterminate".
        * action_type (ActionType, optional): Type of action of the event e.g. 'action', 'llm', 'api'
        * model (Models, optional): The model used during the event if an LLM is used (i.e. GPT-4).
            For models, see the types available in the Models enum. 
            If a model is set but an action_type is not, the action_type will be coerced to 'llm'. 
            Defaults to None.
        * prompt (str, optional): The input prompt for an LLM call when an LLM is being used.
        * tags (List[str], optional): Tags that can be used for grouping or sorting later. e.g. ["my_tag"].


Attributes
        * timestamp (float): The timestamp for when the event was created, represented as seconds since the epoch.


## docs/snippets/crewai-card.mdx

<Card title="Crewai boosts my product on Reddit..." icon={<svg width="46" height="28" xmlns="http://www.w3.org/2000/svg" version="1.0" viewBox="0 0 2042 704"><path fill="#FFF" fill-opacity=".4" d="M796.8 32.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m52.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m21.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m15 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m13 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m699 7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m39 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M641.2 176c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m0 14c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m1306.9 34.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M615.8 266.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m1226.5 306.8c0 4.4.2 6.1.4 3.7.2-2.3.2-5.9 0-8-.2-2-.4-.1-.4 4.3m-1032 69.2c3.2.2 8.1.2 11 0s.3-.3-5.8-.3c-6 0-8.4.1-5.2.3m241.5 0c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4m167.5 0c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m396.5 0c2.3.2 5.9.2 8 0 2-.2.1-.4-4.3-.4s-6.1.2-3.7.4m-1232 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m13 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6"/><path fill="#FFF" fill-opacity=".5" d="M93.2 489c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 26c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m152.6 140.7c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4"/><path fill="#FFF" fill-opacity=".7" d="M800.8 32.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m41 0c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4m24 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m17 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m13 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m706.5 7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m29 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m-990 140.8c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m1307 33c0 3.3.2 4.5.4 2.7s.2-4.5 0-6-.4 0-.4 3.3M618.8 266.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m15 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M93.2 493.5c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m0 17c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m1749 52.5c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 21c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-376.4 45.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m16 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-1093 14c1.2.2 3.2.2 4.5 0 1.2-.2.2-.4-2.3-.4s-3.5.2-2.2.4m-158 11c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m1500 17c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><path fill="#FFF" fill-opacity=".8" d="M685.9 75.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-8 8c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m754.6 32.5-4 4.3 4.3-4c3.9-3.6 4.7-4.5 3.9-4.5-.2 0-2.1 1.9-4.2 4.2M536 523.3c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m859 76.2c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m-444 .9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-256 1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m420 9.3-5.5 5.8 6-5.5c3.3-3 6.2-5.6 6.4-5.8.2-.1 0-.2-.5-.2s-3.4 2.6-6.4 5.7m-158-4.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-826 1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m330.9 5.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-6 6c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7"/><path fill="#FFF" fill-opacity=".9" d="M807.3 32.7c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m21 0c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m-37.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m69.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-78.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m96 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m14 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m13 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m11 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m10 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m9 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m9 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m17 2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m652 0c3.4.2 8.8.2 12 0 3.1-.2.3-.3-6.3-.3s-9.2.1-5.7.3m-22 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m53 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-660 2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-291.9 25c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-10 9-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7M1767 86.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M673.9 88.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M1772 90.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m13 12.1c3 3 5.7 5.5 5.9 5.5.3 0-1.9-2.5-4.9-5.5s-5.7-5.5-5.9-5.5c-.3 0 1.9 2.5 4.9 5.5m-335.1-1.8c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-7 6-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m351.1 5.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-371.3 14.1c-1.5 1.4-2.4 2.5-1.9 2.5s1.9-1.1 3.2-2.5c3-3.2 2.3-3.2-1.3 0m419.1.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m14 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m58.2 19.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m8.5 7.6c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m5.5 6.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-1285.9 8.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m0 30c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-159.3 61.1c1.7.2 4.7.2 6.5 0 1.7-.2.3-.4-3.3-.4s-5 .2-3.2.4m142 7c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4M544.5 283c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m-122.6-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-29.9-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m19.5 9.8-5 5.3 5.3-5c4.8-4.6 5.7-5.5 4.9-5.5-.1 0-2.5 2.4-5.2 5.2m1506.4-.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-4 5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-1722 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1714.5 7.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m-12 12-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m-1722.5.5c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M1892.3 350c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3M522.9 507.7c-1.2 1.6-1.2 1.7.2.6.9-.7 2-.9 2.4-.5.5.4.5.1.1-.6-.9-1.6-1.1-1.5-2.7.5m9.1 10.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m73.8 35.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m19 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m1217.4 4.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-481.3 4.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-5.5 6.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-9.9 11-5 5.3 5.3-5c2.8-2.7 5.2-5.1 5.2-5.2 0-.8-.9.1-5.5 4.9m496.7 7.8c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M682 587.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m652 4.1c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m54-.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-699 3.9c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m1154.1 3.3c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-519.7 2.6-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M954 603.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m363.9 2.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m83.1-1.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-701 1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-234.1 2.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m658.1-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m720.1 2.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-303.1 1.9c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-1405 1c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m826-2.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-503.1 5.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1384 0c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-294.9 1.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-444.1 3.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-653 2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1383.5.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-362.6 6.5c1.7.2 4.7.2 6.5 0 1.7-.2.3-.4-3.3-.4s-5 .2-3.2.4m353.6 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-6.5 5.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-976.1 4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-46 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m35 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m212 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m167 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m23 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m398 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-1254 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m1300.7 2.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m9 9c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m47.8 17.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5"/><g fill="#FFF" stroke-width="0"><path d="M801.5 33c-51.2 3-90 18.3-118.5 46.9-22.4 22.5-35.1 48.9-39.6 82.1-4.5 34.2 2.6 73.9 18.3 101.3 2.4 4.3 4.2 8.1 4 8.3-.3.3-3.1-.2-6.3-1-10.1-2.5-19.8-3.6-32.2-3.6-21.6 0-40.8 4.8-63 15.6l-13.3 6.5-7.8-7.3c-16.6-15.5-33-21.8-57.1-21.8-26.3 0-49.5 9.5-70.1 28.8l-8.8 8.3-9.5-9.4c-12.1-11.9-25-21.1-40.1-28.7-27.5-13.7-51.6-16-82.8-8.1-51.8 13.3-104.4 57.9-140.6 119.5-7.8 13.2-19.2 36.6-24.4 49.6-21.2 54-21.4 110-.7 153.5 13.3 27.8 39.3 53.6 67.1 66.3 35.9 16.6 79.7 19.6 123 8.7 11.8-3 28.1-8.6 37.4-12.9l7-3.2 8 3.2c16.3 6.6 38.9 9 56.1 6 35.7-6.2 61.8-28.8 78.1-67.6 5.2-12.5 11.7-24.6 22.8-43 10.8-18 15.6-25.2 16.1-24.8.2.2 2.1 3.1 4.2 6.4 15.5 23.8 40.9 37.4 75.8 40.5 14.9 1.4 31.1 0 45.8-3.7 5.4-1.3 10-2.3 10.1-2.2.1.2 1.6 3.9 3.4 8.3 11.9 28.8 35.5 53.7 64.7 68.3 37 18.5 92.8 23.3 140.7 12.1 23.1-5.4 49.5-16.9 69.5-30.3l11-7.3 3.9 4c5.8 6 17.6 15.2 24.7 19.2 20.7 11.6 61.7 21.7 81.5 20.2 17.1-1.4 33.2-10.2 52.8-29l8.2-7.9 8.8 6.6c15.9 12 38.8 21.5 65.8 27.4 15.1 3.3 37.7 3.7 51.8.9 39.8-7.9 77-33.1 113.8-77l8.2-9.8 1.9 5.8c3.4 10.2 10.6 23.6 17.2 32 19.8 25 49.5 38.3 85.8 38.3 20.3 0 34.6-3.7 54.5-14l11.1-5.8 1.9 2c8.4 9.6 31.6 22.9 46.6 26.7 23.4 6 48.7 5.2 72.7-2.5 4.1-1.3 8-2.4 8.6-2.4s2.7 2.5 4.7 5.5c13.6 20.4 36.8 32.5 62.2 32.5 31.5 0 65.4-17.1 102.2-51.7l6.2-5.8-1.4-13.5c-2.9-27.6-1.5-52.3 5-84.5 4.7-23.5 9.9-41.8 22.1-78.5 20-59.7 21.9-68.3 21.1-90.5-.3-8.5-1.1-15.9-2.1-18.8l-1.5-4.7 4.7-4.2c20.9-18.6 40.1-44.9 48.3-66.3 6.2-15.9 8.5-36.1 5.8-51.1-7.9-44.9-46.2-74.6-96.4-74.7-13 0-19.6 1-31.9 5-7.8 2.5-8 2.5-9.3.7-10.7-14.9-14.6-19.8-21.6-27.1-24.4-25.6-55.2-43.4-90.9-52.8-26.3-6.8-48.3-9.5-78.3-9.5-47.9 0-80.7 7.3-119.5 26.6-26.4 13.2-45.1 26.6-65.7 47.1l-12.2 12.1-47.8-9.9c-172.8-35.9-219.8-45.3-281.8-56.3-96-16.9-177.8-27-235.5-29.1-32.3-1.1-41.9-1.2-54.5-.5M888 49.5c95.5 7.5 198.2 24.3 383 62.6 37.1 7.7 87.2 18.1 111.3 23.1l43.9 9 12-12.6c37.1-39 84.1-63.6 137-71.7 17.2-2.6 51.7-3.6 69.4-2 59.9 5.5 99.7 22.6 133 57.3 10.1 10.5 19.9 24 24.2 33.3l2.3 5 8.7-4.2c13.6-6.7 19-7.8 38.2-7.8 19.6.1 27.8 1.8 43 9.2 8 4 10.9 6.1 18.5 13.7 7.4 7.4 9.9 10.7 13.2 17.6 6.3 12.9 7.7 19.3 7.7 34.5 0 9-.5 15.1-1.8 20-2.7 10.9-13.4 31.4-22.8 44-8.8 11.8-25.3 28.9-33.1 34.5-3.4 2.4-4.6 3.8-4.1 4.9 5 11.2 5.9 15.6 5.9 30.1 0 19.1-1.8 27-15 66.5-17.9 53.7-23.5 73.3-28.9 101-5.3 27.8-7 57-4.5 80.6l1 10.4-4.3 3.8c-20.2 18-31.7 26.2-47.3 33.7-42.6 20.8-80.5 11.2-98-24.7-2-4-3.7-7.3-3.9-7.3s-3.8 1.6-8.1 3.6c-15.1 7-39.6 11.3-56.5 10-27.8-2.2-49-12.4-65-31.1-2.4-2.8-4.6-5.1-5-5.3-.3-.1-3.8 1.9-7.6 4.4-9.1 6.2-22.7 12.6-33.9 16-8.3 2.6-10.3 2.8-26.5 2.8-15.2 0-18.7-.3-26.5-2.3-32.1-8.3-53.8-28.8-63.8-60.6-2.2-7-4.7-20.8-4.7-26.4 0-7.4-1.3-8-5.1-2.5-13.3 19.5-35 45.6-48.6 58.4-47.6 44.7-93.3 56.4-149.2 38.1-21.3-7-34.3-14.4-47.5-27.1l-7.8-7.6-14.2 14.2c-30.2 30.2-43.7 34.1-83.3 24.4-20-4.9-35-11.2-46.9-19.6-6.3-4.4-19.2-17.5-22.1-22.4l-1.6-2.8-8.6 6.6c-16.5 12.6-31.9 21.3-50.1 28.6-49.1 19.4-113.4 19.1-156.5-.8-23.9-11-44.8-31-56.7-54.2-4.6-8.9-10.8-25.8-10.8-29.4 0-1.7-.5-1.7-6.2 1-3.5 1.6-10.6 4.1-15.8 5.6-8.2 2.3-12 2.7-28 3.1-16.2.4-19.8.2-28.8-1.6-12.8-2.7-21.3-6-30.5-12.1-14.2-9.2-25.5-25-29.7-41.6-1.1-4.3-2.2-7.9-2.4-8.1s-4.4 5.6-9.4 12.9c-18.9 28.1-40.1 64.2-44.6 76-14.7 39.2-41.7 61.7-77.2 64.4-15.5 1.2-36.3-2.9-47.9-9.4l-5-2.8-7.5 3.7c-16.2 7.9-38.7 15-59.5 18.5-14.1 2.5-44.4 2.4-58 0-44.6-8-77.3-31.3-96.2-68.2-5.9-11.5-9.4-21.7-12.5-35.8-1.9-8.8-2.2-13.2-2.3-33.1 0-20.2.3-24.5 2.4-35 4.8-24.6 12.3-45.5 25.5-71.1 17.5-33.8 35.7-58.7 60-82.1 28.8-27.7 59.9-45 91.1-50.9 12.1-2.2 35-1.5 44.5 1.4 27.3 8.2 54 27.4 70.4 50.5l4.1 5.9 3-3.9c1.7-2.1 6.6-7.6 11-12.1 19.1-19.9 37.3-28.8 61.6-29.9 17.7-.9 31.6 2.6 43.5 10.7 6.1 4.3 14.9 13.3 18.1 18.7 1.3 2.3 2.8 4.3 3.2 4.3.5 0 5-2.6 10.2-5.7 29.8-18 59.8-24.6 89.8-19.8 15 2.5 31.2 8.7 40.1 15.3 2.1 1.7 7.9 3.8 15.5 5.8 6.6 1.8 12.1 3.1 12.3 2.9.8-.8-9.7-15.6-12.2-17.1-5.3-3.1-17.3-15.3-22.5-22.9-15.9-23-23.3-49-23.4-82.5-.1-19.1 1.3-29.4 5.9-44.1 11.5-36.8 38.6-65.3 75.9-79.8C753.9 54 769 50.6 793 47.6c8-1 75.8.3 95 1.9"/><path d="M793 82.1c-46.2 5.3-77.3 23.9-92.5 55.3-10.7 21.9-13 49.4-6.5 76.5 6.8 28.1 22.1 47.5 43.4 54.9 6.1 2.1 8.3 2.4 16.6 2 15.2-.7 21.6-4.8 36.5-23.7 18.2-23.2 31-31.7 56.5-37.3 9.9-2.1 12.5-2.3 45-2.2 61 .1 128.8 6.7 333.5 32.5 52.3 6.5 108.6 13.6 125.3 15.7 16.6 2 30.2 3.9 30.2 4.2s-3.9 7.6-8.6 16c-17.3 31.2-32.8 63.4-63 131.5-30.4 68.8-46.7 98.6-62.3 114.2-4.3 4.3-9.5 8.3-12.8 9.9-6.4 3.2-15.5 4.3-17.9 2.3-6.3-5.3-9.9-26.6-7.5-45.8 1.6-12.7 7.4-34.8 16.9-64.1 15.4-47.1 18.6-60.2 18-73.5-.5-10.3-2.3-14.8-8.5-20.6-4.7-4.5-10.1-7.3-17.1-8.8-18.1-3.8-34.8 2-48.6 17-8.4 9-13.9 18-22.2 35.9-18.3 39.8-49.5 102.1-62.2 124.5-7.8 13.7-18.9 30-22.7 33.3-8.8 7.8-15.1.5-17.7-20.5-2.1-17.5.7-35.1 15.7-96.3 14.9-61.2 15.7-65.1 15.3-75.5-.5-10.3-3.3-17.2-9-22-13.1-11.1-36.8-11.8-52.8-1.5-5.9 3.8-16.4 14.7-21.7 22.5-15.9 23.7-27.1 61.9-30.3 104-.6 8.2-1.4 16-1.6 17.3-.7 3.4-24.3 26.6-36.1 35.4-17.1 12.9-33.5 21.4-48.8 25.4-9.3 2.4-29.1 2.4-38.9 0-9.9-2.4-18.9-7.1-24.9-13-5.3-5.2-10.7-14.2-12.5-20.6l-.9-3.5 18.6-.1c20.9 0 32.2-1.6 48.1-6.6 36.5-11.6 62.4-37.1 70-68.9 6.6-27.9 1-54.7-15.2-72.5-17.1-18.6-42.7-27.3-76.6-26.1-16.2.6-24.4 2.3-38 8.1-47.6 20-88.9 75.5-101.8 136.8-3.1 15.1-4.3 40.9-2.5 53.9 6.3 44.2 31.7 71.5 76 81.5 29.8 6.7 66.2 4.8 96.1-5.2 29.5-9.8 58-29.5 81.6-56.4 4.8-5.5 9.1-10 9.6-10 .4 0 .8.7.8 1.6s1.5 6.3 3.4 11.9c9.2 27.5 22.5 43.8 41.6 51.4 11.6 4.5 24.3 8 35.6 9.7 9.1 1.4 10.6 1.4 13.8 0 12.5-5.2 33.3-26.8 60.6-63l10.5-13.9 1.1 6.9c6.6 38.9 24.1 56.3 67.5 67.4 12.6 3.2 22.1 4.1 32 3 35.1-4 69.7-29.4 105.4-77.4 23.6-31.8 42.3-62.6 84.3-139.1 28.9-52.8 38.2-69.3 50-89.3l9.7-16.3 4.5.5c17 2.2 91.1 10.8 103 12 33.2 3.5 57.4 3.1 87.3-1.4 42-6.4 73.1-20.2 89.2-39.7 27.5-33.4 17.6-81.6-23.8-114.8-21.3-17.2-43.8-26.9-75.2-32.4-51.3-8.9-100.9-3.5-140.3 15.4-30.4 14.6-54 33.5-75.4 60.2-5 6.2-9.1 11.3-9.3 11.3-.7 0-76.4-15.6-130.8-27-214.4-44.7-316.9-62.2-412.7-70.5-39.8-3.4-85.9-4.5-104-2.4m845.6 65.4c23.9 4.2 42.3 13.2 55.2 26.9 15.9 17 12.5 33.5-8.4 40.4-16.3 5.5-47.9 5.5-85.9.1-25.1-3.6-66.1-13.8-67.2-16.6s6.8-16.3 13.5-23.5c13.4-14.2 30.1-23.3 49.8-27.2 9.6-1.9 32.1-1.9 43-.1M862.8 372.7c4.6 2.2 8.4 8.2 9 14.2.5 5.5-2.3 16.3-6.3 24.1-10.6 21-33.7 35.7-59 37.6-10 .8-9.7 1.4-6.3-11 8.4-31.4 28-59.4 46.1-65.8 5.8-2 10.9-1.7 16.5.9m-571.3-76.2c-32.6 7.1-66.2 31.1-95.2 68-14.7 18.7-32.6 50.2-41.8 73.5-7.9 20.2-12.5 43.5-12.5 63.5 0 55.2 30.8 93.8 82.3 103.2 12.6 2.3 36 2.2 49.2-.1 24.4-4.4 48-13.6 70.1-27.3l8.2-5.1 1.7 3.8c7.9 17.4 37.9 24.2 59.5 13.4 9.7-4.9 18.3-14.3 24.3-26.5 2.5-5.2 7.4-15.5 11-22.9 23.7-49.1 81-129.6 110.8-155.7 10.3-9 16.5-12.5 22.6-12.6 4 0 5 .4 7.5 3.2 2.3 2.5 2.8 4.1 2.8 8.1 0 4.2-1 6.8-6 16.3-18.3 34.1-23.1 46.8-23.2 62.2-.3 20.3 9.8 34.4 29 40.6 10.3 3.3 32.7 3.8 44.4 1 25-5.9 42.3-21.4 50.2-44.7l2.5-7.4h-2.3c-5 0-13.1-3.3-16.4-6.6-3.4-3.4-3.5-3.5-3-10.7.4-6 1.4-9.4 5.7-19.3 13.6-30.9 16.4-42 14.4-56-3.6-24.8-23.9-40.9-54.4-43.1-21.6-1.5-44.6 6.3-68.9 23.5-10.8 7.7-35.2 31-45.5 43.6-3.8 4.6-7.3 8.5-7.6 8.5-.3.1 1.6-6.4 4.3-14.4 7.9-23.4 8.5-35.7 2.5-48.9-10.6-23.1-39.8-26.3-64.8-7-14.7 11.3-28.7 33.4-48 75.9-14.2 31.4-30.1 71.5-38.8 97.7-3.1 9.4-3.5 10-10.4 16.3-13.2 12.3-33.1 23.9-50.7 29.9-14.8 4.9-27.3 6.9-44 7-14.3.1-15.9-.1-20.8-2.3-7.5-3.4-14.4-11.3-17.2-19.6-5.8-17.4-3.9-48.9 4.3-70.6 7.7-20.4 30.4-58.4 48.4-80.9 15.4-19.4 35.6-37 42.3-37 1.5 0 3.5.9 4.5 2 2.7 3 1.5 9-4.5 21.5-7.2 15.2-15.3 36.2-18 46.9-3.3 13.2-2.6 23.7 2.1 33.7 6.2 12.9 18.5 24 30.7 27.5 9.9 2.9 13.9-.3 19.7-15.3 2.1-5.4 7.7-18.1 12.6-28.3 15.1-31.8 19.1-44.4 19.2-60 .1-15.2-4.5-25.6-17.6-39.6-12.8-13.7-30.5-25.1-45.2-28.9-7.2-1.8-21.7-1.9-30 0"/></g><path fill="#FFF" fill-opacity=".6" d="M1437.9 110.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-11.5 11.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-1244.9 185-5 5.3 5.3-5c4.8-4.6 5.7-5.5 4.9-5.5-.1 0-2.5 2.4-5.2 5.2m1719.4 4.5-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-793 306c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m572.1 29.6c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9"/><path fill="#FFF" fill-opacity=".1" d="m681.9 78.7-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4M1785.5 102c2.7 2.7 5.1 5 5.4 5s-1.7-2.3-4.4-5c-2.7-2.8-5.1-5-5.4-5s1.7 2.2 4.4 5m137 51c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M541 278.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-149 3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m155 3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-136 7.1c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m-9-.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m1506.9 12.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-560 274c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M689 596.3c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m-554 16.2c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m1406 0c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5"/><path fill="#FFF" fill-opacity=".2" d="M238.3 655.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m18 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5"/><path fill="#FFF" fill-opacity=".3" d="M544 281.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-146 6.1c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m-209.1 12.2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1717 7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-1731.5 7.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m1721.5 2.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-551.5 265.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m-8.5 8.5-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4"/><path fill="#BFBFBF" d="M814.2 80.7c3.7.2 10 .2 14 0 4-.1 1-.3-6.7-.3s-11 .1-7.3.3m-18.9 1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m57.5 0c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4m-66 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m85 0c1.2.2 3.2.2 4.5 0 1.2-.2.2-.4-2.3-.4s-3.5.2-2.2.4m-91 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m105.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m12 1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m11 1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m11 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m9 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m9 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m659 0c2.3.2 6.1.2 8.5 0 2.3-.2.4-.4-4.3-.4s-6.6.2-4.2.4m-651 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m632.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m42.5 0c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4m-667 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m615 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m63 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-671 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m601 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m78 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-671 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m587 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m90 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-670 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m575 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m101 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-669 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m674 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-668 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m7 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m7 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-310.9 2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m317.9-1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-334.9 3-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m345.9-1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m11 2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m339.1 2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-741 2-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m412.9-2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m11 2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m363.6.9c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m-358.6.1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m586.2.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-565.2 3.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m566.7 1.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-561.7-.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m304.1 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-299.1 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m285.1 1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-280.1 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m223.1 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m317.1.8c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-535.2-.8c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m264.6 2.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-235.6 3.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m221.7 9.5-10 10.3 10.3-10c9.5-9.3 10.7-10.5 9.9-10.5-.1 0-4.7 4.6-10.2 10.2m-216.7-8.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m508.2 1.8c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-503.2-.8c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m348.5 3c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m25 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-358 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m482.2-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-477.2 1.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m19 4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m359.3 1.7c1 .9 8.9 4.6 8.9 4.1s-7.5-4.5-8.6-4.5c-.4 0-.5.2-.3.4m-975 3.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m634.7-.9c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m19 4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-654.7 1.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m659.7-.9c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m102.6 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m98.5 3.5-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-182.1-1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m317.7 1.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-312.7-.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m69.1 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M690.2 173c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m755.2 2.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-41.6-.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m134.1 2-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m152.6-.7c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-281.7-.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m18.7.4c-.4.6-1.7 1.4-2.9 1.9-1.4.6-1.6.9-.5.9.9.1 2.4-.8 3.4-1.9s1.5-2 1.2-2-.9.5-1.2 1.1m335.6 3.5c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m1 5c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M690.2 193c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m1013.1 1.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m77 4.5c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3m-79.2 3.7c-1 2.6-1 2.7.4.9.8-1.1 1.5-2.3 1.5-2.8 0-1.5-.7-.8-1.9 1.9m-1010-.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m1 6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m175.7-.9c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4m35 0c2.3.2 6.1.2 8.5 0 2.3-.2.4-.4-4.3-.4s-6.6.2-4.2.4m-47 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m71.5 0c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m638.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-622 1c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4m835.3.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-821.8.1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m617.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-604.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m609.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m111.2.3c-.9.6-1 1-.3 1 .6 0 1.5-.5 1.8-1 .8-1.2.4-1.2-1.5 0m-709.7.7c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m603.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-592.5 1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m598.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-587.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m593.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-583.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m590.5 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-580.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m588.5 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-790.4 1.9c-1.8 1.4-1.8 1.4.6.3 3.1-1.4 3.6-1.9 2.4-1.9-.6 0-1.9.7-3 1.6m211.9-.9c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m589 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m50.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-630.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m600.5 0c4.5.2 11.9.2 16.5 0 4.5-.1.8-.3-8.3-.3s-12.8.2-8.2.3m-591 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-297.9 3-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m305.9-2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M1771 233c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-608.2-.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-372.4 3.5-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m380.4-2.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M702 238c.6 1.1 1.3 2 1.6 2 .2 0 0-.9-.6-2s-1.3-2-1.6-2c-.2 0 0 .9.6 2m492.8-1.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m546.6 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-538.6-.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-441.9 2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m449.9-1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m7 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m494.6 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M707 246.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m82.9 1.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m483.9-1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M714.5 255c2.7 2.7 5.1 5 5.4 5s-1.7-2.3-4.4-5c-2.7-2.8-5.1-5-5.4-5s1.7 2.2 4.4 5m591.3-4.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-519.9 2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m527.9-1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m439.6 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-431.6-.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m7 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-565.3 4.5-4 4.3 4.3-4c2.3-2.1 4.2-4 4.2-4.2 0-.8-.9 0-4.5 3.9m573.3-3.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-656.8.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m1014 4.1c-2.5 1.3-4.1 2.4-3.7 2.5 1.1 0 8.7-3.9 8.7-4.5 0-.7 0-.7-5 2m-1010-1.1c0 .3.9 1 2 1.6s2 .8 2 .6c0-.3-.9-1-2-1.6s-2-.8-2-.6m44 4.1c-1.4.8-2 1.4-1.4 1.5.5 0 1.9-.7 3-1.5 2.4-1.8 1.6-1.9-1.6 0m-20.2 4.2c1.7.2 4.7.2 6.5 0 1.7-.2.3-.4-3.3-.4s-5 .2-3.2.4m732.5 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m9.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M1371 277c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m145.8-1.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m9 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m113.5 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-104.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m97.5 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-88.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m81.5 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-71.5 1c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m62 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m-49 1c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m36.5 0c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4M1365 288c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-830.5 5c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M388 298.5c3.5 3.6 6.7 6.5 6.9 6.5.3 0-2.4-2.9-5.9-6.5s-6.7-6.5-6.9-6.5c-.3 0 2.4 2.9 5.9 6.5m1078-3.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-1160.2-.3c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4m1589.1 2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-1681 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-5 4c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m1676 5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-1684.5 2.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m283.9-1.5c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-87.3.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m79.8.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m366 0c2.9.2 7.4.2 10 0s.2-.3-5.3-.3-7.6.1-4.7.3m190.5 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m9 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-211.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m33 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m164 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-157 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-413.4 1.9c-1.8 1.4-1.8 1.4.6.3 3.1-1.4 3.6-1.9 2.4-1.9-.6 0-1.9.7-3 1.6m47.6 2.9c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m116.8-1.8c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M1454 316c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-838.2-.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m21 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-180.9 2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m611.6.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-704 4c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m639.5 2.7-6.5 6.8 6.8-6.5c3.7-3.5 6.7-6.6 6.7-6.7 0-.8-1 .2-7 6.4m-551.1-4c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m752.9-1c1.2.2 3.2.2 4.5 0 1.2-.2.2-.4-2.3-.4s-3.5.2-2.2.4m11 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-556.7.7c.2.2 1.5.9 2.9 1.5 2.4 1.1 2.4 1.1.6-.3-1.6-1.3-4.9-2.4-3.5-1.2m413 2.6c1.4 3.1 1.9 3.6 1.9 2.4 0-.6-.7-1.9-1.6-3-1.4-1.8-1.4-1.8-.3.6m-888.7 2.2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m262 0-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m782.6-1.8c0 .2.7.7 1.6 1 .8.3 1.2.2.9-.4-.6-1-2.5-1.4-2.5-.6m-561 1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m242 0c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-678.1 2.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1004.6 2.3c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4M674 331.5c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m243 .7c2.4 2.7 4.5 4.8 4.7 4.8.8 0 0-.9-4.4-5l-4.8-4.5zm-341.6-1.6c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m-351.5 3.1-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m215.5-.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m731.1 3-5 5.3 5.3-5c4.8-4.6 5.7-5.5 4.9-5.5-.1 0-2.5 2.4-5.2 5.2m-175.6-2.5c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-819 1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m599 0c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m301.4 6.3c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3m263.9-3.5c-1.7 3.7-1.4 4.5.4 1.2.9-1.6 1.4-3.1 1.1-3.3-.2-.2-.9.7-1.5 2.1m-904.3.2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m333.5 1.5-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m-248.3.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m40.8.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m361.1-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-365.1 3.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m881.1.3c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-275.1.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-405.4 5.5-6 6.3 6.3-6c5.8-5.5 6.7-6.5 5.9-6.5-.1 0-2.9 2.8-6.2 6.2m-206.1-2-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m-344 2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m89 1-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m946.9 2.8c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3m89.3-1.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m100.4.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-898.5 11c-6.6 6.6-11.7 12-11.5 12 .3 0 5.9-5.4 12.5-12s11.7-12 11.5-12c-.3 0-5.9 5.4-12.5 12m-14.3-9c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m165.9 3.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-399.7 2.6-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m461.5-.5-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-364.8-.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m37.9.4c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m651.1 1.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m78.9.5c-.7 1.1-1 2.3-.7 2.5.2.2.9-.7 1.5-2.1 1.4-3 .7-3.3-.8-.4m-955.1.6c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m1231.1.3c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-184.9.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-499.2 2.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-464 3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m104.2.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m356.8.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m330.2.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-218.8.1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m388.8 1.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-656.1-.2c0 .2.7.7 1.6 1 .8.3 1.2.2.9-.4-.6-1-2.5-1.4-2.5-.6m152.9 1.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m123.1-1.3c0 .2.7.7 1.6 1 .8.3 1.2.2.9-.4-.6-1-2.5-1.4-2.5-.6m462.6 1.1c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m98.4.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-1147.1.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m412.3.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m250 .5c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m-366.8.1c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m294.6-1.2c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-345.1 5.3-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m46-1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m25.2.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m237.9 3.9c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m-268.1-.8-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m581.7-.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m177 0c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m96.4 2.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-901.6 1.2-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m356.8-.2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-462.6.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m182.5.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-40.1 4.9c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m270.9.2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m49.3.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m194.9-.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m249.5-.1c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m95.4 2.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-473.8 1.5c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8M724 396c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-321.4 3.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m137.8 3.7-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m426.7-1.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-31 1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m196.1 1.9c-1.7 3.7-1.4 4.5.4 1.2.9-1.6 1.4-3.1 1.1-3.3-.2-.2-.9.7-1.5 2.1M811.5 408c-.9 1.6-1.3 3-1.1 3 .3 0 1.2-1.4 2.1-3 .9-1.7 1.3-3 1.1-3-.3 0-1.2 1.3-2.1 3m154.6-1.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-567.5 1.9c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m181.4.5c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m137.6-.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-183.7 2.2-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m140.7-1.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m290.5 2.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-156.5 1.9c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-279.2 2.7-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m434.7 1.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M1396 421c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-1098.8 1c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m227.7-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m333 0c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m105.2 2.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-441.7 1.6-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m331.5 1.5-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-335 3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m444.3 1.8c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m-115.3.2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-332 2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m780.7 1.8c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-783.7 2.2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m324.5.9c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m83.5.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m191.8 6.8c-2 4.1-3.5 7.5-3.2 7.5.7 0 7.7-14.1 7.3-14.6-.2-.2-2.1 3-4.1 7.1M961.2 443c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m422.8-1c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-875.1.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M831 442c-.8.5-1 1-.5 1 .6 0 1.7-.5 2.5-1s1.1-1 .5-1c-.5 0-1.7.5-2.5 1m-161 2.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m245.4.7-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m-117.3.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m15.7 2.1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-309.9 2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m301.4-1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m484.3.8c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-605.8 1.2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m19.3 1.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m203.3 1.6-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m53.9 2.3c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-648.8-.5c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m250.7 1c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m139.9.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-205.2 2.1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m460.5 3.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m91.7-.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-348 1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m191.3 1c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m-330.2 2.9c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m485.9.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M883 469.5c-2.5 1.3-4.1 2.4-3.7 2.5 1.1 0 8.7-3.9 8.7-4.5 0-.7 0-.7-5 2m63 6.2-8.5 8.8 8.8-8.5c4.8-4.6 8.7-8.6 8.7-8.7 0-.8-1.1.3-9 8.4m-245.8-5.2c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m346 8.5c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m48.8 0c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-420.6 1.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m25 8.8c0 5.8.1 8.1.3 5.2.2-2.8.2-7.6 0-10.5-.2-2.8-.3-.5-.3 5.3m152.4-8.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-7 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-38 1c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4m28.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m378.9 1.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m62.8 0c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-1163.9.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m937.1.4c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-112.3 1.7-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4M570.5 489c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m93.9 1.2-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9M1088 492c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m180 0c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-341.1.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m281.5 8.8c0 5.5.1 7.6.3 4.7s.2-7.4 0-10-.3-.2-.3 5.3M575 493.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M473 496c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m449.9-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m121.4 5.8c0 4.4.2 6.1.4 3.7.2-2.3.2-5.9 0-8-.2-2-.4-.1-.4 4.3m305.6-4.8c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-431 2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M219.2 501c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m416.6 2.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-35 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m30 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m277.6.9c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m171.6.4c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m-467.7-.3c2.6.2 6.7.2 9 0s.2-.3-4.8-.3c-4.9 0-6.8.1-4.2.3m87.9 1.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m110.3 0c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m532.4-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-87 4c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-440.9-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-462.1 2.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m348.2.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m636.8.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-85.5 1.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-430 .2c1.1.9 2.4 1.6 3 1.6 1.2 0 .7-.5-2.4-1.9-2.4-1.1-2.4-1.1-.6.3m222.7.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m164 1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-80 .7c-.8 1-.7 1.4.4 1.4.8 0 1.5-.6 1.5-1.4 0-.7-.2-1.3-.4-1.3s-.9.6-1.5 1.3m205.8.4c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M458 521c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m502.4.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m286 1-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m85.5-.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-490.1 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m27 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-16 1c2.3.2 5.9.2 8 0 2-.2.1-.4-4.3-.4s-6.1.2-3.7.4m278.3.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-63.2 1.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m55 1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-169 3-3.4 3.8 3.8-3.4c2-1.9 3.7-3.6 3.7-3.8 0-.8-.8 0-4.1 3.4m374-2c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-279.9-.1c0 .6.7 1.9 1.6 3 1.4 1.8 1.4 1.8.3-.6-1.4-3.1-1.9-3.6-1.9-2.4m191.9 1.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-177 3-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7M453 530c-.6 1.1-.8 2-.6 2 .3 0 1-.9 1.6-2s.8-2 .6-2c-.3 0-1 .9-1.6 2m666.9-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m115.5.9c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m88 1.6-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-107.7.9c1.2 1.2 2.4 2.1 2.7 1.8.2-.2-.8-1.3-2.2-2.3l-2.7-1.9zm-98.8.6c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m200.5 5.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m-204.5-.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M710 541.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m399.9.8c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-175.5 5.5-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m371.1 4-7 7.3 7.3-7c6.7-6.4 7.7-7.5 6.9-7.5-.1 0-3.4 3.3-7.2 7.2m-199.6-4.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-838.1 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m706.2-.1c0 .6.7 1.9 1.6 3 1.4 1.8 1.4 1.8.3-.6-1.4-3.1-1.9-3.6-1.9-2.4m127.4 5.6-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-385.4-.8c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m211.4 1.8-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M720.5 558c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m202.4-.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m173 2-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-658.2 2.8c-.9 1.9-1.5 3.5-1.2 3.5.7 0 3.7-6.1 3.3-6.6-.3-.2-1.2 1.2-2.1 3.1m543.3-2.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m164 0c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m147.9 3.3-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7M986 566.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m102.5 1.7-4 4.3 4.3-4c3.9-3.6 4.7-4.5 3.9-4.5-.2 0-2.1 1.9-4.2 4.2m198.9 0-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-555.9.8c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m604.9.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-181.4 1.2c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m127.9 1.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-384.5 1.9c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1m95.1.4c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M425 580.7l-5.5 5.8 5.8-5.5c5.3-5 6.2-6 5.4-6-.1 0-2.7 2.6-5.7 5.7m317-4.3c0 .3.9 1 2 1.6s2 .8 2 .6c0-.3-.9-1-2-1.6s-2-.8-2-.6m530.4 2.2c-1.8 1.4-1.8 1.5.4.4 1.2-.6 2.2-1.3 2.2-1.5 0-.8-.5-.6-2.6 1.1M358.5 584c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m714.9-.8-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2M1006 582c.8.5 2 1 2.5 1 .6 0 .3-.5-.5-1s-1.9-1-2.5-1c-.5 0-.3.5.5 1m258 1c-1.9 1.1-3.1 1.9-2.7 2 1.1 0 6.7-2.9 6.7-3.5 0-.7-.1-.7-4 1.5m57.9.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-956.5 5.7c1.1.9 2.4 1.6 3 1.6 1.2 0 .7-.5-2.4-1.9-2.4-1.1-2.4-1.1-.6.3m47.6.1c-1.4.8-2 1.4-1.4 1.5.5 0 1.9-.7 3-1.5 2.4-1.8 1.6-1.9-1.6 0m701.9.2c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-336.1 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m72 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-67 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m61 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-54 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m46 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m205 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m195 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-860 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m424.5 0c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m23 0c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m224 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m167 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m12 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-844.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m14 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5M143 599.3c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m1407 1.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-1312.2 6.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m20 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-19.5 34c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m18 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5"/><path fill="gray" d="M797.8 47.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m51.5 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m20.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m729 9c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m37 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-919.9 56-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7M1754 139.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-206.6 33.8-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m144.1-.2c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-1035.3 3.5c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m0 13c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m1276.9 34.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M295.8 261.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m6 33c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m10 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-119.8 23-5.5 5.8 5.8-5.5c3.1-3 5.7-5.6 5.7-5.7 0-.8-1 .1-6 5.4m100.9 37c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m264 32c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-11 11c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-248.8 20.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m614.3 31.6-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m-241.5 35.5-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7m-449.8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M108.2 489c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m33.1 13c0 4.7.2 6.6.4 4.2.2-2.3.2-6.1 0-8.5-.2-2.3-.4-.4-.4 4.3m-33.2 12.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m1270.1 3.4c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-433.7 21.2-6 6.3 6.3-6c5.8-5.5 6.7-6.5 5.9-6.5-.1 0-2.9 2.8-6.2 6.2m369.4 4.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-1062.1 4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m10.5 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5M726 564.5c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m422-.2c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m679.3 9.2c0 4.4.2 6.1.4 3.7.2-2.3.2-5.9 0-8-.2-2-.4-.1-.4 4.3M988 569.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m93 7.1c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m-911 2c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m90.8 28.2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m549 21c2.8.2 7.6.2 10.5 0 2.8-.2.5-.3-5.3-.3s-8.1.1-5.2.3m242.5 0c1.5.2 3.9.2 5.5 0 1.5-.2.2-.4-2.8-.4s-4.3.2-2.7.4m166.4 0c1.8.2 5 .2 7 0 2.1-.2.7-.4-3.2-.4-3.8 0-5.5.2-3.8.4m396.6 0c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m-1231.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m13 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-151 12c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4"/><path fill="#804040" d="M804.3 80.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m33 0c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m25.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m16 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m726.5 8c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m22 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m16.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M1443 126.5c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m23.5 23.7-5 5.3 5.3-5c2.8-2.7 5.2-5.1 5.2-5.2 0-.8-.9.1-5.5 4.9m141.3-3.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m16 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-741.5 61c3.2.2 8.1.2 11 0s.3-.3-5.8-.3c-6 0-8.4.1-5.2.3m38.5 1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m18 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m14 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m13 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m12 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m11 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m640 4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M713 254.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-22 25.1c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m911.8 6.2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m48 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-27 1c2.3.2 6.1.2 8.5 0 2.3-.2.4-.4-4.3-.4s-6.6.2-4.2.4m-1001.5 28c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m8.5 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m371.6 9.5-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m-786.9 17-5 5.3 5.3-5c4.8-4.6 5.7-5.5 4.9-5.5-.1 0-2.5 2.4-5.2 5.2m306.7 5.8c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m13.3 15.2-6 6.3 6.3-6c5.8-5.5 6.7-6.5 5.9-6.5-.1 0-2.9 2.8-6.2 6.2m152.7-.2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 7c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m176.8 6.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m73.3 10.1c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-386.8 6.7-4 4.3 4.3-4c2.3-2.1 4.2-4 4.2-4.2 0-.8-.9 0-4.5 3.9m300.4 35.5-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-52.1 21c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m160.4 2.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-7.3 16.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-253.8 8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m115.7 8.1c1.7.2 4.7.2 6.5 0 1.7-.2.3-.4-3.3-.4s-5 .2-3.2.4m-115.7 16.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-339.7 4.6-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m450.6-.8c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-263 7.9c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m698.9 8.4-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-294 10c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M230.5 538c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m706.4 8.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m150 24c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-662 11-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-67.9 1.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m1046 2.9c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m-441 4.1c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-165.2 3.3c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m307.7 6.5-6 6.3 6.3-6c3.4-3.3 6.2-6.1 6.2-6.2 0-.8-1 .1-6.5 5.9M967 595.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-518.6 9.8-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9"/><path fill="#404040" d="M801.8 47.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m39 0c1.2.2 3.2.2 4.5 0 1.2-.2.2-.4-2.3-.4s-3.5.2-2.2.4m24.5 1c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m16.5 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m13 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m12 1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m696.5 6c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m28 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5M656.2 183c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m1277.1 33.5c0 3.3.2 4.5.4 2.7s.2-4.5 0-6-.4 0-.4 3.3M298.8 261.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m16 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m178 13c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m127 7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m14 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-430.9 26c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m19 27c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m101.2 8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-114.2 4.1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m77 15c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-65.8 127.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-79 1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-32.9 2.4c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m0 16c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m33 2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m115.1 35.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5M1827.2 563c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 21c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M146 601.5c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m1319.8 13.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-631 12c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-446 2c1.2.2 3.2.2 4.5 0 1.2-.2.2-.4-2.3-.4s-3.5.2-2.2.4m-125 11c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m1468 17c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m8 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><g fill="#400000" stroke-width="0"><path d="M795 48.1c-53.1 4.4-93.2 25-116.5 59.9-10.5 15.7-16.9 32.5-20 52.5-2.3 14.6-1.6 41.3 1.5 56 6.7 31.6 20.2 54.9 41.2 71.2 3.8 2.8 7.5 7.1 10.7 12.1 2.7 4.2 4.8 7.8 4.7 8-.2.1-5.8-1.2-12.5-2.9-9.4-2.5-13.8-4.3-18.9-7.6-27.5-17.7-66.4-20.7-100.8-7.8-8.8 3.3-23.6 10.8-30.5 15.6-3.2 2.1-6.2 3.9-6.7 3.9s-2.4-2.4-4.2-5.3c-21-32.6-67.9-39.1-104.5-14.4-8.8 5.9-16.3 12.7-22.5 20.3-3 3.7-6.4 7.8-7.5 9.2l-2.1 2.4-5.9-7.8c-16-21.5-40.4-39.4-64-47.3-52.8-17.5-119.7 15-170.5 83-4.8 6.4-11.4 15.9-14.6 21-8.5 13.7-22.6 41.7-27.8 55.2-20.7 53.4-19.9 108.8 2.2 148.6 12 21.6 34.4 42.5 56.3 52.4 18.5 8.5 36.3 12.5 58.9 13.4 31.1 1.2 60.8-4.9 92.2-18.7l11.1-4.9 8.1 3.8c11.5 5.5 19 7.2 34.6 7.8 15 .6 24.4-.8 35.1-5.1 20.8-8.4 38.5-27.1 48.8-51.4 6.3-14.7 11.5-24.8 23.7-45.2 11.4-19.1 32.5-51.3 34.4-52.5.6-.4 1 .4 1 1.7 0 4.6 3.1 14.4 7 22.4 7.3 14.9 21.5 27.7 38.3 34.4 25.2 10 63.4 8.9 88.6-2.6 3-1.3 5.6-2.3 5.7-2.1.2.2 1.5 4.3 2.9 9.3 7.5 25.9 25.3 50.5 47.7 65.4 25.5 17.1 55.7 25 95 25 17.1 0 28.1-1 44.2-4.1 29.2-5.5 62.5-21 86.6-40.4l6.6-5.2 3.8 5.3c15.1 20.8 36.3 33 71.2 40.9 16.9 3.9 31.9 4.5 41.1 1.7 11-3.4 21-10.6 37.4-27.1l14.7-14.6 5.9 6.1c14.6 15 35.6 25.9 63.2 32.8 13.4 3.4 22.4 4.6 33.9 4.6 41.5 0 81.6-22.2 120.1-66.6 9.3-10.7 24.2-30 30.7-39.8l3.9-5.9.7 8.4c3.9 46.8 27.1 77.6 66.3 87.8 30.6 8.1 59 3.2 87-15 5.3-3.4 10-5.8 10.4-5.3s2.5 3.1 4.7 5.8c9.1 11.5 28.6 23.5 44.9 27.7 10.5 2.7 31.3 3.6 43.8 1.9 11.5-1.5 23.7-5 33.5-9.5 4.1-2 7.6-3.4 7.6-3.3.1.2 2 4.2 4.4 8.9 14.6 29.5 45 41.1 79.4 30.3 20.3-6.4 39.8-18.6 62.7-39.1l5.3-4.8-1.3-17.7c-3.2-43.8 4.3-83.9 30.6-163.8 16.4-50 18.7-59.3 18.9-76.5 0-12-1.9-21.8-5.9-29.5-1.3-2.5-1.1-2.8 5.7-8 18.1-14.2 37.9-39.1 47.7-60.1 6.4-13.5 8.4-23.8 7.9-39.4-.4-10.9-.9-14.3-3.3-21.4-6.5-19.2-21.6-35.3-40.6-43.3-12.8-5.3-22.9-7.2-38.9-7.1-12.7 0-15.6.3-23.3 2.7-4.8 1.4-11.6 4.1-15 5.9-3.4 1.7-6.5 3.3-6.9 3.5-.3.2-2.4-3-4.5-7-14.5-27.7-41.5-52.5-74.5-68.3-50.6-24.3-131-28.1-189.4-9-21.9 7.2-47.8 20.8-68.5 36-6.2 4.6-16.8 14.1-26.1 23.5-11.2 11.4-16.2 15.7-17.5 15.4-9.4-2.2-187-38.8-225.3-46.4-157.6-31.5-258.6-46.1-346.5-50-25.3-1.1-42.5-1.1-57.5.1M856.1 81c40.9 2.1 82.3 6.2 133.8 13.6 80.7 11.4 150.7 24.5 326.1 60.9 125 26 124.1 25.8 124.9 24.9.4-.5 4.5-5.6 9.2-11.4 34-42.7 79.5-68.8 133.4-76.7 18-2.6 54.3-2.3 73 .5 44.8 6.9 70.8 19 96 44.7 12.1 12.3 19.4 23.6 24.4 37.8 3.4 9.8 3.6 10.8 3.5 23.7 0 12.2-.2 14.3-2.8 21.5-6.8 19.5-19.5 33.1-41.4 44.3-23.1 11.8-52.7 18.6-91.8 21.2-25.4 1.7-39.4.6-127.8-10-18.7-2.3-34.5-3.9-35.1-3.6-1.7.6-19.3 30.8-41.8 71.6-41.4 75.1-46.4 84.2-57.3 102.6-35.2 59.7-62.8 96-90.4 118.8-35.7 29.6-66.9 35.5-109 20.7-30.6-10.8-45.5-28-51.5-59.4-.8-4.4-1.5-8.1-1.5-8.4s-4.5 5.4-10 12.7c-25.9 34.1-45.4 55-57.2 61.1-6.8 3.5-21.2 2-41.2-4.2-19.2-5.9-29.1-12.2-38.1-24.1-6.1-8-13.6-23.7-17-35.2-1.4-5-2.7-9.1-2.8-9.3-.2-.1-3.2 3.2-6.8 7.4-8.7 10.3-23.4 24-34.3 32.2-22.7 17-44.1 26.6-71.3 32.2-14.3 2.9-46.8 3.7-61.9 1.5-40.4-5.9-66-22.6-80-52.1-22.7-47.8-7.1-123.7 36.3-177.6 3.4-4.1 10.6-11.8 16-17 27-25.7 54.3-37.8 85.8-37.9 20.6 0 42.6 5.1 56.2 13.1 8.2 4.8 21.2 18 25.5 25.9 14.2 25.7 11.9 63.2-5.5 89-9.7 14.5-23.7 26.2-41.2 34.6-20.7 9.9-38.6 13.7-65.7 13.7h-17.7l1.9 4.9c4.4 11.5 13.1 21.3 23.3 26.1 17.9 8.5 41.7 8.7 62.7.6 20.4-7.8 49.2-28.8 66.1-48.2l6.6-7.5.7-9.8c3.3-48.1 14.3-87.3 31.7-113.1 13.3-19.7 28.6-29.2 46.9-29.3 23.3 0 36.7 10.8 37.8 30.5.5 10-.8 16.4-13.8 69.6-11.2 45.6-15.3 64.6-17.1 79.9-2.3 19.8.8 39.1 7.2 44.4 2.9 2.4 3.2 2.5 5.9 1.1 4.8-2.5 14.3-15.9 26.5-37.4 12.8-22.7 42.7-82.5 61-122.3 8.9-19.3 16.1-30.2 25.8-39.5 12.2-11.5 24-16.1 38.8-15 14.7 1.2 25.2 7.1 30.4 17.2 2.8 5.5 3 6.5 2.9 16.9 0 9.7-.6 13.1-4.2 26.6-2.2 8.5-8.2 28.1-13.2 43.5-14.4 44.1-18 60.1-18 78.8 0 15.6 4.1 29 9.5 31.5 3.3 1.5 11.1-.3 17.3-4.1 19.1-11.5 36.6-40.8 72.7-122.2 30.8-69.4 45.5-100.3 62.9-131.4 4.7-8.5 8.5-15.6 8.3-15.7-.6-.6-269.2-33.9-309.2-38.3-89.7-10-128-13.1-171.5-13.8-36.8-.6-48.3.4-64.5 5.8-17.1 5.7-27.7 13.7-42.3 32.2-15.5 19.6-23 24.7-37.3 25.5-21.9 1.3-41.2-11.4-53.3-34.8-8.9-17.2-12.1-34-11.3-58.4.6-18.5 2.5-27 8.8-40.5 15.3-32.8 48.3-52.4 96.2-57 12.2-1.2 37.7-1.2 60.8-.1m1007.7 94.6c19.9 4.1 32.2 15.2 35.8 32.5 3.9 18.7-6.5 42-29.6 66.3-21.9 22.8-39 30.8-48.4 22.6-7.6-6.7-14.2-21.4-18.7-42-1.9-8.3-2.4-13.6-2.4-25 0-16.6 1.7-23.3 8.4-33.7 11.8-18.1 31.4-25.5 54.9-20.7M328.1 297.9c12.2 4.5 23.5 11.8 34.4 22.3 9.7 9.2 16.8 18.7 20 26.8 1.1 2.8 2 8.3 2.3 14.6.8 17.2-3.2 30.8-18.4 62.4-5.1 10.7-10.9 23.8-12.9 29-4.3 11.3-7.3 15.6-12 16.9-7.4 2-19-3.2-29.1-13.3-10.3-10.2-15.4-21.8-15.4-34.6 0-11.6 7.1-33.6 19.1-59.1 7-14.9 8.4-21 5.7-23.7-7.2-7.2-36.9 20.6-61.7 57.7-14.4 21.5-28.6 48-33.4 62.3-7.5 22.2-8.6 51.4-2.6 67.3 2.7 7.2 9.5 14.6 16.1 17.6 4.9 2.2 6.6 2.4 21.3 2.4 13 0 18-.5 26.4-2.3 21.4-4.8 39.8-13.1 56.3-25.4 14.2-10.7 17.4-14.1 19.9-21.7 10.3-31.4 31.5-83.6 47.3-116.6 19.1-39.8 33.4-58.4 52-67.6 28.5-13.9 54.5-2 58.6 26.9 1.5 10.6-.8 23.6-7.6 42-2.3 6.4-2.3 6.6-.3 4.2 18.7-22.4 37.3-39.9 54.9-51.7 21.3-14.3 44.8-21.5 64.4-19.8 31.2 2.7 51.1 18.7 54.8 44.3 1.6 10.9-.4 22.2-6.6 37.1-2.5 6.1-6.7 16.1-9.2 22.3-7.2 17.3-6.4 24.7 3.2 28.9 2.4 1.1 6.3 2.3 8.7 2.8 3.9.6 4.2.9 4.1 3.7-.2 4.9-6.1 17.5-11.6 24.9-6.5 8.5-15 15.5-24.3 20-10.2 4.9-17.4 6.5-31.5 7.2-38.5 1.7-59-13.4-59-43.6 0-15.8 4.3-28 19.5-56.2 7.6-13.9 9-17.1 9.3-21.9.4-4.9.1-6-2.1-8.6-3.6-4.2-9.5-3.9-17.3.7-7.1 4.2-25 21.2-35.7 33.9-12.6 15-33.7 43.6-47.1 63.9-22.6 34-33.4 53.1-49.1 86.6-7.5 16-16 25.1-29 31.2-10.9 5-32.5 4.4-43.7-1.3-5.5-2.8-11.1-8-13.4-12.7-.9-1.7-1.8-3.4-2-3.6-.1-.1-4.1 2-8.8 4.8-18.9 11.5-39.1 19.7-60.2 24.8-11.3 2.6-13.1 2.8-34.4 2.7-21.1 0-23.1-.2-32.3-2.7-11.6-3.1-26.5-9.9-35.1-16.1-29.9-21.3-44-60.7-38.1-105.9 2.8-21.1 8.6-39.7 19.6-62.7 29.4-61 72.3-104.8 117.8-120 12-4 17.9-4.9 29.6-4.6 8.3.3 11.5.9 18.6 3.5M1586.4 309c19.5 2.8 33.3 11.3 39.9 24.7l2.1 4.3 3.4-2.6c13.2-10.2 21.1-12.9 34.9-12.2 7.7.3 9.7.8 14.4 3.5 4.3 2.4 6 4.2 8.2 8.4 2.8 5.2 2.9 5.8 2.4 16.9-.6 14.6-2.4 21.4-17.7 67-12.5 37.4-15.9 49.8-18.4 67.1-2.7 18.1-1.8 28.8 2.7 33.7 7.5 8.1 21.1-5.3 48.7-48.3 12.1-18.8 12.3-19.1 17.2-34.9 13.6-44 29.9-74.8 50.2-95.1 8-8 11.6-10.8 17.6-13.7 10.1-5 18.9-6.5 27.8-4.9 23 4.1 29.1 17 22.1 47-1.1 4.7-7.3 24.5-13.9 44.1-17 51-23.6 75-28.4 104.2-4.1 24.4-4.9 33-5 55.7l-.2 21.4-5.4 4.4c-13.7 11.1-31.5 20.3-44.4 23-19.1 3.9-32.4-5.7-38-27.4-2.5-9.6-3.9-26.7-3.2-38.8l.7-11-3.3 4c-12.7 15.7-29.1 29.4-43.3 36.1s-19.6 7.9-37.5 7.9c-17 0-21-.8-32.4-6.8-15.5-8.2-28.9-29.4-31.2-49.4-.4-2.9-.8-5.3-.9-5.3s-5.1 5-11.1 11.1c-18.7 19-34.9 30.3-50.9 35.6-6.8 2.3-9.7 2.7-19.5 2.7-34.2 0-55-17.4-60.9-50.8-1.7-9.5-1.3-28.1.9-40.8 6.6-38.9 29.6-86.7 63.2-131.6 20.3-27.1 45.1-43.6 73.1-48.7 8.9-1.6 26.1-1.8 36.1-.5"/><path d="M1604 147.1c-22.4 2.6-44.3 13.4-58.4 29-5.3 5.9-12.6 18.2-12.6 21.2 0 2.2 33.3 11.3 56 15.2 29 5 69.2 7 85.1 4.2 29.7-5.2 37.2-22 18.9-42-13.4-14.7-37.9-25.1-64.8-27.7-11.9-1.1-13.5-1.1-24.2.1M845.5 373c-11.6 4.5-25.2 19.7-34.5 38.5-5.1 10.2-10.2 24.7-11.5 32.2l-.7 4.3h5.4c6.6 0 19.7-3.1 27.1-6.4 18.4-8.1 33.3-25.1 38.2-43.6 2.7-10.3 1.7-16.1-3.8-21.8-5.2-5.4-11.8-6.4-20.2-3.2m727.3-9.6c-5.1 1.8-12.6 7.5-17.4 13.3-16.2 19.2-37.3 61.3-47 93.9-3 10.2-4.4 26.7-2.5 30.2 3.3 6.3 13.7 1.5 28.1-12.8 36.7-36.5 67-109 51.9-124.1-2.2-2.2-7.6-2.5-13.1-.5"/></g><path fill="#BF8080" d="m1889.9 301.7-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7m-560.9 275-5.5 5.8 5.8-5.5c5.3-5 6.2-6 5.4-6-.1 0-2.7 2.6-5.7 5.7m498.9 33c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M1693 639.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5"/><path fill="maroon" d="M1888.5 188c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m11.7 28c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m-100.1 22.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-220.3 70.1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m236 14c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m21.7 8.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-346.6 10.7-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m353.2 12.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M1504.2 493c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m290.9 53.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-91.8 11.9c0 2.7.2 3.8.4 2.2.2-1.5.2-3.7 0-5-.2-1.2-.4 0-.4 2.8m90.9 2c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m-365.2 5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m274.3 4.5c0 2.5.2 3.5.4 2.2.2-1.2.2-3.2 0-4.5-.2-1.2-.4-.2-.4 2.3m-128.3 7.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-106.2 4.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m10 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m315.4 5.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-59.4 36.7c1.2.2 3 .2 4 0 .9-.3-.1-.5-2.3-.4-2.2 0-3 .2-1.7.4"/><path fill="#800040" d="M1844.8 174.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m8 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-52.7 47.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m.1 12.4c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m72.7 35.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-313.1 38c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m15.5 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m236 14c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m32.9 27.3c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m-340.1 138.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-92 14c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m0 21c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m137.8 13.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-16 16c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m260.3 3.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-100.3.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m9.2 7.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-21.2 4.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-208.6 13c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m320.8 8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3"/><path fill="#BF0040" d="M1848.8 174.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-33.9 14-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-14.6 39.8c0 2.7.2 3.8.4 2.2.2-1.5.2-3.7 0-5-.2-1.2-.4 0-.4 2.8m65.7 49.2-5.5 5.8 5.8-5.5c5.3-5 6.2-6 5.4-6-.1 0-2.7 2.6-5.7 5.7m-47 16.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-253.2 14.3c1.8.2 4.5.2 6 0s0-.4-3.3-.4-4.5.2-2.7.4m51.7 14.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m50.3.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m108.6 15.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-85.3 4.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m79.8 1.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-79.8 9.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-113.3 7.1c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m12.4 15.3c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m-55.7 111.2-4 4.3 4.3-4c2.3-2.1 4.2-4 4.2-4.2 0-.8-.9 0-4.5 3.9m118.8 14.3c0 3.3.2 4.5.4 2.7s.2-4.5 0-6-.4 0-.4 3.3m-241.1 4c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m0 13c0 1.6.2 2.2.5 1.2.2-.9.2-2.3 0-3-.3-.6-.5.1-.5 1.8m249.6 3.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-119.8 22-6.5 6.8 6.8-6.5c6.2-6 7.2-7 6.4-7-.1 0-3.2 3-6.7 6.7m161.7 0c-.3.5-.1 1.7.4 2.8.7 1.7.8 1.7.8-.8.1-2.7-.4-3.4-1.2-2m90.4 7.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-106.1 9.9c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m16.1 17.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m90 14c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-182.3.1c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m13 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><path fill="#BF4040" d="M1836.1 176.6c-16 4.3-28.9 18.1-33.6 36-5.6 21.7 3.8 66.7 17.1 81.4 9.6 10.6 23.1 6 45.3-15.5 25.9-25.1 38.2-50.2 34.1-70-2.5-12.2-9.5-21.6-20.3-27.2-11.6-5.8-30.5-8-42.6-4.7m-288.4 134c-24.6 5.5-46.4 19.2-63.7 40.1-26.6 32.1-54.5 83.2-64.4 117.8-12.2 42.8-8.4 77.6 10.5 96.6 9.7 9.7 22 14.5 39.9 15.6 8.7.5 11.9.2 18.7-1.5 17.2-4.4 31.7-13.8 52.5-34l14.7-14.4.6 3.9c1.8 10.2 4.1 17.6 8.2 26.2 10.4 21.8 28.7 32.5 55.3 32.4 20.2-.1 37.1-5.8 53.6-18.1 7.5-5.6 21.4-19.3 27.9-27.4l3-3.6.1 20.1c0 17.3.3 21.5 2.2 28.9 4.7 19 13.2 28.3 27.1 29.5 13.8 1.2 36.7-8.8 55.9-24.5l4.2-3.4v-20.2c0-28.8 3.5-55.9 11.7-89.1 5.1-21.1 8.3-31.6 21.8-72 6.8-20.4 13.4-41.7 14.5-47.5 5.8-27.9-2.1-40.9-25.6-42.7-8.8-.7-15.8.8-24.6 5.4-25.7 13.3-50.1 52.9-67.2 108.7-4.3 14.2-5 15.6-15.3 31.7-20.6 32.3-33.7 48.5-42.3 52.4-10.9 4.9-15.8-7.3-12.7-31.5 2.5-18.8 6.5-33.9 20.2-74.5 13.7-41 16.7-52.6 16.9-66.1 0-7.8-.3-9.7-2.5-14-3.3-6.4-9-9.9-18.4-11.4-12.3-2-21.7.5-33.7 8.6-4.3 3-8 5.4-8.2 5.4s-1.9-2.6-3.9-5.8c-6.7-10.8-15.2-16.9-29.2-20.8-8.7-2.4-38.6-2.9-47.8-.8m38.2 52.9c9.4 8 4.5 34.9-12.8 69.4-11.6 23-24.7 41.8-39 56-14.4 14.3-25.1 18.8-28.6 12.1-5.7-11 5.6-48.8 26.6-88.8 15.1-29 28.7-45.1 41.8-49.8 5.4-2 8.6-1.7 12 1.1"/></svg>}>
  <p></p>
</Card>

## docs/snippets/claude-card.mdx

<Card title="Claude auto-replies to all my emails..." icon={<svg width="28" height="28" xmlns="http://www.w3.org/2000/svg" version="1.0" viewBox="0 0 999 999"><path fill="#BF8040" fill-opacity=".1" d="m63.9 52.7-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m870.1-.2c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5M51.9 64.7l-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m894.1-.2c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-894 870c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m893.9.2-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7M64 946.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m869.9.2-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7"/><path fill="#BF8040" fill-opacity=".3" d="m57.9 58.7-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7m882.1-.2c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m-882 882c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m881.9.2-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7"/><path fill="#BF8040" fill-opacity=".8" d="M191.8.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m613 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-738.9 51c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m866.6.3c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M51.4 66.2l-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m894.6-.8c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M.2 193c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m998 0c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M.2 806c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m998 0c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M50 931.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m896.4 1.8-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9M65.5 947c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m866.4.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-740.1 51c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m613 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><g fill="#BF8040" stroke-width="0"><path d="M184.5 1.1C130 5.3 76.9 33.6 43.7 76.2 18.1 109.1 4 145.6 1 186.6-.3 205.6-.4 793.5 1 812c5.4 73.1 48 135.9 113.3 167 23.1 11 46.3 17.1 72.3 19 19.4 1.4 607 1.4 625.9 0 72.6-5.4 135.4-48.2 166.5-113.3 11-23.1 17.1-46.3 19-72.3 1.3-19 1.4-606.9 0-625.4-5.4-73.1-48-135.9-113.3-167-23.1-11-46.3-17.1-72.3-19-17.1-1.2-611.6-1.1-627.9.1m268.9 228.1c.6.7 1.8 3.3 2.6 5.8s1.7 4.7 2 5 1.2 2.5 2 5c.7 2.5 1.7 4.9 2 5.5.4.5 2.8 6.6 5.4 13.5s5.1 13.3 5.7 14.2c.5 1 .9 2.5.9 3.3s.4 1.5.8 1.5c.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8 1.1.8 2.5s.5 2.5 1 2.5c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .5 1.9 1 1.9.6 0 1 1.1 1 2.5s.4 2.5.8 2.5c.5 0 1.5 2.2 2.2 5 .7 2.7 1.6 5 2 5 .3 0 1.2 2.1 1.9 4.7.8 2.7 1.7 5.2 2.1 5.8.4.5 2.8 6.6 5.4 13.5s5.1 13.3 5.7 14.2c.5 1 .9 2.5.9 3.3s.5 1.5 1 1.5c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .4 1.9.8 1.9.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8.6.8 1.3s2.7 8 6 16.2c3.4 8.3 6.8 17.2 7.7 20 1 2.7 2 5.2 2.3 5.5.4.3 1.3 2.6 2.1 5.2.7 2.7 1.7 4.8 2.1 4.8s.8 1.1.8 2.5.5 2.5 1 2.5c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .5 1.9 1 1.9.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .4 1.9.8 1.9.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8 1.1.8 2.5s.4 2.5.8 2.5c.5 0 1.5 2.2 2.2 5 .7 2.7 1.6 5 2 5 .3 0 1.2 2.1 1.9 4.7.8 2.7 1.7 5.2 2.1 5.8.4.5 2.8 6.6 5.4 13.5s5.1 13.3 5.7 14.2c.5 1 .9 2.5.9 3.3s.5 1.5 1 1.5c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5s.4 2.3.9 3.3c1.2 2.1 9.6 23.5 12.5 31.7 1.1 3.3 2.3 6.2 2.7 6.5.3.3 1.2 2.6 2 5.2.7 2.7 1.7 4.8 2.1 4.8s.8 1.1.8 2.5.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.4 2.5.8 2.5c.5 0 1.5 2.3 2.2 5 .7 2.8 1.6 5 2 5s1.3 2.3 2 5c.7 2.8 1.7 5 2.2 5 .4 0 .8.7.8 1.5s.4 2.3.9 3.3c.8 1.6 1.7 3.6 4.1 9.7.6 1.4 1.4 3.5 1.9 4.7.5 1.3 2.4 6.1 4.1 10.8 1.8 4.7 3.5 9.1 3.7 9.7.4 1-12.2 1.3-61.1 1.3-60.9 0-61.6 0-61.6-2 0-1.1-.4-2-1-2-.5 0-1-.9-1-2s-.4-2.8-.9-3.8c-.8-1.6-1.4-3-4-9.5-.5-1.2-1.7-4.2-2.6-6.7s-2.2-5.9-2.9-7.5c-.8-1.7-2.5-6.4-4-10.5s-3.1-8.3-3.7-9.2c-.5-1-.9-2.7-.9-3.8s-.4-2-1-2c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.3-1-2.2s-1-2.7-2.2-4l-2.1-2.3H391.1c-66 0-110.2.4-111.1 1s-1.7.5-2.2-.3-.8-.6-.8.6c0 .9-.4 2.5-.9 3.5-1 1.9-2.2 5-4 10.2-.7 1.9-2.2 5.7-3.2 8.5-1.1 2.7-2.7 7-3.5 9.5-.9 2.5-2 4.9-2.5 5.5-.5.5-.9 1.9-.9 3.2s-.4 2.3-.8 2.3c-.5 0-1.5 2.2-2.2 5-.7 2.7-1.7 5-2.2 5-.4 0-.8 1.1-.8 2.4 0 1.4-.5 2.8-1 3.1-.6.3-1 1.5-1 2.6 0 1-.4 1.9-1 1.9-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.1-1 2.4 0 1.4-.4 2.8-1 3.1s-1 1.7-1 3.1c0 1.3-.4 2.4-1 2.4-.5 0-1 .9-1 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 1-1 2.2 0 1.3-.7 3.1-1.4 4-1.2 1.7-5.4 1.8-62 1.8H112l.6-2.8c.4-1.5 1.1-2.9 1.6-3 .4-.2.8-1.7.8-3.3s.5-2.9 1-2.9c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.4-2.8.9-3.8c.9-1.7 2.2-5 4-9.5.4-1.2 1.7-4.5 2.9-7.2 1.1-2.8 2.8-7 3.7-9.5s2.6-6.8 3.7-9.5c1.2-2.8 2.5-6.1 3-7.5.6-1.4 1.4-3.3 1.9-4.2.5-1 .9-2.7.9-3.8s.5-2 1-2c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.4 1-3 0-1.7.5-3 1-3 .6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.1 1-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.8 1-1.8s.9-3.7 2-6c1.1-2.2 2-4.6 2-5.1 0-.6.9-3 1.9-5.3 1.1-2.4 2.7-6.3 3.6-8.8 2.6-6.9 3.7-9.3 4.6-11.2.5-1 .9-2.7.9-3.8s.5-2 1-2c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.1 1-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-.9 1-2s.5-2.9 1-4 1.5-2.9 2-4 1-2.7 1-3.5.5-1.5 1-1.5c.6 0 1-1.1 1-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .4-1.9.8-1.9.5 0 1.5-2.2 2.2-5 .7-2.7 1.7-5 2.2-5 .4 0 .8-1.1.8-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.4-2.8.9-3.8c.5-.9 1.3-2.8 1.8-4.2 1.6-4 2.9-7.2 5.1-12.5 1.8-4.3 7.9-19.9 10.2-26 1.2-3.3 3.2-7.9 4.1-9.7.5-1 .9-2.5.9-3.3s.4-1.5.8-1.5c.5 0 1.5-2.3 2.2-5 .7-2.8 1.7-5 2.2-5 .4 0 .8-1.1.8-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.3 1-2.9s.4-3.1.8-3.3 1.5-2.6 2.4-5.3c2.3-6.7 11.1-29.2 12.9-32.7.5-1 .9-2.5.9-3.3s.4-1.5.8-1.5c.5 0 1.5-2.3 2.2-5 .7-2.8 1.7-5 2.2-5 .4 0 .8-1.1.8-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.4 1-3 0-1.7.5-3 1-3 .6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.4-2.8.9-3.8c.5-.9 1.3-2.8 1.9-4.2.5-1.4 1.9-4.8 3-7.5 1.8-4.3 7.9-19.9 10.2-26 1.2-3.3 3.2-7.9 4.1-9.7.5-1 .9-2.5.9-3.3s.4-1.5.8-1.5c.5 0 1.5-2.3 2.2-5 .7-2.8 1.7-5 2.2-5 .4 0 .8-1.1.8-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.4 1-3 0-1.7.4-3 .8-3s1.6-2.7 2.5-6l1.8-6h62.5c48.7 0 62.8.3 63.8 1.2m220.3 4.8c.9 3.3 2.1 6 2.5 6s.8 1.3.8 3c0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.4 2.4.8 2.4.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8.7.8 1.5s.4 2.3.9 3.3c.5.9 1.4 3 2.1 4.7.7 1.6 1.6 3.8 2.1 4.7.5 1 .9 2.2.9 2.8s.8 2.9 1.9 5.3c1 2.3 2.6 6.2 3.6 8.7.9 2.5 2.5 6.4 3.6 8.8 1 2.3 1.9 4.9 1.9 5.7s.9 3 2 5 2 4.4 2 5.5.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .5 1.9 1 1.9.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.4 2.4.8 2.4.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8.7.8 1.5s.4 2.3.9 3.3c1.8 3.5 10.6 26 12.9 32.7.9 2.7 2 5.1 2.4 5.3s.8 1.7.8 3.3.5 2.9 1 2.9c.6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.4 2.4.8 2.4.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8.7.8 1.5s.4 2.3.9 3.3c.5.9 1.3 2.8 1.8 4.2 1.3 3.1 2.5 6.1 5.1 12.5 1.1 2.7 2.8 7 3.7 9.5s2.6 6.7 3.7 9.5c1.2 2.7 2.5 6 2.9 7.2 1.8 4.5 3.1 7.8 4 9.5.5 1 .9 2.7.9 3.8s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5s.4 2.3.9 3.3c.5.9 1.4 3 2.1 4.7.7 1.6 1.6 3.7 1.9 4.5.4.8 1.4 3.6 2.2 6.2.7 2.7 1.7 4.8 2.1 4.8s.8.7.8 1.5.4 2.3.9 3.3c.9 1.9 2.9 6.6 4.1 9.7 2.5 6.7 4.4 11.5 5.9 15 1 2.2 2.4 6.1 3.2 8.7.7 2.7 1.7 4.8 2.1 4.8s.8 1.1.8 2.5.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 .9 1 2s.4 2.8.9 3.8c.5.9 1.3 2.8 1.9 4.2.5 1.4 1.8 4.7 3 7.5 1.1 2.7 2.8 7 3.7 9.5s2.6 6.7 3.7 9.5c2.6 6.4 3.8 9.4 5.1 12.5.5 1.4 1.3 3.3 1.8 4.2.5 1 .9 2.7.9 3.8s.5 2 1 2c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 2.9s.4 3.1.8 3.3c.5.1 1.2 1.5 1.6 3l.6 2.8H767.2l-1.2-3.3c-.6-1.7-1.6-4-2.1-4.9-.5-1-.9-2.7-.9-3.8s-.4-2-1-2c-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-.9-1-2s-.4-2.8-.9-3.8c-.5-.9-1.3-2.8-1.9-4.2-.5-1.4-1.9-4.6-3.1-7.2s-2.1-5.4-2.1-6.1-.4-1.7-.8-2.2c-.5-.6-1.4-2.4-2-4-1-3.1-7.6-20.2-9.3-24.3-.5-1.2-1.4-3.4-2-5-.6-1.5-1.5-3.5-2-4.4-.5-1-.9-2.5-.9-3.3s-.4-1.5-1-1.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-.9-1-2s-.4-2.9-1-4c-.5-1.1-1.4-2.9-2-4-.5-1.1-1-2.7-1-3.5s-.4-1.5-.8-1.5c-.5 0-1.5-2.2-2.2-5-.7-2.7-1.6-5-2-5-.3 0-1.2-2.1-1.9-4.8-.8-2.6-1.7-5.2-2.1-5.7-.4-.6-2.8-6.6-5.4-13.5s-5.1-13.3-5.7-14.2c-.5-1-.9-2.5-.9-3.3s-.4-1.5-.8-1.5c-.5 0-1.5-2.2-2.2-5-.7-2.7-1.7-5-2.2-5-.4 0-.8-1.1-.8-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.3-2.4-.8-2.4-.4 0-1.3-1.9-2-4.3-1.4-4.5-7.8-21.1-9.3-23.9-.5-1-.9-2.5-.9-3.3s-.4-1.5-.8-1.5c-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-1.4-.8-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-.8-2.4s-1.4-2.1-2.1-4.8c-.8-2.6-1.7-5.2-2.1-5.7-.4-.6-2.4-5.5-4.5-11s-4.1-10.5-4.5-11c-.4-.6-1.3-3.1-2.1-5.8-.7-2.6-1.6-4.7-1.9-4.7-.4 0-1.3-2.3-2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-1.1-.8-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-.8-1.9-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-.7-.8-1.5s-.4-2.3-.9-3.3c-1.2-2.4-4.4-10.1-6.1-14.7-.4-1.1-1.2-3.1-1.8-4.5s-1.9-5-3-8c-3.3-9.2-7.3-19.4-8.3-21.2-.5-1-.9-2.5-.9-3.3s-.4-1.5-1-1.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5V228h119.9z"/><path d="M388.8 372.7c-.9 2.1-2.9 7.2-4.6 11.3s-3.7 9.4-4.4 11.7c-.7 2.4-1.6 4.3-2 4.3-.5 0-.8 1.3-.8 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 .9-1 2s-.4 2.9-1 4c-.5 1.1-1.4 2.9-2 4-.5 1.1-1 2.9-1 4s-.4 2-1 2c-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.1-1 2.5s-.4 3.3-.9 4.3c-1.6 3.2-3.2 7.1-8.1 20.2-1.7 4.7-3.8 10-4.6 11.8s-1.4 3.7-1.4 4.2c0 .6-.9 3.2-2.1 5.8s-2.6 5.8-3.1 7.2c-.6 1.4-1.4 3.3-1.9 4.2-.5 1-.9 2.7-.9 3.8s-.4 2-.8 2c-.5 0-1.5 2.2-2.2 5-.7 2.7-1.7 5-2.2 5-.4 0-.8 1.3-.8 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 .9-1 2 0 2 .7 2 71.9 2h72l-.3-2.3c-.2-2.1-.7-3.6-2.6-8.2-.4-1.1-2.2-5.8-4-10.5-1.7-4.7-3.7-9.9-4.4-11.5-.7-1.7-1.9-5.1-2.7-7.8-.7-2.6-1.7-4.7-2.1-4.7s-.8-.8-.8-1.8-.9-3.7-2-6c-1.1-2.2-2-4.8-2-5.7 0-.8-.4-1.5-1-1.5-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-.8-2.5c-.5 0-1.5-2.3-2.2-5-.7-2.8-1.6-5-2-5-.3 0-1.2-2.1-2-4.7-.7-2.6-3.7-10.7-6.6-18-2.8-7.3-5.8-15.3-6.5-17.8s-1.7-4.5-2.1-4.5-.8-1.4-.8-3c0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.4-1-3 0-4.1-1.4-3.8-3.2.7"/></g><path fill="#808040" d="M326.6 229.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m344.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M310.5 270c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m376.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-112 19.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-274.5 5.2c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m397.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M579 299.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m2 5c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m122.6 5.4c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-416 17.4c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m422.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M285.5 333c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1M726 368c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-337.4 2.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m218.4-.6c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M515.5 387c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m3.5 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-143.5 8c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m-120 3.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m487.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-120.1.7c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-375.9 21.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m504.4-.6c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M634 438c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-216.4 1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m4 10c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M230.5 471c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m536.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-217 1.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M431.5 475c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m3.5 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-92.4 4.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m440.4 20c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-334 10c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M328 526c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-120.4 2.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m582.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M324.5 536c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1M460 548.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M802 559c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m4 10c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-630.5 40.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m647.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-652 12.4c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m656.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M611 627c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m99 2c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-543.4 2.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m664.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M615 637c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m99 2c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m4 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-381.7 8.7c29.5.2 77.9.2 107.5 0 29.5-.1 5.3-.2-53.8-.2s-83.3.1-53.7.2M150.5 672c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m121.1.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1M847 672c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-217 1.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M509 676c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-363 6.1c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m121.6.4c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m583.9-.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M513 686c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-371 6.1c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m713.5-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-720.9 19.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m728.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-335.4 11.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M250.5 727c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m498.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-621.4 2.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m742.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-338 6c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m124.6 5.4c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m2 5c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M759 752c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-644 8c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m767 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1"/><path fill="#404040" d="M552.5 231c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-97.5 5.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m7 18.4c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m2 5c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m10.5 26.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m228.1 24.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-411 9.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m414.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-323.5 65.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m352.1 6.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-474 7.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m476.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M404.5 403c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-147.9 4.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m161.4 30c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-59 6.5c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m183 11.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-196.4 23.6c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m89.9 3.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-211 5.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m549.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-110 21c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-448.1 1.8c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m567.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-450.1 1.8c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m-120.9 7.6c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m574.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-332.4 9.4c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M195 561c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m390.6 3.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m110 25c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-512.1 1.8c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m631.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M698 595.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-517.4 3c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m636.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M164 639c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m669 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-673 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m159 7.7c106.7.7 184.8.7 184.3 0-.2-.4-50.9-.6-112.6-.5-61.7.2-94 .4-71.7.5m307.6 10.6c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M513 684c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-368.5 5.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m709.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M743 708.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m119.6 3.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M525.5 715c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m340.5 5.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M119 752c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m759 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-217 1.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-119 4c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m221 2.5c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-648 2.1c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m767-.1c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-216.4 3.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-431.6.8c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m312 1.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7"/><path fill="#404000" d="M359.8 228.7c17.2.2 45.1.2 62 0 16.9-.1 2.8-.2-31.3-.2s-47.9.1-30.7.2m223 0c16 .2 42.4.2 58.5 0 16-.1 2.9-.2-29.3-.2s-45.3.1-29.2.2m-28.3 6.3c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M322 243c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m353.5 0c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M558 245c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1M277.5 356c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m326 2c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M273 366.1c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m451.5-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M402 397c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m217-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M519 399c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-143.4 2.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m-117 1.8c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m480.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M523 409c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-151.4 2.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m45 21.6c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-55.6 4.8c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m174-.6c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-114.4 5.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-178.6.8c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m513.5-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M638 446c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-411.4 37.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m544.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M437 487c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m117 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-336.4 19.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m227.9 1.3c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m16.5 42.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-276.5 36.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4M599 600c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-420.4 4.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m640.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M603 610c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-437 25.1c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m449 3.4c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-453 6.6c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m673.5-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-688.9 39.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m704.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M634 688c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-110.5 21c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m121 4c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-106.9 32.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M240 750c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m419-.1c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M236 760c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-93.8 10.7c16.4.2 43.2.2 59.5 0 16.4-.1 3-.2-29.7-.2s-46.1.1-29.8.2m436.6 0c16.1.2 42.2.2 58 0 15.8-.1 2.6-.2-29.3-.2s-44.8.1-28.7.2m221 0c16.1.2 42.2.2 58 0 15.8-.1 2.6-.2-29.3-.2s-44.8.1-28.7.2"/><path fill="#804040" d="M453 230.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-129 6.2c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m230.5-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m119 0c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M457 240.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m4 10c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m101.5 6.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-250 9c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m253.5-.1c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m119 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-216.5 4c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M308 276c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m262-.1c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m119 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-217 4c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m102 5.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-97.5 4.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-174 1.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m393.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M577 293.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-278.5 7.2c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m401.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M482.5 305c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m100.5 3.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-289.4 4.6c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m410.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-118.4 1.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-99.1.7c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m221.5 7.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-418.5 1.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m-2 4.9c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m204.5-.1c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m99 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-215.4 5.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M283.5 339c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m430.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-432.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m434.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-215.4 5.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M277.5 354c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m323.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-447 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m332 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-335 4.5c-.7.9-1 1.8-.6 2.2.3.3.6.1.6-.5 0-.7.7-1.2 1.5-1.2s1.5-.5 1.5-1c0-1.5-1.6-1.2-3 .5m118.5-.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m1.5 3.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M269.5 374c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m339.5-.1c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m119 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-341.5 1.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m124.5 1.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-126.5 3.2c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4M396 383c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m117.5 0c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-248 1.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m467.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-351 3.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-118.1 1.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m471-.1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M616 390.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-236.4 3.2c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2M517 393c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m219-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-474.5 1.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m116.1 3.4c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1M402 399c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-143.8 3.5c-1.7 3.7-1.4 4.5.4 1.2.9-1.6 1.4-3.1 1.1-3.3-.2-.2-.9.7-1.5 2.1m361.8-2c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m119.1 0c0 .5.6 2.1 1.4 3.5s1.4 2 1.4 1.5c0-.6-.6-2.1-1.4-3.5s-1.4-2.1-1.4-1.5m-363.5 3.2c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m145.9-.7c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-147.9 5.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m32.4.3c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-151.4 2.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m488.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-118.4 1.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-253 1.4c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m37 .6c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-156 2.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m492.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M626 417c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-98.5 1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-158 1c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m41.1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M250.5 422c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m496.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-379.5 2c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m-118.9 2.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m501 .8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M533 431.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m218 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-387.5 2c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m-121 8c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m176.5-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m118 0c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m99 .5c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-398 8c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m66.6 3.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m1.4 4.6c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m2 5c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-75.5 2.2c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m-119 1.9c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m413.5-.1c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m119 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-335.5 2c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-80 1.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m199.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-201 5.4c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2M228 477c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m541 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-336 2c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 2c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-208.4 4.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1M437 489c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119.5 2c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-334 1.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m553.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-435 1.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m316.4 1.4c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-436.5 2.2c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m557.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-439 1.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m-120.1 3.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m561.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M661 503.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-98.5 2.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-228 3.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m111 .9c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m118.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-350.8 5.5c-1.7 3.7-1.4 4.5.4 1.2.9-1.6 1.4-3.1 1.1-3.3-.2-.2-.9.7-1.5 2.1m233.8-2c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m218 0c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m119.1 0c0 .5.6 2.1 1.4 3.5s1.4 2 1.4 1.5c0-.6-.6-2.1-1.4-3.5s-1.4-2.1-1.4-1.5M330 520c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-120.4 4.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m241.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m337 0c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-462 6.6c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m246-.2c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m99 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-584.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m369.1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M792 535c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-588.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m119 0c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m471.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-336 2.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M201.5 545c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m594.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-215.4 5.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M462 552.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M197 555c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m484 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-217 1.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M193 565c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m492 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-216.4 3.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-396.1 1.8c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m497.5 4.8c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m-98 3.1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-403.5 2.1c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m-2 5c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m627.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M595 588c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m219.5 2c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M696 591.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-514.4 3c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m634.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M599 598c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m101 3.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m119.1 0c0 .5.6 2.1 1.4 3.5s1.4 2 1.4 1.5c0-.6-.6-2.1-1.4-3.5s-1.4-2.1-1.4-1.5m-641.5 3c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m423.9-.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m-426.9 8.5c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m648.4-1c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-118.4 1.8c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-532 4.4c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m652.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M706 618c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-98.5 1c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m1.5 4c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m99 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-539.4 4.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m660.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M712 633c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-550 10c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m455-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m99 .5c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-215.4 5.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M158 653c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m562 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-682.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m341.3-.3c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m343.2.3c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-338 .9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M275.5 661c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m229.5 2.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M273.5 666c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m-121 2c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m574.1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M845 668c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-338 2c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m121.6 1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m100 2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-219.6.6c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-239.4 2.6c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1M148 678c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m582 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-338 2c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m121.5 2c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m99.5-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-585.5 1.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m705.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-586 3.2c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1M515 690c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-251.7 2.5c-.3.9-.3 1.8-.1 2.1.3.3.8-.4 1.1-1.6.6-2.3-.1-2.7-1-.5m-122.8 5.6c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m717.1.2c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-719.1 4.8c-.4.6-.5 1.2-.2 1.5.2.3.7-.2 1-1.1.7-1.7.1-2-.8-.4m602.5 1.4c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M521.5 706c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m339.6.5c.1 1.9 1.8 5.1 1.8 3.5 0-.8-.4-2.2-.9-3s-.9-1.1-.9-.5m-604.5 3c-.9 2.6-.7 3.2.5 1.2.6-1 .9-2.1.6-2.3-.2-.3-.7.2-1.1 1.1m266.9 1.5c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m120.5 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-510.8 5.5c-1.7 3.7-1.4 4.5.4 1.2.9-1.6 1.4-3.1 1.1-3.3-.2-.2-.9.7-1.5 2.1m121.4-.8c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m490.4-1.2c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m119.1 0c0 .5.6 2.1 1.4 3.5s1.4 2 1.4 1.5c0-.6-.6-2.1-1.4-3.5s-1.4-2.1-1.4-1.5M648 721c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m99 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-617.4 4.7c-.6 1.4-.5 1.5.5.6.7-.7 1-1.5.7-1.8s-.9.2-1.2 1.2m738.4-.8c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-338.4 1.4c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M248.5 731c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m282.5-.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m121 .4c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m99 .1c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-625.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m529.1.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M872 736c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-748.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m631.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-633.5 5c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m121 0c-.3.5-.1 1 .4 1 .6 0 1.1-.5 1.1-1 0-.6-.2-1-.4-1-.3 0-.8.4-1.1 1m633.5 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-216.9 1.5c0 .5.6 2.1 1.4 3.5s1.4 2 1.4 1.5c0-.6-.6-2.1-1.4-3.5s-1.4-2.1-1.4-1.5M538 748.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m2 4.6c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M117 756c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m644 0c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m119 0c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-217 1.5c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5m-427 4.6c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m308 1.4c0 .5.5 1.7 1 2.5s1 1 1 .5c0-.6-.5-1.7-1-2.5s-1-1.1-1-.5M113 766c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m652-.1c0 .5.5 1.3 1 1.6.6.3 1 .2 1-.4 0-.5-.4-1.3-1-1.6-.5-.3-1-.2-1 .4m119 .1c0 .5.5 1 1 1 .6 0 1-.5 1-1 0-.6-.4-1-1-1-.5 0-1 .4-1 1m-217 3.6c0 .8.5 1.2 1 .9.6-.4.8-1.1.5-1.6-.9-1.4-1.5-1.1-1.5.7m-121 .4c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1"/><path fill="#400000" d="M472.5 282c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1M605 362c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m-65 89.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7m-193 21.2c0 .5.5.7 1 .4.6-.3 1-.8 1-1.1 0-.2-.4-.4-1-.4-.5 0-1 .5-1 1.1m205.5 9.9c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1m104.1 7.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M681 553c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1m16.6 40.3c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6M648 722.9c0 .6.4 1.3 1 1.6s.7-.1.4-.9c-.7-1.8-1.4-2.1-1.4-.7M761 754c0 .5.5 1 1.1 1 .5 0 .7-.5.4-1-.3-.6-.8-1-1.1-1-.2 0-.4.4-.4 1"/><path d="M328.7 229.7c-.4.3-.7 1.5-.7 2.5s-.4 1.8-1 1.8c-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .7-1 1.5s-.5 2.5-1.2 3.8c-2.3 4.6-2.8 5.7-2.8 7 0 .7-.9 3.2-2 5.5-1.1 2.2-2 4.8-2 5.7 0 .8-.4 1.5-1 1.5-.5 0-1 .6-1 1.3 0 .8-2.5 7.6-5.5 15.2-3 7.7-5.5 14.5-5.5 15.2s-.4 1.3-.8 1.3c-.5 0-1.5 2.2-2.2 5-.7 2.7-1.7 5-2.2 5-.4 0-.8.9-.8 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 .9-1 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 .6-1 1.2 0 .7-2.5 7.5-5.5 15.2-3 7.6-5.5 14.5-5.5 15.2 0 .8-.4 1.4-1 1.4-.5 0-1 .7-1 1.5s-.9 3.7-2.1 6.3-2.6 5.8-3.1 7.2c-.6 1.4-1.4 3.3-1.9 4.2-.5 1-.9 2.7-.9 3.8s-.4 2-1 2c-.5 0-1 .9-1 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .7-1 1.5s-.4 2.3-.9 3.3c-.5.9-1.3 2.8-1.8 4.2-.6 1.4-2.5 6.1-4.3 10.5-1.7 4.4-3.8 9.6-4.5 11.5-4.7 12.3-6 15.6-6.6 16.5-.3.5-1.2 3.1-2 5.7-.7 2.7-1.6 4.8-1.9 4.8-.4 0-1.3 2.2-2 5-.7 2.7-1.7 5-2.2 5-.4 0-.8.9-.8 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 1.1-1 2.5s-.4 2.5-1 2.5c-.5 0-1 .9-1 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .7-1 1.5 0 1.5-.4 2.5-2.8 7.2-.7 1.3-1.2 2.7-1.2 3.3s-.8 2.9-1.9 5.3c-3.2 7.2-9.1 23.1-9.1 24.4 0 .7-.4 1.3-1 1.3-.5 0-1 .7-1 1.5s-.9 3.7-2.1 6.3-2.6 5.8-3.1 7.2c-.6 1.4-1.4 3.3-1.9 4.2-.5 1-.9 2.7-.9 3.8s-.4 2-1 2c-.5 0-1 .9-1 1.9 0 1.1-.4 2.3-1 2.6-.5.3-1 1.7-1 3.1 0 1.3-.4 2.4-1 2.4-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .7-1 1.5s-.4 2.3-.9 3.3c-.5.9-1.3 2.8-1.8 4.2-.6 1.4-2.5 6.1-4.3 10.5-1.7 4.4-3.9 9.8-4.7 12s-2.4 6.2-3.5 9c-1.2 2.7-2.5 6.1-3 7.5-.6 1.4-1.4 3.3-1.9 4.2-.5 1-.9 2.7-.9 3.8s-.4 2-1 2c-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 .9-1 2s-.5 2.9-1 4-1.4 2.9-2 4c-.5 1.1-1 2.9-1 4s-.4 2-1 2c-.5 0-1 .7-1 1.5s-.4 2.3-.9 3.3c-.5.9-1.4 3-2.1 4.7-.7 1.6-1.6 3.8-2.1 4.7-.5 1-.9 2.5-.9 3.3s-.4 1.5-1 1.5c-.5 0-1 1.3-1 3 0 1.6-.4 3-1 3-.5 0-1 .9-1 2s-.4 2-1 2c-.5 0-1 1-1 2.3s-.5 2.8-1.2 3.5c-.9.9 11.9 1.2 57.9 1.2 32.5 0 59.8-.3 60.7-.6s1.6-1.5 1.6-2.5.5-1.9 1-1.9c.6 0 1-1.4 1-3 0-1.7.5-3 1-3 .6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.4 1-3 0-1.7.5-3 1-3 .6 0 1-.9 1-2s.5-2.9 1-4c.6-1.1 1.5-2.9 2-4 .6-1.1 1-2.4 1-3s.8-2.9 1.9-5.3c2.5-5.6 4.6-11.2 10.1-26.2 1.1-3 3.1-7.8 4.1-9.7.5-1 .9-2.7.9-3.8s.4-2 .8-2c.5 0 1.5-2.2 2.2-5 .7-2.7 1.6-5 2-5s1.3-2.2 2-5c.7-2.7 1.7-5 2.2-5 .4 0 .8-1.1.8-2.4 0-1.4.5-2.8 1-3.1.6-.3 1-1.5 1-2.6 0-1 .5-1.9 1-1.9.6 0 1-.7 1-1.5 0-1.3 13.2-1.5 113-1.5h113l1.4 2.2c.8 1.3 1.7 3.4 2.1 4.8s1.5 4.3 2.4 6.5c1 2.2 2.4 6.1 3.2 8.7.7 2.7 1.7 4.8 2.1 4.8s.8.9.8 1.9c0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .7 1 1.5s.4 2.3.9 3.3c.8 1.6 1.7 3.7 4 9.4.5 1.3 1.4 3.9 2.1 5.8s1.6 4.4 2.1 5.5c.4 1.1 2.4 6 4.3 11l3.4 9 59.6.3c52.6.2 59.6 0 59.6-1.3 0-.8-.4-1.5-.8-1.5-.5 0-1.5-2.2-2.2-5-.7-2.7-1.6-5-2-5-.3 0-1.2-2.1-1.9-4.7-.8-2.7-1.7-5.3-2.1-5.8-.4-.6-2.4-5.5-4.5-11s-4.1-10.5-4.5-11c-.4-.6-1.3-3.1-2.1-5.8-.7-2.6-1.7-4.7-2.1-4.7s-.8-1.1-.8-2.4c0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-.8-1.9-.5 0-1.5-2.2-2.2-5-.7-2.7-1.7-5-2.2-5-.4 0-.8-.7-.8-1.5s-.4-2.3-.9-3.3c-1.2-2.4-2.5-5.7-5.1-12.7-1.2-3.3-2.8-7.4-3.5-9.1s-3.2-8-5.5-14c-2.2-6-4.5-11.7-5.1-12.6-.5-1-.9-2.5-.9-3.3s-.4-1.5-.8-1.5c-.5 0-1.5-2.3-2.2-5s-1.7-5-2.2-5c-.4 0-.8-1.4-.8-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-.8-2.4-.5 0-1.5-2.2-2.2-5-.7-2.7-1.7-5-2.2-5-.4 0-.8-.7-.8-1.5s-.9-3.4-1.9-5.8c-1.1-2.3-3.6-8.7-5.7-14.2-2-5.5-4-10.5-4.4-11-.4-.6-1.3-3.1-2.1-5.8-.7-2.6-1.6-4.7-1.9-4.7-.4 0-1.3-2.2-2-5-.7-2.7-1.7-5-2.2-5-.4 0-.8-1.1-.8-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-.8-2.4-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-.7-.8-1.5s-.4-2.3-.9-3.3c-.8-1.6-1.5-3.1-4-9.5-.5-1.2-3.5-9-6.7-17.2-3.1-8.3-6-15.5-6.4-16-.4-.6-1.3-3.1-2.1-5.8-.7-2.6-1.6-4.7-1.9-4.7-.4 0-1.3-2.3-2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-1.1-.8-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-.8-2.4-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-.7-.8-1.5s-.8-3.4-1.9-5.8c-1.9-4.2-5.8-14.3-8.2-20.7-.7-1.9-1.5-4-1.9-4.5-.4-.6-1.3-3.1-2.1-5.8-.7-2.6-1.7-4.7-2.1-4.7s-.8-.9-.8-1.9c0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-.7-1-1.5s-.4-2.3-.9-3.3q-2.25-4.5-6.6-16.2c-.9-2.5-2.8-7.3-4.3-10.8l-2.6-6.2h-61.2c-33.6 0-61.4.3-61.7.7M392 369c0 1.1.5 2 1 2 .6 0 1 .7 1 1.5s.5 2.4 1 3.5c.6 1.1 1.5 2.9 2 4 .6 1.1 1 2.9 1 4s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 2s.5 2 1 2c.6 0 1 1.3 1 3 0 1.6.5 3 1 3 .6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5s.5 2.4 1 3.5c.6 1.1 1.5 2.9 2 4 .6 1.1 1 2.8 1 3.9s.3 2.1.8 2.3c.4.1 1.8 3.2 3 6.8 1.3 3.6 3 8.1 3.7 10 .8 1.9 3.5 8.9 6.1 15.5 2.5 6.6 4.9 13 5.1 14.2.3 1.3.9 2.3 1.4 2.3s.9 1.3.9 3c0 1.6.5 3 1 3 .6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .5 1.9 1 1.9.6 0 1 .9 1 2s.4 2.8.9 3.8c1.7 3.2 3.7 8.3 7.1 17.7 1.2 3.3 3 7.9 4.1 10.3 1 2.3 1.9 4.5 1.9 4.7 0 .3-33.5.5-74.5.5-45.9 0-74.5-.4-74.5-.9 0-.6.5-1.3 1-1.6.6-.3 1-1.7 1-3.1 0-1.3.5-2.4 1-2.4.6 0 1-1.1 1-2.5s.5-2.5 1-2.5c.6 0 1-.9 1-2s.4-2.8.9-3.8c.5-.9 1.4-3.1 2.1-4.7.7-1.7 1.6-3.7 1.9-4.5.4-.8 1.4-3.6 2.2-6.3.7-2.6 1.7-4.7 2.1-4.7s.8-.7.8-1.6c0-1.4 3.1-10.2 6.2-17.4.6-1.4 1.5-3.6 2.1-5 .5-1.4 1.3-3.2 1.6-4 .4-.8 1.4-3.6 2.2-6.3.7-2.6 1.7-4.7 2.1-4.7s.8-1.4.8-3c0-1.7.5-3 1-3 .6 0 1-.9 1-2s.5-2 1-2c.6 0 1-.9 1-2s.4-2.8.9-3.8c.5-.9 1.4-3.1 2.1-4.7.7-1.7 1.6-3.8 2.1-4.7.5-1 .9-2.7.9-3.8s.5-2 1-2c.6 0 1-.9 1-2s.5-2 1-2c.6 0 1-1.4 1-3 0-1.7.4-3 .8-3 .5 0 1.5-2.3 2.2-5 .7-2.8 1.7-5 2.2-5 .4 0 .8-.9.8-2s.4-2.8.9-3.8c1.6-3.1 6.4-15.4 10.3-26.2 1.1-3 2.4-6.5 2.9-7.8.5-1.2 1.3-3.2 1.8-4.5 1.4-3.7 3.3-8.1 4.2-9.9.5-1 .9-2.7.9-3.8 0-1.3.7-2 2-2s2 .7 2 2m162-137.5c0 1.4.4 2.5.8 2.5.5 0 1.5 2.2 2.2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8 1.1.8 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.4 2.4.8 2.4s1.4 2.1 2.1 4.7c.8 2.7 1.7 5.2 2.1 5.8.8 1.3 9.3 22.6 11.2 28 .7 2.2 1.6 4.2 1.9 4.5s1.2 2.6 2 5.2c.7 2.7 1.7 4.8 2.1 4.8s.8 1.1.8 2.5.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.4 0 1.4.5 2.8 1 3.1.6.3 1 1.5 1 2.6 0 1 .5 1.9 1 1.9.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3s.4 2.5.8 2.7c.4.1 1.5 2.5 2.4 5.3 1.9 5.6 9.3 24.2 10.6 26.5.5.8 1.5 3.6 2.3 6.2.7 2.7 1.6 4.8 1.9 4.8.4 0 1.3 2.2 2 5 .7 2.7 1.7 5 2.2 5 .4 0 .8 1.1.8 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5 0 .9.9 3.5 2 5.7 1.1 2.3 2 4.5 2 5s.8 2.7 1.9 5.1c1.9 4.2 5.8 14.3 8.2 20.7.7 1.9 1.5 3.9 1.9 4.5.4.5 1.3 3.1 2.1 5.7.7 2.7 1.7 4.8 2.1 4.8s.8 1.1.8 2.5.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5s.4 2.3.9 3.3c1 1.9 3.4 7.9 6 14.4.4 1.3 1.3 3.4 1.9 4.8 1.3 3.1 7.8 19.7 10.4 27 1.2 3 2.4 5.9 2.8 6.5.4.5 1.3 3.1 2.1 5.7.7 2.7 1.7 4.8 2.1 4.8s.8.9.8 1.9c0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 .7 1 1.5s.9 3.7 2.1 6.3c2.6 5.8 3.3 7.5 8.4 20.7 3.7 9.6 6.6 16.7 7.6 18.7.5 1 .9 2.7.9 3.8s.5 2 1 2c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.5 2.4 1 2.4.6 0 1 1.1 1 2.5s.5 2.5 1 2.5c.6 0 1 .9 1 1.9 0 1.1.5 2.3 1 2.6.6.3 1 1.7 1 3.1 0 1.3.4 2.4.8 2.4s1.4 2.1 2.1 4.8c.8 2.6 1.9 5 2.5 5.4s27.4.8 59.6.8c45.4 0 58.1-.3 57.2-1.2-.7-.7-1.2-2.2-1.2-3.5s-.4-2.3-1-2.3c-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-1.9 0-1.1-.4-2.3-1-2.6-.5-.3-1-1.7-1-3.1 0-1.3-.4-2.4-1-2.4-.5 0-1-.7-1-1.5s-.4-2.3-.9-3.3c-.5-.9-1.4-3.1-2.1-4.7-.7-1.7-1.6-3.8-2.1-4.7-.5-1-.9-2.5-.9-3.3s-.4-1.5-.8-1.5c-.5 0-1.5-2.2-2.2-5-.7-2.7-1.7-5-2.2-5-.4 0-.8-.9-.8-2s-.4-2.8-.9-3.8c-.5-.9-1.3-2.8-1.9-4.2-.5-1.4-1.8-4.8-3-7.5-1.1-2.8-2.7-6.8-3.5-9s-3-7.6-4.7-12c-1.8-4.4-3.7-9.1-4.3-10.5-.5-1.4-1.3-3.3-1.8-4.2-.5-1-.9-2.5-.9-3.3s-.4-1.5-1-1.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-.9-1-2s-.4-2.8-.9-3.8c-.5-.9-1.3-2.8-1.9-4.2-.5-1.4-1.9-4.6-3.1-7.2s-2.1-5.5-2.1-6.3-.4-1.5-.8-1.5-1.4-2.1-2.1-4.8c-1.4-4.5-2-6.3-5.4-14.2-.9-1.9-2.3-5.5-3.2-8-2.1-5.7-3.5-9.1-4.6-11.2-.5-1-.9-2.5-.9-3.3s-.4-1.5-1-1.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-.8-1.9-.5 0-1.5-2.2-2.2-5-.7-2.7-1.6-5-2-5-.3 0-1.2-2.1-1.9-4.8-.8-2.6-1.7-5.2-2.1-5.7-.3-.6-2.3-5.5-4.4-11s-5.2-13.4-6.8-17.5c-1.7-4.1-3.5-8.6-4.1-10-.5-1.4-1.3-3.3-1.8-4.2-.5-1-.9-2.5-.9-3.3s-.4-1.5-1-1.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-.8-1-1.8s-.9-3.7-2-6c-1.1-2.2-2-4.5-2-5 0-.4-.9-2.7-2-5-1.1-2.2-2-4.8-2-5.7 0-.8-.4-1.5-1-1.5-.5 0-1-.6-1-1.4 0-.7-2.5-7.6-5.5-15.2-3-7.7-5.5-14.5-5.5-15.2 0-.6-.4-1.2-1-1.2-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.1-1-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-.8-1.9-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-.6-.8-1.3s-2.5-7.5-5.5-15.2c-3-7.6-5.5-14.4-5.5-15.2 0-.7-.4-1.3-.8-1.3-.5 0-1.5-2.3-2.2-5-.7-2.8-1.7-5-2.2-5-.4 0-.8-1.1-.8-2.4 0-1.4-.4-2.8-1-3.1-.5-.3-1-1.5-1-2.6 0-1-.4-1.9-1-1.9-.5 0-1-1.1-1-2.5s-.4-2.5-1-2.5c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-1-2c-.5 0-1-1.4-1-3 0-1.7-.4-3-1-3-.5 0-1-.9-1-2s-.4-2-.9-2-1.1-1-1.3-2.3l-.3-2.2-58.2-.3-58.3-.2z"/><path fill="#BF8080" d="M278 659c0 .5.7.7 1.5.4.8-.4 1.5-.8 1.5-1s-.7-.4-1.5-.4-1.5.4-1.5 1m223.5 0c.3.5.8 1 1.1 1 .2 0 .4-.5.4-1 0-.6-.5-1-1.1-1-.5 0-.7.4-.4 1"/></svg>}>
  <p></p>
</Card>

## docs/snippets/openai-icon.mdx

<Card title="Automates job postings with OpenAI..." icon={<svg width="28" height="28" xmlns="http://www.w3.org/2000/svg" version="1.0" viewBox="0 0 1000 1000"><path fill="teal" fill-opacity=".2" d="M.1 192.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M.2 808c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3"/><path fill="teal" fill-opacity=".3" d="m64.4 55.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-9 9-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2"/><path fill="teal" fill-opacity=".5" d="M.2 198c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m.1 604.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3"/><path fill="#008040" fill-opacity=".7" d="M190.8.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m618 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M.5 500c0 164.2.1 231.3.2 149.2.2-82 .2-216.4 0-298.5-.1-82-.2-14.9-.2 149.3"/><g fill="#008040" stroke-width="0"><path d="M185.5 1.1C153.3 4.2 119.7 16 93 33.8 49.7 62.6 19.7 105 7.1 154.9.6 180.7 1 159.2 1 499.8c0 205.5.3 312.6 1 318.2 7.5 62 40.7 115.7 92 149.2 26.8 17.5 58.3 28.6 90.5 31.8 7.8.8 101.4 1 323 .8 293-.3 312.6-.4 321.5-2.1 46.6-8.6 82.9-28 114.6-60.9C974.5 904.6 994 863.2 999 819c.8-6.7 1-103.6.8-325.5l-.3-316-2.8-12.4c-11.1-49.5-35.6-90-72.7-119.8-31.7-25.5-69-40.5-110-44.2-14.1-1.3-614.9-1.3-628.5 0m295.1 140c41.4 4 78.6 21.1 109 50.2l9.2 8.7 9.3-1.4c29.4-4.5 57.6-2.1 86.4 7.5 59.4 19.8 104.4 69.5 119 131.4 3.8 15.9 4.8 27.7 4.2 47.3-.5 17.5-3.2 34.7-7 45.6l-1.4 4 6.1 7.5c28.8 35.8 43.6 83.5 39.8 128.1-5 57.6-33.5 106.3-80.5 137.9-15.4 10.4-32.3 18-52 23.5l-10.9 3.1-3.3 8.5c-11.3 29.3-29 54.5-51.7 73.6-28.7 24.1-61.6 38.2-98.8 42.5-13.1 1.5-39.1.7-52-1.6-35.2-6.3-67.2-22.6-93.4-47.6l-10.1-9.7-7.5 1.4c-11.6 2.1-34.9 2.8-47.5 1.5-61.4-6.6-112.7-40.8-142.8-95.2-5.1-9.3-12.1-26.8-15.1-37.9-4.7-17.5-6-27.2-6-47-.1-19.9 1.4-32.4 5.9-48.6l2.4-8.6-6.1-7.6c-18.6-23.3-31.1-50.2-37-79.4-2-9.9-2.3-14.2-2.3-34.8s.3-24.9 2.3-34.8c13.3-65.5 58.8-117.6 120.7-138.1 6.6-2.2 13.8-4.3 16.1-4.7 4-.6 4.1-.7 6.8-8.2 28.3-76.5 106.3-125.1 188.2-117.1"/><path d="M439.5 190.7c-27.4 4.9-54.3 19.6-73.8 40.3-17.6 18.7-28.6 39.8-34.3 66.1-1.8 7.8-1.9 14.5-1.9 100.8v92.5l29.3 16.9c16.1 9.3 29.6 16.6 30 16.2.4-.3.8-46.4 1-102.3.3-112.7-.1-106.6 7-113.4 2.1-2.1 35.4-21.9 76.7-45.7 40.2-23.2 73.1-42.5 73.3-43 .5-1.4-13.2-10.7-23.2-15.8-11.1-5.6-26.5-10.7-38-12.8-11.1-1.9-35.1-1.8-46.1.2"/><path d="M625.2 245.1c-16.1 1.2-33 5.8-47.5 13-7.5 3.7-150.9 86.1-153.4 88.1-1 .8-1.3 9.1-1.3 35.4 0 18.9.2 34.4.3 34.4.2 0 39-22.3 86.3-49.6 47.2-27.3 87.7-50.5 89.9-51.5 6.4-3 15.3-2.6 22.3 1.1 3.1 1.6 37.2 21.3 75.7 43.8 59.3 34.6 70.2 40.6 71 39.3.5-.9 1-9.5 1-19.6 0-19.6-1.2-27.2-6.4-42.8-4.3-12.5-8.5-21.1-15.7-31.9-27.1-40.8-73.1-63.2-122.2-59.7m-355.7 79.3c-33.9 16.5-60.8 48.2-70.6 83.1-14.8 53 5 110.9 49.1 143.7 6 4.5 154.5 91.1 157.8 92 1 .3 60.1-33.4 60.1-34.2.1-.3-39.3-23.2-87.3-50.9-48.1-27.7-88.8-51.7-90.5-53.5-6.4-6.6-6.1-1.8-6.1-98.1 0-48.1-.3-87.5-.7-87.5-.5 0-5.7 2.4-11.8 5.4"/><path d="m564.3 374.2-29.3 17 88 51.2c48.4 28.1 89.4 52.4 91.1 54 6.8 6.5 6.3-.2 6.9 97.5l.5 88.3 12-6.1c35.7-18.1 60.9-49.7 70.3-88.1 13.1-53.6-9-111-54.6-141.9-8.9-6-153.2-89.1-154.5-89-.7.1-14.4 7.8-30.4 17.1"/><path d="m462 433.3-38.5 22.3-.3 42.4c-.1 23.4 0 43.5.3 44.6.3 1.6 10.5 8 39 24.4l38.6 22.1 38.4-22.2 38.5-22.1v-89.4l-38.1-21.9c-21-12.1-38.4-22.1-38.8-22.2-.3-.1-17.9 9.8-39.1 22"/><path d="M612 577.3c0 113.1.3 108.3-6.8 115.2-2.2 2.1-34.2 21.2-77.8 46.4l-74.2 42.7 8.2 5.8c72.7 51.7 175.7 17.6 204-67.5 7.1-21.2 6.9-18.5 7.3-118.5l.4-90.2-2.8-1.9c-3.9-2.7-56.9-33.3-57.7-33.3-.3 0-.6 45.6-.6 101.3"/><path d="M517.5 619.1c-131.7 76.1-120.6 70.4-131 67.6-3.1-.8-33.1-17.6-78.9-44-40.7-23.5-74.1-42.5-74.3-42.3s-.8 5.1-1.3 11c-2.8 31.5 6.9 64.5 26.7 90.8 36.9 49 102.1 66.7 158.4 43 5.2-2.1 43.4-23.5 84.9-47.5l75.5-43.6.3-34.5c.1-19-.1-34.6-.5-34.6-.5.1-27.3 15.4-59.8 34.1"/></g><path fill="#008040" fill-opacity=".8" d="M939 58.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m5.9 877.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M60.5 941c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m873.9 5.2-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9"/><path fill="#008040" fill-opacity=".6" d="M935 54.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M60.4 59.2l-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M944.5 64c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M54 934.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m11 11c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3"/><path fill="teal" fill-opacity=".1" d="M67.9 51.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m872.6 889.5-4 4.3 4.3-4c3.9-3.6 4.7-4.5 3.9-4.5-.2 0-2.1 1.9-4.2 4.2"/><path fill="#008040" fill-opacity=".9" d="M52.9 67.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m887 873-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7"/><path fill="#408080" d="M453.3 140.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m17 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m-11 48c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m173 8c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m-3 48c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m10 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-60.8 255.8c0 24.7.1 34.7.2 22.2.2-12.6.2-32.8 0-45-.1-12.2-.2-2-.2 22.8m-189.1 9c0 8.8.2 12.3.3 7.7.2-4.6.2-11.8 0-16-.1-4.2-.3-.5-.3 8.3m466 47c0 4.9.1 7.1.3 4.8s.2-6.4 0-9-.3-.8-.3 4.2m-48.3 10.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-228.6 51.9c0 19.2.1 27 .2 17.2.2-9.8.2-25.6 0-35-.1-9.5-.3-1.5-.2 17.8M183.2 613c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m48 2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 14c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-48 2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m489.1 51.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-281.5 5.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-37 116c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m21 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m148 56c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m29 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6"/><path fill="#40BF80" d="M591 193.2c4.1 4.5 5 5.3 5 4.5 0-.2-2.1-2.3-4.7-4.7l-4.8-4.5zm-257.6 1-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m39.5 28.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-174 92-3.4 3.8 3.8-3.4c2-1.9 3.7-3.6 3.7-3.8 0-.8-.8 0-4.1 3.4M768 461.5c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m-538 74.8c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m575.9 144.4c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-9 9c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M265 711.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m8 8c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m363.9 49.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-8 8c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M410 807.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5"/><path fill="#80BF80" d="M460.3 140.7c1.5.2 3.9.2 5.5 0 1.5-.2.2-.4-2.8-.4s-4.3.2-2.7.4m-7.5 48c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m19 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M328.5 403c0 48.1.1 67.8.2 43.8.2-24.1.2-63.5 0-87.5-.1-24.1-.2-4.4-.2 43.7m488.6-36.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M770.3 379c0 3 .2 4.3.4 2.7.2-1.5.2-3.9 0-5.5-.2-1.5-.4-.2-.4 2.8m46.8 11.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M193.3 444c0 4.1.2 5.8.4 3.7.2-2 .2-5.4 0-7.5-.2-2-.4-.3-.4 3.8m418.2 132.5c0 56.1.1 78.9.2 50.7.2-28.3.2-74.2 0-102-.1-27.9-.2-4.8-.2 51.3M183.3 622c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3m47.8 10.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m441 55c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M361.3 803.7c2.6.2 6.7.2 9 0s.2-.3-4.8-.3c-4.9 0-6.8.1-4.2.3m175 9c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m-9.5 47c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m20 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><path fill="#80BFBF" d="m396.4 308.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M762 455.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-475 48c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-60 30c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m9 9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m565.9 142.3-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4"/><path fill="#BFBFBF" d="M446.8 141.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m31 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m98.2 38.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m8 7.1c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-244.6 1.7-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m110.4-.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-120.4 10.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m292.4-1.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m27 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-53.8.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-273.1 9.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m224.1 8.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-169.1 2.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-9.9 8.8c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m254.8 17.2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m141.7 5.3c3.3 3.3 6.2 6 6.4 6 .3 0-2.1-2.7-5.4-6s-6.2-6-6.4-6c-.3 0 2.1 2.7 5.4 6M720 273.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m10.5 9.6c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m-517.6 19.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-7 6-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m122.3 5.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-135.3 7.7-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-6 7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m56 11c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-10.9 10-6.5 6.8 6.8-6.5c3.7-3.5 6.7-6.6 6.7-6.7 0-.8-1 .2-7 6.4m584.1 8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-594.2 2.1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M770.2 371c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m47.1 1.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m0 12c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-47.1 2.5c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-624 46c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m47 1c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m616.8 3.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M193.2 453c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m565.8-.6c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M146.2 455c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M775 468.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M720.5 597c0 47.6.1 67 .2 43.3.2-23.8.2-62.8 0-86.5-.1-23.8-.2-4.4-.2 43.2M224 530.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m630.1 11.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M239 545.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-55 10c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m5 6c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m665.1 9.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-77.2 72.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-8.5 9.5-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m-9.5 8.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m50.5 15.5-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m-16.5 16.5c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M269.5 717c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m8.5 7.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-53 13c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m11.5 12.6c3.3 3.3 6.2 6 6.4 6 .3 0-2.1-2.7-5.4-6s-6.2-6-6.4-6c-.3 0 2.1 2.7 5.4 6m126.8 6.7c1.5.2 3.9.2 5.5 0 1.5-.2.2-.4-2.8-.4s-4.3.2-2.7.4m269.6 17-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-180.3 7.2c-.3.4.4 1.7 1.7 2.7 2.2 1.9 2.2 1.9.4-.2-.9-1.1-1.4-2.4-1-2.7.3-.4.4-.7 0-.7-.3 0-.8.4-1.1.9m171.3.8c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m53 13c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-274.9 5c.8.3 2.6 1.5 4 2.7l2.5 2.1-2.4-2.8c-1.4-1.5-3.2-2.7-4-2.6-1.4 0-1.4.1-.1.6m264 7-6.5 6.8 6.8-6.5c6.2-6 7.2-7 6.4-7-.1 0-3.2 3-6.7 6.7m-318.2-4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m34 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m31.2 6.6c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m116.3 3.4c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m15 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m109.6 5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-136.1 41c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m39 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-23 1c2.3.2 6.1.2 8.5 0 2.3-.2.4-.4-4.3-.4s-6.6.2-4.2.4"/><path fill="#FFF" d="M442.7 142.1c-62.8 7.2-117.7 47-144.2 104.4-2.3 4.9-5 11.4-6.1 14.3-1.7 4.8-2.1 5.3-5.9 6.2-41.5 10.4-75.9 32.3-100.7 64-43.5 55.8-51.6 128.8-21.3 191.5 5.9 12.4 14.4 25.6 22.9 35.9 3.3 4.1 5.4 7.5 5.2 8.5-6.4 21.1-8 32.4-8.1 54.6 0 31.3 5.2 54.1 18.5 80.5 9.3 18.5 18.3 31.2 33 46.4 29 30 67.5 48.8 110 53.7 12.7 1.4 39.2.6 49.9-1.5l7.3-1.5 7.6 7.5c23 23 51.7 39.4 83.3 47.4 26.8 6.9 62.8 6.7 89.6-.5 34.2-9.1 61.6-25.3 86.1-51 16.6-17.3 29.9-38.5 38.2-60.5l3.2-8.6 9.1-2.3c46-11.6 86.4-42.7 110.6-85.1 10.7-18.5 17.6-37.9 21.8-61 2.7-14.7 2.4-44.8-.5-60.4-5.7-30.3-19.2-59.5-38.1-82.6l-5.8-7.2 1.8-6c7.5-23.9 9.2-56.6 4.4-82.6-8.5-45.5-36.1-88.8-73.5-115.4-38.4-27.3-88.6-39-133.7-31.3l-9.1 1.6-7.5-7.4c-29.3-28.9-67-46.9-108.2-51.7-10.2-1.1-29.7-1.1-39.8.1m47.8 48.4c11.4 2.5 22.4 6.4 33 11.6 10.7 5.3 25.8 15.4 25.3 17-.2.5-33.4 19.9-73.8 43.2-48.2 27.8-74.9 43.8-77.7 46.5-7.7 7.4-7.3 1.7-7.3 116 0 56.4-.4 101.2-.9 101-.4-.2-14.4-8.1-31-17.7L328 490.7v-87.6c0-57.4.4-90.7 1.1-96.6 3.2-26.1 13.7-50 31-70.9 21.3-25.5 50.8-41.6 86.4-47 6.5-1 37.1.3 44 1.9M665 246.9c29 6.7 55.2 22.4 73.3 43.9 20 23.8 30.7 50.8 32.3 81.7.5 10.4-.7 27.9-2 29.2-.2.3-33.2-18.6-73.3-42-40-23.3-74.1-43.1-75.8-43.9-4.8-2.5-10.5-3-15.5-1.5-2.5.8-44 24.2-92.2 52-48.3 27.9-88 50.7-88.3 50.7s-.5-16.1-.5-35.8v-35.7l75.8-43.7c41.6-24 79.4-45.2 83.9-47.2 9.4-4 20.6-7.3 31.3-9.2 11-2 39.9-1.1 51 1.5M282 403.8c0 95.9-.2 92.9 6.8 100 2.4 2.4 34.3 21.4 91.4 54.2 48.3 27.8 87.8 50.7 87.8 51-.1.9-59 34.5-61.1 34.8-2 .3-140.6-78.9-155.9-89.1-9.5-6.3-28-24.9-34.5-34.7-22.7-34.2-29.2-75.7-17.9-114.8 9-31.1 33.1-61.1 62.4-77.7 6.1-3.5 18.6-9.3 20.3-9.4.4-.1.7 38.5.7 85.7m388.9-4.7c41 23.7 78.1 45.6 82.4 48.7 9.7 7 24.7 22.4 31.4 32.2 10.8 16.1 18.6 35.5 21.9 54.3 2.5 14.1 1.5 40.7-2 54.5-8.9 35.2-30.3 64.3-61.1 83.2-6.7 4.1-16.5 8.9-21.3 10.4l-2.2.7v-87.7c0-85.7 0-87.9-2-91.9-1.1-2.3-3.2-5.3-4.7-6.7-1.6-1.4-42.5-25.6-91-53.8-48.6-28.1-88.3-51.5-88.3-51.9.1-.9 59.2-34.9 60.9-35 .7-.1 34.9 19.3 76 43M540 432.3l38.5 22.3.2 45.4.3 45.5-38.3 22c-21 12.1-38.9 22-39.7 21.9-.9 0-18.8-9.9-39.8-22L423 545.5V500l.1-45.5 38.7-22.2c21.3-12.3 38.9-22.3 39.2-22.2.3 0 17.8 10 39 22.2M642.9 492l30.2 17.4-.4 94.6c-.3 92.2-.4 94.7-2.4 103-8.9 35.6-28.6 64.1-57.8 83.5-34.7 23-80.1 28.7-118.7 14.9-15.3-5.4-43.5-22.2-41.2-24.4.5-.4 34.2-20 74.9-43.4 40.7-23.5 75.1-43.7 76.4-44.9s3.4-4.5 4.7-7.2l2.4-5V577.2c0-56.9.4-103.2.8-103 .5.1 14.5 8.2 31.1 17.8M579 618.7v35.8l-75.8 43.7c-41.6 24-78.7 45-82.4 46.7-30.3 13.6-66.6 15.2-98.8 4.5-46.4-15.5-81.5-57.1-89.6-106.3-1.8-11.4-1.7-41.2.3-43.2.7-.7 24.4 12.4 75.6 42 41 23.6 76.5 43.5 78.8 44.1 2.3.7 6.1.9 8.4.6 3-.5 29-15 93.1-52 48.9-28.3 89.2-51.5 89.7-51.5.4-.1.7 16 .7 35.6"/><path fill="teal" d="m339.4 188.2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m245.1-1.2c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-256.1 12.2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m218.2 19.1c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-179.1 9.9-4 4.3 4.3-4c2.3-2.1 4.2-4 4.2-4.2 0-.8-.9 0-4.5 3.9m-8.6 9.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m392.1-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m13 12.1c3.5 3.6 6.7 6.5 6.9 6.5.3 0-2.4-2.9-5.9-6.5s-6.7-6.5-6.9-6.5c-.3 0 2.4 2.9 5.9 6.5m10 10.9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M730.5 284c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m7.5 8.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-533.1 16.3-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-12.5 12.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m40.1 29-6 6.3 6.3-6c5.8-5.5 6.7-6.5 5.9-6.5-.1 0-2.9 2.8-6.2 6.2M811 437.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-52 16c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m15 15c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-549 62c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m14 14c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M189.5 563c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m578.4 88.7-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7m42 25c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-16.5 16.5-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9M261 706.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m9.5 10.6c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m7.5 6.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-42 27.1c3 3 5.7 5.5 5.9 5.5.3 0-1.9-2.5-4.9-5.5s-5.7-5.5-5.9-5.5c-.3 0 1.9 2.5 4.9 5.5m11 9.9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m393.9 3.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-8.5 9.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m45.5 21.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M404.5 802c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m262.5 4.7-6.5 6.8 6.8-6.5c6.2-6 7.2-7 6.4-7-.1 0-3.2 3-6.7 6.7m-254 3.6c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9"/></svg>}>
		<p></p>
	</Card>

## docs/snippets/discord-cta.mdx

<a className="discord-cta-div" href="https://discord.gg/UgJyyxx7uc" target="_blank" rel="noopener noreferrer">
  <img className="discord-logo" src="https://github.com/AgentOps-AI/agentops/blob/e2ab42a607a9657907dc4d3727d4e07d93c52e94/docs/v1/img/discord.png?raw=true" alt="Discord Logo"/>
  <span className="discord-cta-text">
    Join our Discord community for the latest updates and support!
  </span>
</a>

## docs/snippets/openai-card.mdx

<Card title="OpenAI automates my job postings..." icon={<svg width="28" height="28" xmlns="http://www.w3.org/2000/svg" version="1.0" viewBox="0 0 1000 1000"><path fill="teal" fill-opacity=".2" d="M.1 192.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M.2 808c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3"/><path fill="teal" fill-opacity=".3" d="m64.4 55.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m-9 9-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2"/><path fill="teal" fill-opacity=".5" d="M.2 198c0 1.9.2 2.7.5 1.7.2-.9.2-2.5 0-3.5-.3-.9-.5-.1-.5 1.8m.1 604.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3"/><path fill="#008040" fill-opacity=".7" d="M190.8.7c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m618 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M.5 500c0 164.2.1 231.3.2 149.2.2-82 .2-216.4 0-298.5-.1-82-.2-14.9-.2 149.3"/><g fill="#008040" stroke-width="0"><path d="M185.5 1.1C153.3 4.2 119.7 16 93 33.8 49.7 62.6 19.7 105 7.1 154.9.6 180.7 1 159.2 1 499.8c0 205.5.3 312.6 1 318.2 7.5 62 40.7 115.7 92 149.2 26.8 17.5 58.3 28.6 90.5 31.8 7.8.8 101.4 1 323 .8 293-.3 312.6-.4 321.5-2.1 46.6-8.6 82.9-28 114.6-60.9C974.5 904.6 994 863.2 999 819c.8-6.7 1-103.6.8-325.5l-.3-316-2.8-12.4c-11.1-49.5-35.6-90-72.7-119.8-31.7-25.5-69-40.5-110-44.2-14.1-1.3-614.9-1.3-628.5 0m295.1 140c41.4 4 78.6 21.1 109 50.2l9.2 8.7 9.3-1.4c29.4-4.5 57.6-2.1 86.4 7.5 59.4 19.8 104.4 69.5 119 131.4 3.8 15.9 4.8 27.7 4.2 47.3-.5 17.5-3.2 34.7-7 45.6l-1.4 4 6.1 7.5c28.8 35.8 43.6 83.5 39.8 128.1-5 57.6-33.5 106.3-80.5 137.9-15.4 10.4-32.3 18-52 23.5l-10.9 3.1-3.3 8.5c-11.3 29.3-29 54.5-51.7 73.6-28.7 24.1-61.6 38.2-98.8 42.5-13.1 1.5-39.1.7-52-1.6-35.2-6.3-67.2-22.6-93.4-47.6l-10.1-9.7-7.5 1.4c-11.6 2.1-34.9 2.8-47.5 1.5-61.4-6.6-112.7-40.8-142.8-95.2-5.1-9.3-12.1-26.8-15.1-37.9-4.7-17.5-6-27.2-6-47-.1-19.9 1.4-32.4 5.9-48.6l2.4-8.6-6.1-7.6c-18.6-23.3-31.1-50.2-37-79.4-2-9.9-2.3-14.2-2.3-34.8s.3-24.9 2.3-34.8c13.3-65.5 58.8-117.6 120.7-138.1 6.6-2.2 13.8-4.3 16.1-4.7 4-.6 4.1-.7 6.8-8.2 28.3-76.5 106.3-125.1 188.2-117.1"/><path d="M439.5 190.7c-27.4 4.9-54.3 19.6-73.8 40.3-17.6 18.7-28.6 39.8-34.3 66.1-1.8 7.8-1.9 14.5-1.9 100.8v92.5l29.3 16.9c16.1 9.3 29.6 16.6 30 16.2.4-.3.8-46.4 1-102.3.3-112.7-.1-106.6 7-113.4 2.1-2.1 35.4-21.9 76.7-45.7 40.2-23.2 73.1-42.5 73.3-43 .5-1.4-13.2-10.7-23.2-15.8-11.1-5.6-26.5-10.7-38-12.8-11.1-1.9-35.1-1.8-46.1.2"/><path d="M625.2 245.1c-16.1 1.2-33 5.8-47.5 13-7.5 3.7-150.9 86.1-153.4 88.1-1 .8-1.3 9.1-1.3 35.4 0 18.9.2 34.4.3 34.4.2 0 39-22.3 86.3-49.6 47.2-27.3 87.7-50.5 89.9-51.5 6.4-3 15.3-2.6 22.3 1.1 3.1 1.6 37.2 21.3 75.7 43.8 59.3 34.6 70.2 40.6 71 39.3.5-.9 1-9.5 1-19.6 0-19.6-1.2-27.2-6.4-42.8-4.3-12.5-8.5-21.1-15.7-31.9-27.1-40.8-73.1-63.2-122.2-59.7m-355.7 79.3c-33.9 16.5-60.8 48.2-70.6 83.1-14.8 53 5 110.9 49.1 143.7 6 4.5 154.5 91.1 157.8 92 1 .3 60.1-33.4 60.1-34.2.1-.3-39.3-23.2-87.3-50.9-48.1-27.7-88.8-51.7-90.5-53.5-6.4-6.6-6.1-1.8-6.1-98.1 0-48.1-.3-87.5-.7-87.5-.5 0-5.7 2.4-11.8 5.4"/><path d="m564.3 374.2-29.3 17 88 51.2c48.4 28.1 89.4 52.4 91.1 54 6.8 6.5 6.3-.2 6.9 97.5l.5 88.3 12-6.1c35.7-18.1 60.9-49.7 70.3-88.1 13.1-53.6-9-111-54.6-141.9-8.9-6-153.2-89.1-154.5-89-.7.1-14.4 7.8-30.4 17.1"/><path d="m462 433.3-38.5 22.3-.3 42.4c-.1 23.4 0 43.5.3 44.6.3 1.6 10.5 8 39 24.4l38.6 22.1 38.4-22.2 38.5-22.1v-89.4l-38.1-21.9c-21-12.1-38.4-22.1-38.8-22.2-.3-.1-17.9 9.8-39.1 22"/><path d="M612 577.3c0 113.1.3 108.3-6.8 115.2-2.2 2.1-34.2 21.2-77.8 46.4l-74.2 42.7 8.2 5.8c72.7 51.7 175.7 17.6 204-67.5 7.1-21.2 6.9-18.5 7.3-118.5l.4-90.2-2.8-1.9c-3.9-2.7-56.9-33.3-57.7-33.3-.3 0-.6 45.6-.6 101.3"/><path d="M517.5 619.1c-131.7 76.1-120.6 70.4-131 67.6-3.1-.8-33.1-17.6-78.9-44-40.7-23.5-74.1-42.5-74.3-42.3s-.8 5.1-1.3 11c-2.8 31.5 6.9 64.5 26.7 90.8 36.9 49 102.1 66.7 158.4 43 5.2-2.1 43.4-23.5 84.9-47.5l75.5-43.6.3-34.5c.1-19-.1-34.6-.5-34.6-.5.1-27.3 15.4-59.8 34.1"/></g><path fill="#008040" fill-opacity=".8" d="M939 58.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m5.9 877.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M60.5 941c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m873.9 5.2-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9"/><path fill="#008040" fill-opacity=".6" d="M935 54.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M60.4 59.2l-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M944.5 64c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2M54 934.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m11 11c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3"/><path fill="teal" fill-opacity=".1" d="M67.9 51.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m872.6 889.5-4 4.3 4.3-4c3.9-3.6 4.7-4.5 3.9-4.5-.2 0-2.1 1.9-4.2 4.2"/><path fill="#008040" fill-opacity=".9" d="M52.9 67.7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m887 873-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7"/><path fill="#408080" d="M453.3 140.7c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m17 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m-11 48c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m173 8c2 .2 5.4.2 7.5 0 2-.2.3-.4-3.8-.4s-5.8.2-3.7.4m-3 48c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m10 0c.9.2 2.5.2 3.5 0 .9-.3.1-.5-1.8-.5s-2.7.2-1.7.5m-60.8 255.8c0 24.7.1 34.7.2 22.2.2-12.6.2-32.8 0-45-.1-12.2-.2-2-.2 22.8m-189.1 9c0 8.8.2 12.3.3 7.7.2-4.6.2-11.8 0-16-.1-4.2-.3-.5-.3 8.3m466 47c0 4.9.1 7.1.3 4.8s.2-6.4 0-9-.3-.8-.3 4.2m-48.3 10.1c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-228.6 51.9c0 19.2.1 27 .2 17.2.2-9.8.2-25.6 0-35-.1-9.5-.3-1.5-.2 17.8M183.2 613c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m48 2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m0 14c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-48 2c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m489.1 51.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-281.5 5.2c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m-37 116c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m21 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m148 56c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m29 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6"/><path fill="#40BF80" d="M591 193.2c4.1 4.5 5 5.3 5 4.5 0-.2-2.1-2.3-4.7-4.7l-4.8-4.5zm-257.6 1-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m39.5 28.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-174 92-3.4 3.8 3.8-3.4c2-1.9 3.7-3.6 3.7-3.8 0-.8-.8 0-4.1 3.4M768 461.5c1.9 1.9 3.6 3.5 3.9 3.5s-1-1.6-2.9-3.5-3.6-3.5-3.9-3.5 1 1.6 2.9 3.5m-538 74.8c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m575.9 144.4c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-9 9c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M265 711.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m8 8c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m363.9 49.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-8 8c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M410 807.5c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5"/><path fill="#80BF80" d="M460.3 140.7c1.5.2 3.9.2 5.5 0 1.5-.2.2-.4-2.8-.4s-4.3.2-2.7.4m-7.5 48c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m19 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6M328.5 403c0 48.1.1 67.8.2 43.8.2-24.1.2-63.5 0-87.5-.1-24.1-.2-4.4-.2 43.7m488.6-36.4c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M770.3 379c0 3 .2 4.3.4 2.7.2-1.5.2-3.9 0-5.5-.2-1.5-.4-.2-.4 2.8m46.8 11.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M193.3 444c0 4.1.2 5.8.4 3.7.2-2 .2-5.4 0-7.5-.2-2-.4-.3-.4 3.8m418.2 132.5c0 56.1.1 78.9.2 50.7.2-28.3.2-74.2 0-102-.1-27.9-.2-4.8-.2 51.3M183.3 622c0 3.6.2 5 .4 3.2.2-1.7.2-4.7 0-6.5-.2-1.7-.4-.3-.4 3.3m47.8 10.6c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m441 55c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M361.3 803.7c2.6.2 6.7.2 9 0s.2-.3-4.8-.3c-4.9 0-6.8.1-4.2.3m175 9c1.5.2 3.7.2 5 0 1.2-.2 0-.4-2.8-.4-2.7 0-3.8.2-2.2.4m-9.5 47c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m20 0c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5"/><path fill="#80BFBF" d="m396.4 308.2-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2M762 455.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-475 48c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-60 30c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m9 9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m565.9 142.3-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4"/><path fill="#BFBFBF" d="M446.8 141.7c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m31 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m98.2 38.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m8 7.1c1.3 1.4 2.6 2.5 2.8 2.5.3 0-.5-1.1-1.8-2.5s-2.6-2.5-2.8-2.5c-.3 0 .5 1.1 1.8 2.5m-244.6 1.7-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m110.4-.5c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-120.4 10.5-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m292.4-1.5c.6.2 1.8.2 2.5 0 .6-.3.1-.5-1.3-.5s-1.9.2-1.2.5m27 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-53.8.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-273.1 9.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m224.1 8.7c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-169.1 2.3c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-9.9 8.8c-2.4 2.5-4.2 4.5-3.9 4.5s2.5-2 4.9-4.5 4.2-4.5 3.9-4.5-2.5 2-4.9 4.5m254.8 17.2c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m141.7 5.3c3.3 3.3 6.2 6 6.4 6 .3 0-2.1-2.7-5.4-6s-6.2-6-6.4-6c-.3 0 2.1 2.7 5.4 6M720 273.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m10.5 9.6c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m-517.6 19.7c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-7 6-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m122.3 5.3c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-135.3 7.7-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-6 7c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m56 11c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-10.9 10-6.5 6.8 6.8-6.5c3.7-3.5 6.7-6.6 6.7-6.7 0-.8-1 .2-7 6.4m584.1 8.9c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-594.2 2.1c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M770.2 371c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m47.1 1.5c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m0 12c0 2.2.2 3 .4 1.7.2-1.2.2-3 0-4-.3-.9-.5.1-.4 2.3m-47.1 2.5c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m-624 46c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m47 1c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m616.8 3.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M193.2 453c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3m565.8-.6c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M146.2 455c0 1.4.2 1.9.5 1.2.2-.6.2-1.8 0-2.5-.3-.6-.5-.1-.5 1.3M775 468.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M720.5 597c0 47.6.1 67 .2 43.3.2-23.8.2-62.8 0-86.5-.1-23.8-.2-4.4-.2 43.2M224 530.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m630.1 11.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3M239 545.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-55 10c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m5 6c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m665.1 9.2c0 1.1.3 1.4.6.6.3-.7.2-1.6-.1-1.9-.3-.4-.6.2-.5 1.3m-77.2 72.1c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-8.5 9.5-2.9 3.3 3.3-2.9c1.7-1.7 3.2-3.1 3.2-3.3 0-.8-.8-.1-3.6 2.9m-9.5 8.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m50.5 15.5-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9m-16.5 16.5c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3M269.5 717c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m8.5 7.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-53 13c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m11.5 12.6c3.3 3.3 6.2 6 6.4 6 .3 0-2.1-2.7-5.4-6s-6.2-6-6.4-6c-.3 0 2.1 2.7 5.4 6m126.8 6.7c1.5.2 3.9.2 5.5 0 1.5-.2.2-.4-2.8-.4s-4.3.2-2.7.4m269.6 17-2.4 2.8 2.8-2.4c1.5-1.4 2.7-2.6 2.7-2.8 0-.8-.8-.1-3.1 2.4m-180.3 7.2c-.3.4.4 1.7 1.7 2.7 2.2 1.9 2.2 1.9.4-.2-.9-1.1-1.4-2.4-1-2.7.3-.4.4-.7 0-.7-.3 0-.8.4-1.1.9m171.3.8c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m53 13c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-274.9 5c.8.3 2.6 1.5 4 2.7l2.5 2.1-2.4-2.8c-1.4-1.5-3.2-2.7-4-2.6-1.4 0-1.4.1-.1.6m264 7-6.5 6.8 6.8-6.5c6.2-6 7.2-7 6.4-7-.1 0-3.2 3-6.7 6.7m-318.2-4c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m34 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m31.2 6.6c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9m116.3 3.4c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m15 0c.9.2 2.3.2 3 0 .6-.3-.1-.5-1.8-.5-1.6 0-2.2.2-1.2.5m109.6 5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m-136.1 41c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m39 0c.7.3 1.6.2 1.9-.1.4-.3-.2-.6-1.3-.5-1.1 0-1.4.3-.6.6m-23 1c2.3.2 6.1.2 8.5 0 2.3-.2.4-.4-4.3-.4s-6.6.2-4.2.4"/><path fill="#FFF" d="M442.7 142.1c-62.8 7.2-117.7 47-144.2 104.4-2.3 4.9-5 11.4-6.1 14.3-1.7 4.8-2.1 5.3-5.9 6.2-41.5 10.4-75.9 32.3-100.7 64-43.5 55.8-51.6 128.8-21.3 191.5 5.9 12.4 14.4 25.6 22.9 35.9 3.3 4.1 5.4 7.5 5.2 8.5-6.4 21.1-8 32.4-8.1 54.6 0 31.3 5.2 54.1 18.5 80.5 9.3 18.5 18.3 31.2 33 46.4 29 30 67.5 48.8 110 53.7 12.7 1.4 39.2.6 49.9-1.5l7.3-1.5 7.6 7.5c23 23 51.7 39.4 83.3 47.4 26.8 6.9 62.8 6.7 89.6-.5 34.2-9.1 61.6-25.3 86.1-51 16.6-17.3 29.9-38.5 38.2-60.5l3.2-8.6 9.1-2.3c46-11.6 86.4-42.7 110.6-85.1 10.7-18.5 17.6-37.9 21.8-61 2.7-14.7 2.4-44.8-.5-60.4-5.7-30.3-19.2-59.5-38.1-82.6l-5.8-7.2 1.8-6c7.5-23.9 9.2-56.6 4.4-82.6-8.5-45.5-36.1-88.8-73.5-115.4-38.4-27.3-88.6-39-133.7-31.3l-9.1 1.6-7.5-7.4c-29.3-28.9-67-46.9-108.2-51.7-10.2-1.1-29.7-1.1-39.8.1m47.8 48.4c11.4 2.5 22.4 6.4 33 11.6 10.7 5.3 25.8 15.4 25.3 17-.2.5-33.4 19.9-73.8 43.2-48.2 27.8-74.9 43.8-77.7 46.5-7.7 7.4-7.3 1.7-7.3 116 0 56.4-.4 101.2-.9 101-.4-.2-14.4-8.1-31-17.7L328 490.7v-87.6c0-57.4.4-90.7 1.1-96.6 3.2-26.1 13.7-50 31-70.9 21.3-25.5 50.8-41.6 86.4-47 6.5-1 37.1.3 44 1.9M665 246.9c29 6.7 55.2 22.4 73.3 43.9 20 23.8 30.7 50.8 32.3 81.7.5 10.4-.7 27.9-2 29.2-.2.3-33.2-18.6-73.3-42-40-23.3-74.1-43.1-75.8-43.9-4.8-2.5-10.5-3-15.5-1.5-2.5.8-44 24.2-92.2 52-48.3 27.9-88 50.7-88.3 50.7s-.5-16.1-.5-35.8v-35.7l75.8-43.7c41.6-24 79.4-45.2 83.9-47.2 9.4-4 20.6-7.3 31.3-9.2 11-2 39.9-1.1 51 1.5M282 403.8c0 95.9-.2 92.9 6.8 100 2.4 2.4 34.3 21.4 91.4 54.2 48.3 27.8 87.8 50.7 87.8 51-.1.9-59 34.5-61.1 34.8-2 .3-140.6-78.9-155.9-89.1-9.5-6.3-28-24.9-34.5-34.7-22.7-34.2-29.2-75.7-17.9-114.8 9-31.1 33.1-61.1 62.4-77.7 6.1-3.5 18.6-9.3 20.3-9.4.4-.1.7 38.5.7 85.7m388.9-4.7c41 23.7 78.1 45.6 82.4 48.7 9.7 7 24.7 22.4 31.4 32.2 10.8 16.1 18.6 35.5 21.9 54.3 2.5 14.1 1.5 40.7-2 54.5-8.9 35.2-30.3 64.3-61.1 83.2-6.7 4.1-16.5 8.9-21.3 10.4l-2.2.7v-87.7c0-85.7 0-87.9-2-91.9-1.1-2.3-3.2-5.3-4.7-6.7-1.6-1.4-42.5-25.6-91-53.8-48.6-28.1-88.3-51.5-88.3-51.9.1-.9 59.2-34.9 60.9-35 .7-.1 34.9 19.3 76 43M540 432.3l38.5 22.3.2 45.4.3 45.5-38.3 22c-21 12.1-38.9 22-39.7 21.9-.9 0-18.8-9.9-39.8-22L423 545.5V500l.1-45.5 38.7-22.2c21.3-12.3 38.9-22.3 39.2-22.2.3 0 17.8 10 39 22.2M642.9 492l30.2 17.4-.4 94.6c-.3 92.2-.4 94.7-2.4 103-8.9 35.6-28.6 64.1-57.8 83.5-34.7 23-80.1 28.7-118.7 14.9-15.3-5.4-43.5-22.2-41.2-24.4.5-.4 34.2-20 74.9-43.4 40.7-23.5 75.1-43.7 76.4-44.9s3.4-4.5 4.7-7.2l2.4-5V577.2c0-56.9.4-103.2.8-103 .5.1 14.5 8.2 31.1 17.8M579 618.7v35.8l-75.8 43.7c-41.6 24-78.7 45-82.4 46.7-30.3 13.6-66.6 15.2-98.8 4.5-46.4-15.5-81.5-57.1-89.6-106.3-1.8-11.4-1.7-41.2.3-43.2.7-.7 24.4 12.4 75.6 42 41 23.6 76.5 43.5 78.8 44.1 2.3.7 6.1.9 8.4.6 3-.5 29-15 93.1-52 48.9-28.3 89.2-51.5 89.7-51.5.4-.1.7 16 .7 35.6"/><path fill="teal" d="m339.4 188.2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m245.1-1.2c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m-256.1 12.2-2.9 3.3 3.3-2.9c3-2.8 3.7-3.6 2.9-3.6-.2 0-1.6 1.5-3.3 3.2m218.2 19.1c.3 1 .9 1.5 1.2 1.2s0-1.1-.7-1.8c-1-.9-1.1-.8-.5.6m-179.1 9.9-4 4.3 4.3-4c2.3-2.1 4.2-4 4.2-4.2 0-.8-.9 0-4.5 3.9m-8.6 9.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7m392.1-.3c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m13 12.1c3.5 3.6 6.7 6.5 6.9 6.5.3 0-2.4-2.9-5.9-6.5s-6.7-6.5-6.9-6.5c-.3 0 2.4 2.9 5.9 6.5m10 10.9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M730.5 284c2.1 2.2 4.1 4 4.4 4s-1.3-1.8-3.4-4-4.1-4-4.4-4 1.3 1.8 3.4 4m7.5 8.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-533.1 16.3-2.4 2.8 2.8-2.4c2.5-2.3 3.2-3.1 2.4-3.1-.2 0-1.4 1.2-2.8 2.7m-12.5 12.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m40.1 29-6 6.3 6.3-6c5.8-5.5 6.7-6.5 5.9-6.5-.1 0-2.9 2.8-6.2 6.2M811 437.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-52 16c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m15 15c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-549 62c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m14 14c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3M189.5 563c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m578.4 88.7-3.4 3.8 3.8-3.4c3.4-3.3 4.2-4.1 3.4-4.1-.2 0-1.9 1.7-3.8 3.7m42 25c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-16.5 16.5-1.9 2.3 2.3-1.9c1.2-1.1 2.2-2.1 2.2-2.3 0-.8-.8-.2-2.6 1.9M261 706.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m9.5 10.6c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m7.5 6.4c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m-42 27.1c3 3 5.7 5.5 5.9 5.5.3 0-1.9-2.5-4.9-5.5s-5.7-5.5-5.9-5.5c-.3 0 1.9 2.5 4.9 5.5m11 9.9c0 .2.8 1 1.8 1.7 1.5 1.3 1.6 1.2.3-.4s-2.1-2.1-2.1-1.3m393.9 3.3c-1.3 1.6-1.2 1.7.4.4.9-.7 1.7-1.5 1.7-1.7 0-.8-.8-.3-2.1 1.3m-8.5 9.5-1.9 2.3 2.3-1.9c2.1-1.8 2.7-2.6 1.9-2.6-.2 0-1.2 1-2.3 2.2m45.5 21.5c-1.3 1.6-1.2 1.7.4.4s2.1-2.1 1.3-2.1c-.2 0-1 .8-1.7 1.7M404.5 802c1 1.1 2 2 2.3 2s-.3-.9-1.3-2-2-2-2.3-2 .3.9 1.3 2m262.5 4.7-6.5 6.8 6.8-6.5c6.2-6 7.2-7 6.4-7-.1 0-3.2 3-6.7 6.7m-254 3.6c0 .2 1.5 1.6 3.3 3.3l3.2 2.9-2.9-3.3c-2.8-3-3.6-3.7-3.6-2.9"/></svg>}>
		<p></p>
	</Card>

## docs/snippets/add-code-tooltip.mdx

<Note>
	Make sure to call `agentops.init` before calling any `openai`, `cohere`, `crew`, etc models.
</Note>

## docs/snippets/github-stars.mdx

<Check>Look useful? [Star us on GitHub](https://github.com/AgentOps-AI/agentops)! (you may be our <span id="stars-text">3,000th</span> üòä)</Check>

## docs/snippets/supported-models.mdx

<Note>
	At the time of writing, AgentOps offers first class support for `openai` and `cohere` models. However we support many
	more models such as Llama, Mistral, Claude, and Gemini via LiteLLM. Visit our [LiteLLM docs](/v1/integrations/litellm) to learn more.
</Note>

## docs/snippets/no-mintlify-cards.mdx

<div className='unminted-cards'>
	<div className='intro-row'>
		<div className='intro-card-container'>
			<img className = 'intro-icon-img' src="https://github.com/AgentOps-AI/agentops/blob/3c03341f5129f9f494ca64ed4e8d03b9a0575db4/docs/images/docs-icons/chat.png?raw=true" />
			<p className='intro-card-text'>OpenAI automates my job posts...</p>
		</div>
		<div className='intro-card-container'>
			<p className='intro-card-text trailing'>but with a 50% error rate!</p>
			<img className = 'intro-icon-img trailing-img' src="https://github.com/AgentOps-AI/agentops/blob/438604c3c652a109c90b3d2484f0a18525c55b9e/docs/v1/img/docs-icons/bug-icon.png?raw=true" />
		</div>
	</div>
	<div className='intro-row'>
		<div className='intro-card-container'>
			<img className = 'intro-icon-img' src="https://github.com/AgentOps-AI/agentops/blob/388a8a94603393cd2aa15e1adcd56e7f435839f9/docs/images/docs-icons/crew.png?raw=true" />
			<p className='intro-card-text'>Crew boosts my product on Reddit...</p>
		</div>
		<div className='intro-card-container'>
			<p className='intro-card-text trailing'>but misses key subreddits!</p>
			<img className = 'intro-icon-img trailing-img' src="https://github.com/AgentOps-AI/agentops/blob/438604c3c652a109c90b3d2484f0a18525c55b9e/docs/v1/img/docs-icons/miss-icon.png?raw=true" />
		</div>
	</div>
	<div className='intro-row'>
		<div className='intro-card-container'>
			<img className = 'intro-icon-img' src="https://github.com/AgentOps-AI/agentops/blob/3c03341f5129f9f494ca64ed4e8d03b9a0575db4/docs/images/docs-icons/claude.png?raw=true" />
			<p className='intro-card-text'>Claude auto-replies to my emails...</p>
		</div>
		<div className='intro-card-container'>
			<p className='intro-card-text trailing'>but has the wrong tone!</p>
			<img className = 'intro-icon-img trailing-img' src="https://github.com/AgentOps-AI/agentops/blob/438604c3c652a109c90b3d2484f0a18525c55b9e/docs/v1/img/docs-icons/curse-icon.png?raw=true" />
		</div>
	</div>
	<img className="resolved-stamp" src="https://github.com/AgentOps-AI/agentops/blob/d4b8b4eafa005a5455758dc5ddf1d33ed2f0fcc9/docs/v1/img/docs-icons/stamp.png?raw=true"/>
</div>

## docs/snippets/add-env-tooltip.mdx

<Tip>
	Set your API key as an `.env` variable for easy access.
</Tip>

## docs/v2/quickstart.mdx

---
title: "Quickstart"
description: "Get started with AgentOps in minutes with just 2 lines of code for basic monitoring, and explore powerful decorators for custom tracing."
---

AgentOps is designed for easy integration into your AI agent projects, providing powerful observability with minimal setup. This guide will get you started quickly.

<Check>[Give us a star on GitHub!](https://github.com/AgentOps-AI/agentops) Your support helps us grow. ‚≠ê</Check>

<Note title="Chat with the docs in your IDE">
Prefer asking your IDE? Install the Mintlify <a href="/v2/usage/mcp-docs"><strong>MCP Docs Server</strong></a> for AgentOps to chat with the docs while you code:
`npx mint-mcp add agentops`
</Note>

## Installation
First, install the AgentOps SDK. We recommend including `python-dotenv` for easy API key management.
<CodeGroup>
  ```bash pip 
  pip install agentops python-dotenv
  ```
  ```bash poetry
  poetry add agentops python-dotenv
  ```
  ```bash uv
  uv add agentops python-dotenv
  ```
</CodeGroup>

## Initial Setup (2 Lines of Code)

At its simplest, AgentOps can start monitoring your supported LLM and agent framework calls with just two lines of Python code.

1.  **Import AgentOps**: Add `import agentops` to your script.
2.  **Initialize AgentOps**: Call `agentops.init()` with your API key.

<CodeGroup>
```python Python
import agentops
import os
from dotenv import load_dotenv

# Load environment variables (recommended for API keys)
load_dotenv()

# Initialize AgentOps
# The API key can be passed directly or set as an environment variable AGENTOPS_API_KEY
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") 
agentops.init(AGENTOPS_API_KEY) 

# That's it for basic auto-instrumentation!
# If you're using a supported library (like OpenAI, LangChain, CrewAI, etc.),
# AgentOps will now automatically track LLM calls and agent actions.
```
</CodeGroup>

### Setting Your AgentOps API Key
You need an AgentOps API key to send data to your dashboard.
- Get your API key from the [AgentOps Dashboard](https://app.agentops.ai/settings/projects).

It's best practice to set your API key as an environment variable.
<CodeGroup>
  ```bash Export to CLI
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>
If you use a `.env` file, make sure `load_dotenv()` is called before `agentops.init()`.

## Running Your Agent & Viewing Traces

After adding the two lines and ensuring your API key is set up:
1.  Run your agent application as you normally would.
2.  AgentOps will automatically instrument supported libraries and send trace data.
3.  Visit your [AgentOps Dashboard](https://app.agentops.ai/traces) to observe your agent's operations!


## Beyond Automatic Instrumentation: Decorators

While AgentOps automatically instruments many popular libraries, you can gain finer-grained control and track custom parts of your code using our powerful decorators. This allows you to define specific operations, group logic under named agents, track tool usage with costs, and create custom traces.

### Tracking Custom Operations with `@operation`
Instrument any function in your code to create spans that track its execution, parameters, and return values. These operations will appear in your session visualization alongside LLM calls.
```python
from agentops.sdk.decorators import operation

@operation
def process_data(data):
    # Your function logic here
    processed_result = data.upper()
    # agentops.record(Events("Processed Data", result=processed_result)) # Optional: record specific events
    return processed_result

# Example usage:
# my_data = "example input"
# output = process_data(my_data)
```

### Tracking Agent Logic with `@agent`
If you structure your system with specific named agents (e.g., classes), use the `@agent` decorator on the class and `@operation` on its methods to group all downstream operations under that agent's context.
```python
from agentops.sdk.decorators import agent, operation

@agent(name="MyCustomAgent") # You can provide a name for the agent
class MyAgent:
    def __init__(self, agent_id):
        self.agent_id = agent_id # agent_id is a reserved parameter for AgentOps
        
    @operation
    def perform_task(self, task_description):
        # Agent task logic here
        # This could include LLM calls or calls to other @operation decorated functions
        return f"Agent {self.agent_id} completed: {task_description}"

# Example usage:
# research_agent = MyAgent(agent_id="researcher-001")
# result = research_agent.perform_task("Analyze market trends")
```

### Tracking Tools with `@tool`
Track the usage of specific tools or functions, and optionally associate costs with them. This data will be aggregated in your dashboard.
```python
from agentops.sdk.decorators import tool

@tool(name="WebSearchTool", cost=0.05) # Cost is optional
def web_search(query: str) -> str:
    # Tool logic here
    return f"Search results for: {query}"
    
@tool # No cost specified
def calculator(expression: str) -> str:
    try:
        return str(eval(expression))
    except Exception as e:
        return f"Error: {e}"

# Example usage:
# search_result = web_search("AgentOps features")
# calculation = calculator("2 + 2")
```

### Grouping with Traces (`@trace` or manual)
Create custom traces to group a sequence of operations or define logical units of work. You can use the `@trace` decorator or manage traces manually for more complex scenarios.
<Note>If `auto_start_session=False` in `agentops.init()`, you must use `@trace` or `agentops.start_trace()` for any data to be recorded.</Note>

```python
from agentops.sdk.decorators import trace
# Assuming MyAgent and web_search are defined as above

# Option 1: Using the @trace decorator
@trace(name="MyMainWorkflow", tags=["main-flow"])
def my_workflow_decorated(task_to_perform):
    # Your workflow code here
    main_agent = MyAgent(agent_id="workflow-agent") # Assuming MyAgent is defined
    result = main_agent.perform_task(task_to_perform)
    # Example of using a tool within the trace
    tool_result = web_search(f"details for {task_to_perform}") # Assuming web_search is defined
    return result, tool_result
    
# result_decorated = my_workflow_decorated("complex data processing")

# Option 2: Managing traces manually
# import agentops # Already imported

# custom_trace = agentops.start_trace(name="MyManualWorkflow", tags=["manual-flow"])
# try:
#     # Your code here
#     main_agent = MyAgent(agent_id="manual-workflow-agent") # Assuming MyAgent is defined
#     result = main_agent.perform_task("another complex task")
#     tool_result = web_search(f"info for {result}") # Assuming web_search is defined
#     agentops.end_trace(custom_trace, end_state="Success", end_prompt=f"Completed: {result}")
# except Exception as e:
#     if custom_trace: # Ensure trace was started before trying to end it
#         agentops.end_trace(custom_trace, end_state="Fail", error_message=str(e))
#     raise
```

### Updating Trace Metadata

You can also update metadata on running traces to add context or track progress:

```python
from agentops import update_trace_metadata

# Update metadata during trace execution
update_trace_metadata({
    "operation_name": "AI Agent Processing",
    "processing_stage": "data_validation",
    "records_processed": 1500,
    "user_id": "user_123",
    "tags": ["validation", "production"]
})
```

## Complete Example with Decorators

Here's a consolidated example showcasing how these decorators can work together:
```python
import agentops
from agentops.sdk.decorators import agent, operation, tool, trace
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")

# Initialize AgentOps. 
# Set auto_start_session=False because @trace will manage the session.
agentops.init(AGENTOPS_API_KEY, auto_start_session=False, tags=["quickstart-complete-example"])

# Define a tool
@tool(name="AdvancedSearch", cost=0.02)
def advanced_web_search(query: str) -> str:
    # Simulate a more advanced search
    return f"Advanced search results for '{query}': [Details...]"

# Define an agent class
@agent(name="ResearchSpecialistAgent")
class ResearchAgent:
    def __init__(self, agent_id: str):
        self.agent_id = agent_id # This will be used as the agent_id in AgentOps
        
    @operation(name="ConductResearch")
    def conduct_research(self, research_topic: str) -> str:
        # Use the tool within the agent's operation
        search_results = advanced_web_search(f"Deep dive into {research_topic}")
        # Simulate further processing
        analysis = f"Analysis of '{research_topic}': Based on '{search_results}', the key findings are..."
        return analysis

# Define a workflow using the @trace decorator
@trace(name="FullResearchWorkflow", tags=["research", "analysis", "example"])
def run_full_research_workflow(topic: str) -> str:
    specialist_agent = ResearchAgent(agent_id="researcher-alpha-007")
    research_findings = specialist_agent.conduct_research(topic)
    
    final_report = f"Research Report for '{topic}':\n{research_findings}"
    # agentops.record(Events("ReportGenerated", details=final_report)) # Optional: record a custom event
    return final_report

# Execute the workflow
final_output = run_full_research_workflow("AI in healthcare")
print(final_output)
```

## Next Steps

You've seen how to get started with AgentOps! Explore further to leverage its full potential:

<CardGroup cols={2}>
  <Card
    title="Integrations"
    icon="plug"
    href="/v2/integrations"
  >
    See how AgentOps automatically instruments popular LLM and agent frameworks.
  </Card>
  <Card
    title="Examples"
    icon="square-code"
    href="/v2/examples"
  >
    Explore detailed examples for various use cases and integrations.
  </Card>
  <Card title="SDK Reference" icon="book-open" href="/v2/sdk-reference">
    Dive deeper into the AgentOps SDK capabilities and API.
  </Card>
  <Card title="Trace Decorator" icon="code" href="/v2/usage/trace-decorator">
    Learn how to group operations and create custom traces using the @trace decorator.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/introduction.mdx

---
title: "Introduction"
description: "AgentOps is the developer favorite platform for testing, debugging, and deploying AI agents and LLM apps."
---
<iframe
  width="100%"
  height="300"
  src="https://www.youtube.com/embed/EU_A3_ux09s?si=I05b3Jf5eufCy3R-"
  title="AgentOps Introduction"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

<Note title="Chat with the docs in your IDE">
Prefer asking your IDE? Install the Mintlify <a href="/v2/usage/mcp-docs"><strong>MCP Docs Server</strong></a> for AgentOps to chat with the docs while you code:
`npx mint-mcp add agentops`
</Note>


## Integrate with developer favorite LLM providers and agent frameworks

### Agent Frameworks

<CardGroup cols={2}>
  <Card title="AG2" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ag2/ag2-logo.svg?raw=true" alt="AG2"/>} iconType="image" href="/v2/integrations/ag2" />
  <Card title="Agno" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/agno/agno.png?raw=true" alt="Agno" />} iconType="image" href="/v2/integrations/agno" />
  <Card title="AutoGen" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/autogen/autogen-logo.svg?raw=true" alt="AutoGen" />} iconType="image" href="/v2/integrations/autogen" />
  <Card title="CrewAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/crewai/crewai-logo.png?raw=true" alt="CrewAI" />} iconType="image" href="/v2/integrations/crewai" />
  <Card title="Google ADK" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/google-adk/google-adk-logo.png?raw=true" alt="Google ADK" />} iconType="image" href="/v2/integrations/google_adk" />
  <Card title="LangChain" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/langchain/langchain-logo.svg?raw=true" alt="LangChain" />} iconType="image" href="/v2/integrations/langchain" />
  <Card title="OpenAI Agents (Python)" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI Agents Python" />} iconType="image" href="/v2/integrations/openai_agents_python" />
  <Card title="OpenAI Agents (TypeScript)" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI Agents JS" />} iconType="image" href="/v2/integrations/openai_agents_js" />
  <Card title="Smolagents" icon={<img src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/huggingface/hf-logo.png?raw=true" alt="smolagents" />} iconType="image" href="/v2/integrations/smolagents" />
</CardGroup>

### LLM Providers

<CardGroup cols={2}>
  <Card title="Anthropic" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/anthropic/anthropic_icon_slate.png?raw=true" alt="Anthropic" />} iconType="image" href="/v2/integrations/anthropic" />
  <Card title="Google Generative AI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/deepmind/gemini-logo.png?raw=true" alt="Gemini" />} iconType="image" href="/v2/integrations/google_generative_ai" />
  <Card title="OpenAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI" />} iconType="image" href="/v2/integrations/openai" />
  <Card title="LiteLLM" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/litellm/litellm.png?raw=true" alt="LiteLLM" />} iconType="image" href="/v2/integrations/litellm" />
  <Card title="Watsonx" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ibm/ibm-logo.svg?raw=true" alt="IBM" />} iconType="image" href="/v2/integrations/ibm_watsonx_ai" />
  <Card title="x.AI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/xai/xai-logo.png?raw=true" alt="x.AI" />} iconType="image" href="/v2/integrations/xai" />
  <Card title="Mem0" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/mem0/mem0.png?raw=true" alt="Mem0" />} iconType="image" href="/v2/integrations/mem0" />
</CardGroup>

Observability and monitoring for your AI agents and LLM apps. And we do it all in just two lines of code...
<CodeGroup>
	```python python
	import agentops
	agentops.init(<INSERT YOUR API KEY HERE>)
	```
</CodeGroup>
... that logs everything back to your AgentOps Dashboard.

<Note>
AgentOps is also available for TypeScript/JavaScript applications. Check out our [TypeScript SDK guide](/v2/usage/typescript-sdk) for Node.js projects.
</Note>

That's it! AgentOps will automatically instrument your code and start tracking traces.

Need more control? You can create custom traces using the `@trace` decorator (recommended) or manage traces manually for advanced use cases:

<CodeGroup>
	```python python
	import agentops
	from agentops.sdk.decorators import trace
	
	agentops.init(<INSERT YOUR API KEY HERE>, auto_start_session=False)
	
	@trace(name="my-workflow", tags=["production"])
	def my_workflow():
		# Your code here
		return "Workflow completed"
	
	```
</CodeGroup>

You can also set a custom trace name during initialization:

<CodeGroup>
	```python python
	import agentops
	agentops.init(<INSERT YOUR API KEY HERE>, trace_name="custom-trace-name")
	```
</CodeGroup>

## The AgentOps Dashboard

<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) to bookmark on GitHub, save for later üñáÔ∏è)</Check>

With just two lines of code, you can free yourself from the chains of the terminal and, instead, visualize your agents' behavior
in your AgentOps Dashboard. After setting up AgentOps, each execution of your program is recorded as a session and the above
data is automatically recorded for you.

The examples below were captured with two lines of code.

### Session Drilldown
Here you will find a list of all of your previously recorded sessions and useful data about each such as total execution time.
You also get helpful debugging info such as any SDK versions you were on if you're building on a supported agent framework like Crew or AutoGen.
LLM calls are presented as a familiar chat history view, and charts give you a breakdown of the types of events that were called and how long they took.

<Frame type="glass" caption="Session Summary">
  <img height="200" src="/images/session-summary.gif" />
</Frame>

Find any past sessions from your Session Drawer.
<Frame type="glass" caption="Session Drawer">
  <img height="250" src="/images/session-drawer.gif" />
</Frame>

Most powerful of all is the Session Waterfall. On the left, a time visualization of all your LLM calls, Action events, Tool calls, and Errors.
On the right, specific details about the event you've selected on the waterfall. For instance the exact prompt and completion for a given LLM call. 
Most of which has been automatically recorded for you.

<Frame type="glass" caption="Session Waterfall">
  <img height="200" src="/images/session-waterfall.gif" />
</Frame>


### Session Overview
View a meta-analysis of all of your sessions in a single view.
<Frame type="glass" caption="Session Overview">
  <img height="200" src="/images/overview.png" />
</Frame>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/usage/tracking-agents.mdx

---
title: "Tracking Agents"
description: "Associate operations with specific named agents"
---

AgentOps automatically tracks LLM interactions in your application. For more detailed tracking, especially in multi-agent systems, you can use the `@agent` decorator to associate operations with specific agents.

## Using the Agent Decorator

For structured tracking in complex applications, you can use the `@agent` decorator to explicitly identify different agents in your system:

```python
import agentops
from agentops.sdk.decorators import agent, operation, trace
from openai import OpenAI

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

# Create a decorated agent class
@agent(name='ResearchAgent')
class MyAgent:
    def __init__(self):
        self.client = OpenAI()
        
    @operation
    def search(self, query):
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": f"Research about: {query}"}]
        )
        return response.choices[0].message.content

# Create a trace to group the agent operations
@trace(name="research-workflow")
def research_workflow(topic):
    agent = MyAgent()
    result = agent.search(topic)
    return result

# Execute the function to properly register the agent span
result = research_workflow("quantum computing")
```

If you don't specify a name, the agent will use the class name by default:

```python
@agent
class ResearchAgent:
    # This agent will have the name "ResearchAgent"
    pass
```

## Basic Agent Tracking (Simple Applications)

For simple applications, AgentOps will automatically track your LLM calls without additional configuration:

```python
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init("your-api-key")

# Create a simple agent function
def research_agent(query):
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": f"Research about: {query}"}]
    )
    return response.choices[0].message.content

# Use your agent - all LLM calls will be tracked automatically
result = research_agent("quantum computing")
```

## Multi-Agent Systems

For complex multi-agent systems, you can organize multiple agents within a single trace:

```python
import agentops
from agentops.sdk.decorators import agent, operation, tool, trace

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@agent
class DataCollectionAgent:
    @tool(cost=0.02)
    def fetch_data(self, source):
        return f"Data from {source}"

@agent  
class AnalysisAgent:
    @operation
    def analyze_data(self, data):
        return f"Analysis of {data}"

@agent
class ReportingAgent:
    @tool(cost=0.01)
    def generate_report(self, analysis):
        return f"Report: {analysis}"

@trace(name="multi-agent-workflow")
def collaborative_workflow(data_source):
    """Workflow using multiple specialized agents"""
    
    # Data collection
    collector = DataCollectionAgent()
    raw_data = collector.fetch_data(data_source)
    
    # Analysis
    analyzer = AnalysisAgent()
    analysis = analyzer.analyze_data(raw_data)
    
    # Reporting
    reporter = ReportingAgent()
    report = reporter.generate_report(analysis)
    
    return {
        "source": data_source,
        "analysis": analysis,
        "report": report
    }

# Run the collaborative workflow
result = collaborative_workflow("customer_database")
```

## Agent Communication and Coordination

You can track complex agent interactions and communication patterns:

```python
import agentops
from agentops.sdk.decorators import agent, operation, tool, trace

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@agent
class CoordinatorAgent:
    def __init__(self):
        self.task_queue = []
    
    @operation
    def assign_task(self, task, agent_type):
        self.task_queue.append({"task": task, "agent": agent_type})
        return f"Task assigned to {agent_type}: {task}"
    
    @operation
    def collect_results(self, results):
        return f"Collected {len(results)} results"

@agent
class WorkerAgent:
    def __init__(self, agent_id):
        self.agent_id = agent_id
    
    @tool(cost=0.05)
    def process_task(self, task):
        return f"Agent {self.agent_id} processed: {task}"

@trace(name="coordinated-processing")
def coordinated_processing_workflow(tasks):
    """Workflow with agent coordination"""
    coordinator = CoordinatorAgent()
    workers = [WorkerAgent(f"worker_{i}") for i in range(3)]
    
    # Assign tasks
    assignments = []
    for i, task in enumerate(tasks):
        worker_type = f"worker_{i % len(workers)}"
        assignment = coordinator.assign_task(task, worker_type)
        assignments.append(assignment)
    
    # Process tasks
    results = []
    for i, task in enumerate(tasks):
        worker = workers[i % len(workers)]
        result = worker.process_task(task)
        results.append(result)
    
    # Collect results
    summary = coordinator.collect_results(results)
    
    return {
        "assignments": assignments,
        "results": results,
        "summary": summary
    }

# Run coordinated workflow
tasks = ["analyze_data", "generate_report", "send_notification"]
result = coordinated_processing_workflow(tasks)
```

## Dashboard Visualization

All operations are automatically associated with the agent that originated them. Agents are given a name which is what you will see in the dashboard.

<Frame type="glass" caption="Operations are labeled with the name of the Agent that originated them">
  <img height="200" src="/images/agent-name.png" />
</Frame>

## Best Practices

1. **Start Simple**: For most applications, just using `agentops.init()` is sufficient.

2. **Use Decorators When Needed**: Add the `@agent` decorator when you need to clearly distinguish between multiple agents in your system.

3. **Meaningful Names**: Choose descriptive names for your agents to make them easier to identify in the dashboard.

4. **Organize with Traces**: Use the `@trace` decorator to group related agent operations into logical workflows.

5. **Track Costs**: Use the `@tool` decorator with cost parameters to track the expenses associated with agent operations.

6. **Agent Specialization**: Create specialized agents for different types of tasks to improve observability and maintainability.

## Migration from Session Decorator

If you're migrating from the legacy `@session` decorator, replace it with the `@trace` decorator:

```python
# New approach (recommended)
from agentops.sdk.decorators import trace, agent

@trace(name="my-workflow")
def my_workflow():
    # workflow code
    pass

# Old approach (deprecated)
from agentops.sdk.decorators import session, agent

@session
def my_workflow():
    # workflow code
    pass
```

The `@trace` decorator provides the same functionality as the legacy `@session` decorator but with more flexibility and better integration with the new trace management features.

## docs/v2/usage/dashboard-info.mdx

---
title: "Dashboard"
description: "Visualize your AgentOps analysis."
---

## Insights Dashboard

You need better insights to turn error-prone AI into stable workflows. Here's your new best friend.

### Session Drilldown
Here you will find a list of all of your previously recorded sessions and useful data about each such as total execution time.
You also get helpful debugging info such as any SDK versions you were on if you're building on a supported agent framework like Crew or AutoGen.
LLM calls are presented as a familiar chat history view, and charts give you a breakdown of the types of events that were called and how long they took.

<Frame type="glass" caption="Session Summary">
  <img height="200" src="/images/session-summary.gif" />
</Frame>

Find any past sessions from your Session Drawer.
<Frame type="glass" caption="Session Drawer">
  <img height="250" src="/images/session-drawer.gif" />
</Frame>

Most powerful of all is the Session Waterfall. On the left, a time visualization of all your LLM calls, Action events, Tool calls, and Errors.
On the right, specific details about the event you've selected on the waterfall. For instance the exact prompt and completion for a given LLM call.
Most of which has been automatically recorded for you.

<Frame type="glass" caption="Session Waterfall">
  <img height="200" src="/images/session-waterfall.gif" />
</Frame>


### Session Overview
View a meta-analysis of all of your sessions in a single view.
<Frame type="glass" caption="Session Overview">
  <img height="200" src="/images/overview.png" />
</Frame>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/intro.css"></script>

## docs/v2/usage/mcp-docs.mdx

---
title: "MCP Docs"
description: "Chat with the AgentOps documentation directly from your IDE using the Mintlify MCP Docs Server."
---


<Note>
Looking for the AgentOps API tools instead? See the <a href="/v2/usage/mcp-server">MCP Server</a> guide.
</Note>

# MCP Docs Server

The **Mintlify MCP Docs Server** gives you and your coding agents instant, programmatic access to every page in the AgentOps docs. It works in **Cursor**, **Windsurf**, **VS Code**, **Zed**, **Claude Code**, or any other tool that speaks the [Model Context Protocol](https://modelcontextprotocol.io/).

## Installation

```bash
npx mint-mcp add agentops
```

Run the command above in any folder and your IDE will automatically register the docs server. No extra configuration required.

## How it works

When you (or your in-editor AI assistant) ask a question, the IDE sends a request to the MCP Docs Server. The server searches the AgentOps docs and returns the most relevant sections so the assistant can craft a precise answer.

> **Example prompt**
> "How do I record custom spans with the `@operation` decorator?"

## What you can ask

Here are a few ideas to get you started:

### Add features
- "Add a chat interface with streaming support to my app"
- "Instrument my agent with the `@trace` decorator"

### Ask about integrations
- "How do I integrate AgentOps with the Vercel AI SDK?"
- "Show me a working example for CrewAI"

### Debug or update existing code
- "My trace isn't showing spans‚Äîwhat could be wrong?"
- "How do I customize the styling of the Session Waterfall?"

If the answer lives in the docs, the server will find it.

## Common issues

### Server not starting
1. Ensure `npx` is installed and working.
2. Check for other MCP servers running on the same port.
3. Verify your configuration file syntax.
4. On Windows, confirm Node.js and npm are installed.

### Tool calls failing
1. Restart the MCP Docs Server and/or your IDE.
2. Update to the latest version of your IDE.
3. Confirm that the AgentOps docs server appears in your IDE‚Äôs list of MCP servers.

## docs/v2/usage/public-api.mdx

---
title: "Public API"
description: "Read-only HTTP API for accessing AgentOps trace and span data"
---

# Public API

The AgentOps Public API provides read-only HTTP access to your monitoring data. This RESTful API allows you to retrieve trace information, span details, and metrics from any application or framework, regardless of programming language.

<Note>
This is a **read-only API** for accessing existing data. To create traces and spans, use the [AgentOps SDK](/v2/quickstart) or our instrumentation libraries.
</Note>

## Base URL

All API requests should be made to:
```
https://api.agentops.ai
```

## Authentication

The API uses JWT token authentication. You'll need to exchange your API key for a JWT token first.

### Get Access Token

Convert your API key to a bearer token for API access.

<CodeGroup>
```bash curl
curl -X POST https://api.agentops.ai/public/v1/auth/access_token \
  -H "Content-Type: application/json" \
  -d '{
    "api_key": "YOUR_API_KEY"
  }'
```

```json Response
{
  "bearer": "eyJhbGciOiJIUzI1NiIs..."
}
```

```json Error Response
{
  "detail": [
    {
      "loc": ["body", "api_key"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ]
}
```
</CodeGroup>

**Important**: Bearer tokens are valid for **30 days**. Store them securely and refresh before expiration.

## Core Endpoints

### Get Project Information

Retrieve details about your current project.

<CodeGroup>
```bash curl
curl -X GET https://api.agentops.ai/public/v1/project \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
```

```json Response
{
  "id": "proj_abc123",
  "name": "My AI Project",
  "environment": "production"
}
```
</CodeGroup>

This endpoint returns information about the project associated with your API key.

### Get Trace Details

Retrieve comprehensive information about a specific trace, including all its spans.

<CodeGroup>
```bash curl
curl -X GET https://api.agentops.ai/public/v1/traces/trace_123 \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
```

```json Response
{
  "trace_id": "trace_123",
  "project_id": "proj_abc123",
  "tags": ["production", "chatbot", "gpt-4"],
  "spans": [
    {
      "span_id": "span_456",
      "parent_span_id": null,
      "span_name": "User Query Processing",
      "span_kind": "SPAN_KIND_INTERNAL",
      "start_time": "2024-03-14T12:00:00.000Z",
      "end_time": "2024-03-14T12:00:05.000Z",
      "duration": 5000,
      "status_code": "STATUS_CODE_OK",
      "status_message": "Success"
    },
    {
      "span_id": "span_789",
      "parent_span_id": "span_456",
      "span_name": "OpenAI GPT-4 Call",
      "span_kind": "SPAN_KIND_CLIENT",
      "start_time": "2024-03-14T12:00:01.000Z",
      "end_time": "2024-03-14T12:00:03.000Z",
      "duration": 2000,
      "status_code": "STATUS_CODE_OK",
      "status_message": "Success"
    }
  ]
}
```

```json Error Response
{
  "detail": [
    {
      "loc": ["path", "trace_id"],
      "msg": "trace not found",
      "type": "value_error.not_found"
    }
  ]
}
```
</CodeGroup>

**Parameters:**
- `trace_id` (path, required): The unique identifier of the trace

**Response Fields:**
- `trace_id`: Unique trace identifier
- `project_id`: Associated project ID
- `tags`: Array of tags associated with the trace
- `spans`: Array of span summaries within the trace

### Get Trace Metrics

Retrieve aggregated metrics and statistics for a trace.

<CodeGroup>
```bash curl
curl -X GET https://api.agentops.ai/public/v1/traces/trace_123/metrics \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
```

```json Response
{
  "span_count": 5,
  "trace_count": 1,
  "success_count": 4,
  "fail_count": 1,
  "indeterminate_count": 0,
  "prompt_tokens": 150,
  "completion_tokens": 75,
  "cache_read_input_tokens": 0,
  "reasoning_tokens": 25,
  "total_tokens": 250,
  "prompt_cost": "0.0030",
  "completion_cost": "0.0015",
  "average_cost_per_trace": "0.0045",
  "total_cost": "0.0045"
}
```
</CodeGroup>

**Metrics Explained:**
- `span_count`: Total number of spans in the trace
- `success_count`/`fail_count`/`indeterminate_count`: Status breakdown
- `*_tokens`: Token usage breakdown by type
- `*_cost`: Cost calculations in USD

### Get Span Details

Retrieve comprehensive information about a specific span, including full attribute payloads.

<CodeGroup>
```bash curl
curl -X GET https://api.agentops.ai/public/v1/spans/span_456 \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
```

```json Response
{
  "span_id": "span_456",
  "parent_span_id": null,
  "span_name": "User Query Processing",
  "span_kind": "SPAN_KIND_INTERNAL",
  "service_name": "chatbot-service",
  "start_time": "2024-03-14T12:00:00.000Z",
  "end_time": "2024-03-14T12:00:05.000Z",
  "duration": 5000,
  "status_code": "STATUS_CODE_OK",
  "status_message": "Success",
  "attributes": {
    "llm.model": "gpt-4-turbo",
    "llm.prompt": "What is the weather like today?",
    "llm.completion": "I need your location to provide weather information.",
    "llm.usage.prompt_tokens": 50,
    "llm.usage.completion_tokens": 25
  },
  "resource_attributes": {
    "service.name": "chatbot-service",
    "service.version": "1.2.3"
  },
  "span_attributes": {
    "user_id": "user_123",
    "session_id": "session_456"
  }
}
```
</CodeGroup>

**Parameters:**
- `span_id` (path, required): The unique identifier of the span

**Response Fields:**
- `attributes`: Core span data (LLM calls, tool usage, etc.)
- `resource_attributes`: Service and infrastructure metadata
- `span_attributes`: Custom attributes set by your application

### Get Span Metrics

Retrieve detailed metrics for a specific span.

<CodeGroup>
```bash curl
curl -X GET https://api.agentops.ai/public/v1/spans/span_456/metrics \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
```

```json Response
{
  "total_tokens": 75,
  "prompt_tokens": 50,
  "completion_tokens": 25,
  "cache_read_input_tokens": 0,
  "reasoning_tokens": 0,
  "success_tokens": 75,
  "fail_tokens": 0,
  "indeterminate_tokens": 0,
  "prompt_cost": "0.0015",
  "completion_cost": "0.0005",
  "total_cost": "0.0020"
}
```
</CodeGroup>

## MCP Server

AgentOps provides a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server that exposes the Public API as tools for AI assistants. This allows AI models to directly query your AgentOps data during conversations.

### Configuration

Create an MCP server configuration file (typically `mcp_config.json`):

**Python-based configuration:**
```json
{
  "mcpServers": {
    "agentops": {
      "command": "python",
      "args": ["-m", "agentops.mcp.server"],
      "env": {
        "AGENTOPS_API_KEY": "your-api-key-here"
      }
    }
  }
}
```

**Docker-based configuration:**
```json
{
  "mcpServers": {
    "agentops": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e",
        "AGENTOPS_API_KEY",
        "agentops/agentops-mcp:latest"
      ],
      "env": {
        "AGENTOPS_API_KEY": "your-agentops-api-key-here"
      }
    }
  }
}
```

### Available Tools

The MCP server exposes the following tools that mirror the Public API endpoints:

#### `auth`
Authorize using an AgentOps project API key.
- **Parameters**: `api_key` (string) - Your AgentOps project API key
- **Usage**: The server will automatically prompt for authentication when needed

#### `get_project`
Get details about the current project.
- **Parameters**: None
- **Returns**: Project information including ID, name, and environment

#### `get_trace`
Get comprehensive trace information by ID.
- **Parameters**: `trace_id` (string) - The trace identifier
- **Returns**: Trace details with associated spans

#### `get_trace_metrics`
Get aggregated metrics for a specific trace.
- **Parameters**: `trace_id` (string) - The trace identifier
- **Returns**: Cost, token usage, and performance metrics

#### `get_span`
Get detailed span information by ID.
- **Parameters**: `span_id` (string) - The span identifier
- **Returns**: Complete span data including attributes

#### `get_span_metrics`
Get metrics for a specific span.
- **Parameters**: `span_id` (string) - The span identifier
- **Returns**: Span-specific cost and token metrics

### Environment Variables

The MCP server supports the following environment variables:

- `AGENTOPS_API_KEY`: Your AgentOps project API key
- `HOST`: API endpoint (defaults to `https://api.agentops.ai`)


## docs/v2/usage/mcp-server.mdx

---
title: "MCP Server"
description: "MCP server for accessing AgentOps trace and span data"
---

<iframe
  width="100%"
  height="300"
  src="https://www.youtube.com/embed/lTa3Sk8C4f0?si=3r7GO8N1Csh0P9C5RR"
  title="AgentOps MCP Server"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

# MCP Server

AgentOps provides a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server that exposes the Public API as a set of tools for AI assistants. This allows AI models to directly query your AgentOps data during conversations and debug AI agents with greater context.

### Configuration & Installation

Add the AgentOps MCP to your MCP client's configuration file.

**npx configuration:**
```json
{
  "mcpServers": {
    "agentops": {
      "command": "npx",
      "args": [
        "agentops-mcp"
      ],
      "env": {
        "AGENTOPS_API_KEY": ""
      }
    }
  }
}
```

**Cursor Deeplink:**

Add the AgentOps MCP to Cursor with Deeplink.

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=agentops&config=eyJjb21tYW5kIjoibnB4IGFnZW50b3BzLW1jcCIsImVudiI6eyJBR0VOVE9QU19BUElfS0VZIjoiIn19)

**Smithery:**

To install agentops-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@AgentOps-AI/agentops-mcp):

```bash
npx -y @smithery/cli install @AgentOps-AI/agentops-mcp --client claude
```

### Available Tools

The MCP server exposes the following tools that mirror the Public API endpoints:

#### `auth`
Authorize using an AgentOps project API key.
- **Parameters**: `api_key` (string) - Your AgentOps project API key
- **Usage**: The server will automatically prompt for authentication when needed

#### `get_trace`
Get trace information by ID.
- **Parameters**: `trace_id` (string) - The trace identifier
- **Returns**: Trace details and metrics

#### `get_span`
Get span information by ID.
- **Parameters**: `span_id` (string) - The span identifier
- **Returns**: Span attributes and metrics

#### `get_complete_trace`
Get complete trace information by ID.
- **Parameters**: `span_id` (string) - The trace identifier
- **Returns**: Complete trace and associated span details.

### Environment Variables

The MCP server supports the following environment variables:

- `AGENTOPS_API_KEY`: Your AgentOps project API key
- `HOST`: API endpoint (defaults to `https://api.agentops.ai`)


## docs/v2/usage/context-managers.mdx

---
title: "Context Managers"
description: "Use AgentOps traces as Python context managers for automatic lifecycle management"
---

# Context Managers

AgentOps provides native context manager support for traces, allowing you to use Python's `with` statement for automatic trace lifecycle management. This approach ensures traces are properly started and ended, even when exceptions occur.

## Basic Usage

The simplest way to use context managers is with the `start_trace()` function:

```python
import agentops

# Initialize AgentOps
agentops.init(api_key="your-api-key")

# Use context manager for automatic trace management
with agentops.start_trace("my_workflow") as trace:
    # Your code here
    print("Processing data...")
    # Trace automatically ends when exiting the with block
```

The trace will automatically:
- Start when entering the `with` block
- End with "Success" status when exiting normally
- End with "Error" status if an exception occurs
- Clean up resources properly in all cases

## Advanced Usage

### Traces with Tags

You can add tags to traces for better organization and filtering:

```python
import agentops

agentops.init(api_key="your-api-key")

# Using list tags
with agentops.start_trace("data_processing", tags=["batch", "production"]):
    process_batch_data()

# Using dictionary tags for more structured metadata
with agentops.start_trace("user_request", tags={
    "user_id": "12345",
    "request_type": "query",
    "priority": "high"
}):
    handle_user_request()
```

### Parallel Traces

Context managers create independent parallel traces, not parent-child relationships:

```python
import agentops

agentops.init(api_key="your-api-key")

# Sequential parallel traces
with agentops.start_trace("task_1"):
    print("Task 1 executing")

with agentops.start_trace("task_2"):
    print("Task 2 executing")

# Nested context managers create parallel traces
with agentops.start_trace("outer_workflow"):
    print("Outer workflow started")
    
    with agentops.start_trace("inner_task"):
        print("Inner task executing (parallel to outer)")
    
    print("Outer workflow continuing")
```

### Exception Handling

Context managers automatically handle exceptions and set appropriate trace states:

```python
import agentops

agentops.init(api_key="your-api-key")

# Automatic error handling
try:
    with agentops.start_trace("risky_operation"):
        # This will automatically set trace status to "Error"
        raise ValueError("Something went wrong")
except ValueError as e:
    print(f"Caught error: {e}")
    # Trace has already been ended with Error status

# Graceful degradation pattern
try:
    with agentops.start_trace("primary_service"):
        result = call_primary_service()
except ServiceUnavailableError:
    with agentops.start_trace("fallback_service"):
        result = call_fallback_service()
```

### Concurrent Execution

Context managers work seamlessly with threading and asyncio:

<CodeGroup>

```python Threading
import agentops
import threading

agentops.init(api_key="your-api-key")

# With threading
def worker_function(worker_id):
    with agentops.start_trace(f"worker_{worker_id}"):
        # Each thread gets its own independent trace
        process_work(worker_id)

threads = []
for i in range(3):
    thread = threading.Thread(target=worker_function, args=(i,))
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()
```

```python Asyncio
import agentops
import asyncio

agentops.init(api_key="your-api-key")

# With asyncio
async def async_task(task_id):
    with agentops.start_trace(f"async_task_{task_id}"):
        await asyncio.sleep(0.1)  # Simulate async work
        return f"result_{task_id}"

async def main():
    tasks = [async_task(i) for i in range(3)]
    results = await asyncio.gather(*tasks)
    return results

# Run async tasks
results = asyncio.run(main())
```

</CodeGroup>

## Production Patterns

### API Endpoint Monitoring

```python
import agentops
from flask import Flask, request

app = Flask(__name__)
agentops.init(api_key="your-api-key")

@app.route('/api/process', methods=['POST'])
def process_request():
    # Create trace for each API request
    with agentops.start_trace("api_request", tags={
        "endpoint": "/api/process",
        "method": "POST",
        "user_id": request.headers.get("user-id")
    }):
        try:
            data = request.get_json()
            result = process_data(data)
            return {"status": "success", "result": result}
        except Exception as e:
            # Exception automatically sets trace to Error status
            return {"status": "error", "message": str(e)}, 500
```

### Batch Processing

```python
import agentops

agentops.init(api_key="your-api-key")

def process_batch(items):
    with agentops.start_trace("batch_processing", tags={
        "batch_size": len(items),
        "batch_type": "data_processing"
    }):
        successful = 0
        failed = 0
        
        for item in items:
            try:
                with agentops.start_trace("item_processing", tags={
                    "item_id": item.get("id"),
                    "item_type": item.get("type")
                }):
                    process_item(item)
                    successful += 1
            except Exception as e:
                failed += 1
                print(f"Failed to process item {item.get('id')}: {e}")
        
        print(f"Batch completed: {successful} successful, {failed} failed")
```

### Retry Logic

```python
import agentops
import time

agentops.init(api_key="your-api-key")

def retry_operation(operation_name, max_retries=3):
    for attempt in range(max_retries):
        try:
            with agentops.start_trace(f"{operation_name}_attempt_{attempt + 1}", tags={
                "operation": operation_name,
                "attempt": attempt + 1,
                "max_retries": max_retries
            }):
                # Your operation here
                result = perform_operation()
                return result  # Success - exit retry loop
                
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"All {max_retries} attempts failed")
                raise
```

## Backward Compatibility

Context managers are fully backward compatible with existing AgentOps code patterns:

<CodeGroup>

```python Manual Management
import agentops

agentops.init(api_key="your-api-key")

# Manual trace management (legacy)
trace = agentops.start_trace("manual_trace")
# ... your code ...
agentops.end_trace(trace, "Success")
```

```python Context Manager
import agentops

agentops.init(api_key="your-api-key")

# Context manager (new, recommended)
with agentops.start_trace("context_managed_trace") as trace:
    # ... your code ...
    pass  # Automatically ended
```

```python Property Access
import agentops

agentops.init(api_key="your-api-key")

# Accessing trace properties
with agentops.start_trace("property_access") as trace:
    span = trace.span  # Access underlying span
    trace_id = trace.span.get_span_context().trace_id
```

```python Mixed Usage
import agentops

agentops.init(api_key="your-api-key")

# Mixed usage
trace = agentops.start_trace("mixed_usage")
try:
    with trace:  # Use existing trace as context manager
        # ... your code ...
        pass
except Exception:
    agentops.end_trace(trace, "Error")
```

</CodeGroup>

## Examples

For complete working examples, see the following files in the AgentOps repository:

<CardGroup cols={2}>
  <Card title="Basic Usage" icon="play" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/context_manager/basic_usage.py">
    Simple context manager patterns and error handling
  </Card>
  <Card title="Parallel Traces" icon="arrows-split-up-and-left" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/context_manager/parallel_traces.py">
    Sequential, nested, and concurrent trace patterns
  </Card>
  <Card title="Error Handling" icon="shield-exclamation" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/context_manager/error_handling.py">
    Exception handling, retry patterns, and graceful degradation
  </Card>
  <Card title="Production Patterns" icon="server" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/context_manager/production_patterns.py">
    API endpoints, batch processing, microservices, and monitoring
  </Card>
</CardGroup>

These examples demonstrate real-world usage patterns and best practices for using AgentOps context managers in production applications.

## API Reference

For detailed API information, see the [SDK Reference](/v2/usage/sdk-reference#trace-management) documentation.


## docs/v2/usage/tracking-llm-calls.mdx

---
title: "Tracking LLM Calls"
description: "Tracking LLM Calls using the AgentOps SDK"
---

## Automatic LLM Call Tracking

AgentOps makes tracking LLM calls incredibly simple. Just initialize the SDK with your API key, and AgentOps will automatically track all your LLM calls:

```python
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init("your-api-key")

# Make LLM calls as usual - they'll be tracked automatically
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello, world!"}]
)
```

### How it works

When the AgentOps SDK detects a supported LLM provider module installed, it will automatically
start tracking its usage. No further work is required from you! üòä

### Supported LLM Providers

AgentOps supports automatic tracking for many popular LLM providers, including:

- OpenAI
- Anthropic
- Google (Gemini)
- LiteLLM
- And more

### Not working? 

Try these steps:
1. Make sure you have the latest version of the AgentOps SDK installed. We are constantly updating it to support new LLM libraries and releases.
2. Make sure you are calling `agentops.init()` *after* importing the LLM module but *before* you are calling the LLM method.
3. Make sure the `instrument_llm_calls` parameter of `agentops.init()` is set to `True` (default).

Still not working? Please let us know! You can find us on [Discord](https://discord.gg/DR2abmETjZ),
 [GitHub](https://github.com/AgentOps-AI/agentops), 
 or email us at [engineering@agentops.ai](mailto:engineering@agentops.ai).

To get started, just follow the quick start guide.

<CardGroup cols={2}>
  <Card title="Quick Start" icon="code" href="/v2/quickstart">
    Get started with AgentOps in under 5 minutes
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/usage/sdk-reference.mdx

---
title: "SDK Reference"
description: "All functions and classes exposed in the top layer of the SDK"
---

# SDK Reference

This reference documents the functions and classes available with `import agentops` for the Python SDK. The AgentOps SDK is designed for easy integration with your agent applications, offering both simple auto-instrumentation and more detailed manual tracing capabilities.

<Note>
This documentation covers the Python SDK. A TypeScript/JavaScript SDK is also available - see our [TypeScript SDK guide](/v2/usage/typescript-sdk) for details.
</Note>

## Core Functions

These are the primary functions you'll use to initialize and configure AgentOps in your application.

### `init()`

Initializes the AgentOps SDK and automatically starts tracking your application.

**Parameters**:

- `api_key` (str, optional): API Key for AgentOps services. If not provided, the key will be read from the `AGENTOPS_API_KEY` environment variable.
- `endpoint` (str, optional): The endpoint for the AgentOps service. If not provided, will be read from the `AGENTOPS_API_ENDPOINT` environment variable. Defaults to 'https://api.agentops.ai'.
- `app_url` (str, optional): The dashboard URL for the AgentOps app. If not provided, will be read from the `AGENTOPS_APP_URL` environment variable. Defaults to 'https://app.agentops.ai'.
- `max_wait_time` (int, optional): The maximum time to wait in milliseconds before flushing the queue. Defaults to 5,000 (5 seconds).
- `max_queue_size` (int, optional): The maximum size of the event queue. Defaults to 512.
- `default_tags` (List[str], optional): Default tags for the sessions that can be used for grouping or sorting later (e.g. ["GPT-4"]).
- `tags` (List[str], optional): **[Deprecated]** Use `default_tags` instead. Will be removed in v4.0.
- `instrument_llm_calls` (bool, optional): Whether to instrument LLM calls automatically. Defaults to True.
- `auto_start_session` (bool, optional): Whether to start a session automatically when the client is created. Set to False if running in a Jupyter Notebook. Defaults to True.
- `auto_init` (bool, optional): Whether to automatically initialize the client on import. Defaults to True.
- `skip_auto_end_session` (bool, optional): Don't automatically end session based on your framework's decision-making. Defaults to False.
- `env_data_opt_out` (bool, optional): Whether to opt out of collecting environment data. Defaults to False.
- `log_level` (str, int, optional): The log level to use for the client. Defaults to 'INFO'.
- `fail_safe` (bool, optional): Whether to suppress errors and continue execution when possible. Defaults to False.
- `exporter_endpoint` (str, optional): Endpoint for the exporter. If not provided, will be read from the `AGENTOPS_EXPORTER_ENDPOINT` environment variable. Defaults to 'https://otlp.agentops.ai/v1/traces'.
- `export_flush_interval` (int, optional): Time interval in milliseconds between automatic exports of telemetry data. Defaults to 1000.
- `trace_name` (str, optional): Custom name for the automatically created trace. If not provided, a default name will be used.

**Returns**:

- If `auto_start_session=True`, returns the created Session object. Otherwise, returns None.

**Example**:

```python
import agentops

# Basic initialization with automatic session creation
agentops.init("your-api-key")

# Initialize with custom trace name
agentops.init("your-api-key", trace_name="my-workflow")
```

### `configure()`

Updates client configuration after initialization. Supports the same parameters as `init()`.

**Parameters**:

- `api_key` (str, optional): API Key for AgentOps services.
- `endpoint` (str, optional): The endpoint for the AgentOps service.
- `app_url` (str, optional): The dashboard URL for the AgentOps app.
- `max_wait_time` (int, optional): Maximum time to wait in milliseconds before flushing the queue.
- `max_queue_size` (int, optional): Maximum size of the event queue.
- `default_tags` (List[str], optional): Default tags for the sessions.
- `instrument_llm_calls` (bool, optional): Whether to instrument LLM calls.
- `auto_start_session` (bool, optional): Whether to start a session automatically.
- `auto_init` (bool, optional): Whether to automatically initialize the client on import.
- `skip_auto_end_session` (bool, optional): Don't automatically end session.
- `env_data_opt_out` (bool, optional): Whether to opt out of collecting environment data.
- `log_level` (str, int, optional): The log level to use for the client.
- `fail_safe` (bool, optional): Whether to suppress errors and continue execution.
- `exporter` (object, optional): Custom span exporter for OpenTelemetry trace data.
- `processor` (object, optional): Custom span processor for OpenTelemetry trace data.
- `exporter_endpoint` (str, optional): Endpoint for the exporter.
- `export_flush_interval` (int, optional): Time interval in milliseconds between automatic exports of telemetry data.
- `trace_name` (str, optional): Custom name for traces.

**Example**:

```python
import agentops

# Initialize first
agentops.init()

# Later, update configuration
agentops.configure(
    max_wait_time=10000,
    max_queue_size=200,
    default_tags=["production", "gpt-4"],
    trace_name="production-workflow"
)
```

### `get_client()`

Gets the singleton client instance. Most users won't need to use this function directly.

**Returns**:

- The AgentOps client instance.

## Trace Management

These functions help you manage the lifecycle of tracking traces.

### `start_trace()`

Starts a new AgentOps trace manually. This is useful when you've disabled automatic session creation or need multiple separate traces.

**Parameters**:

- `trace_name` (str, optional): Name for the trace. If not provided, a default name will be used.
- `tags` (Union[Dict[str, Any], List[str]], optional): Optional tags to attach to the trace, useful for filtering in the dashboard. Can be a list of strings or a dict of key-value pairs.

**Returns**:

- TraceContext object representing the started trace.

**Example**:

```python
import agentops

# Initialize without auto-starting a session
agentops.init("your-api-key", auto_start_session=False)

# Start a trace manually
trace = agentops.start_trace("customer-service-workflow", tags=["customer-query"])
```

### `end_trace()`

Ends a specific trace or all active traces.

**Parameters**:

- `trace` (TraceContext, optional): The specific trace to end. If not provided, all active traces will be ended.
- `end_state` (str, optional): The end state for the trace(s). You can use any descriptive string that makes sense for your application (e.g., "Success", "Indeterminate", "Error", "Timeout", etc.).

**Example**:

```python
import agentops

# End a specific trace
trace = agentops.start_trace("my-workflow")
# ... your code ...
agentops.end_trace(trace, "Success")

# End all active traces
agentops.end_trace(end_state="Emergency_Shutdown")
```

### `update_trace_metadata()`

Updates metadata on the currently running trace. This is useful for adding context, tracking progress, or storing intermediate results during trace execution.

**Parameters**:

- `metadata` (Dict[str, Any]): Dictionary of key-value pairs to set as trace metadata. Values must be strings, numbers, booleans, or lists of these types. Lists are automatically converted to JSON string representation.
- `prefix` (str, optional): Prefix for metadata attributes. Defaults to "trace.metadata". Ignored for semantic convention attributes.

**Returns**:

- `bool`: True if metadata was successfully updated, False otherwise.

**Features**:

- **Semantic Convention Support**: User-friendly keys like "tags", "agent_name", "workflow_name" are automatically mapped to OpenTelemetry semantic conventions.
- **Custom Attributes**: Non-semantic keys are prefixed with the specified prefix (default: "trace.metadata").
- **Type Safety**: Validates input types and converts lists to JSON strings for OpenTelemetry compatibility.
- **Error Handling**: Returns boolean success indicator and logs warnings for invalid data.

**Example**:

```python
import agentops
from agentops import update_trace_metadata

# Initialize and start trace with initial tags
agentops.init(auto_start_session=False)
trace = agentops.start_trace("ai-workflow", tags=["startup", "initialization"])

# Your code here...

# Update metadata mid-run with new tags and operation info
update_trace_metadata({
    "operation_name": "OpenAI GPT-4o-mini",
    "tags": ["ai-agent", "processing", "gpt-4"],  # Updates tags
    "status": "processing"
})

# End the trace
agentops.end_trace(trace, "Success")
```

For detailed examples and use cases, see [Manual Trace Control](/v2/usage/manual-trace-control#updating-trace-metadata-during-execution).

## Decorators for Detailed Instrumentation

For more granular control, AgentOps provides decorators that explicitly track different components of your application. **The `@trace` decorator is the recommended approach for creating custom traces**, especially in multi-threaded environments. These decorators are imported from `agentops.sdk.decorators`.

```python
import agentops
from agentops.sdk.decorators import trace, agent, operation, tool

# Initialize without automatic session creation
agentops.init("your-api-key", auto_start_session=False)

# Create and run a trace using the decorator
@trace
def my_workflow():
    # Your workflow code here
    pass

# Run the workflow, which creates and manages the trace
my_workflow()
```

### Available Decorators

- `@trace`: Creates a trace span for grouping related operations
- `@agent`: Creates an agent span for tracking agent operations
- `@operation` / `@task`: Creates operation/task spans for tracking specific operations (these are aliases)
- `@workflow`: Creates workflow spans for organizing related operations
- `@tool`: Creates tool spans for tracking tool usage and cost in agent operations. Supports cost parameter for tracking tool usage costs.

**Tool Decorator Example**:

```python
from agentops.sdk.decorators import tool

@tool(cost=0.05)
def web_search(query):
    # Tool implementation with cost tracking
    return f"Search results for: {query}"

@tool
def calculator(expression):
    # Tool without cost tracking
    return eval(expression)
```

See [Decorators](/v2/concepts/decorators) for more detailed documentation on using these decorators.

## Legacy Functions

<Warning>
The following functions are **deprecated** and will be removed in v4.0. They are maintained for backward compatibility with older versions of the SDK and integrations. New code should use the functions and decorators described above instead. When used, these functions will log deprecation warnings.
</Warning>

- `start_session()`: **Deprecated.** Legacy function for starting sessions. Use `@trace` decorator or `start_trace()` instead.
- `end_session()`: **Deprecated.** Legacy function for ending sessions. Use `end_trace()` instead.
- `record(event)`: **Deprecated.** Legacy function to record an event. Replaced by decorator-based tracing.
- `track_agent()`: **Deprecated.** Legacy decorator for marking agents. Replaced by the `@agent` decorator.
- `track_tool()`: **Deprecated.** Legacy decorator for marking tools. Replaced by the `@tool` decorator.
- `ToolEvent()`, `ErrorEvent()`, `ActionEvent()`, `LLMEvent()`: **Deprecated.** Legacy event types. Replaced by automatic instrumentation and decorators.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/usage/track-endpoint-decorator.mdx

---
title: "Track Endpoint Decorator"
description: "HTTP endpoint tracing for Flask applications using the @track_endpoint decorator"
---

## Overview

The `@track_endpoint` decorator provides HTTP endpoint tracing for Flask applications with automatic request/response monitoring. It's designed to work seamlessly with Flask and extends the functionality of the basic `@trace` decorator.

## Quick Example with OpenAI

Here's a simple Flask endpoint that generates text using OpenAI:

```python
from flask import Flask, request
from openai import OpenAI
import agentops

# Initialize AgentOps
agentops.init(
    api_key="your-api-key",
    auto_start_session=False,  # Required for endpoint tracing
)

app = Flask(__name__)
client = OpenAI()

@app.route("/api/generate", methods=["POST"])
@agentops.track_endpoint(
    name="generate_text",
    tags=["ai", "openai"]
)
def generate_text():
    """Generate text using OpenAI"""
    data = request.get_json()
    prompt = data.get("prompt", "Hello!")
    
    # OpenAI call is automatically traced
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )
    
    return {
        "text": response.choices[0].message.content,
        "usage": {
            "total_tokens": response.usage.total_tokens
        }
    }

if __name__ == "__main__":
    app.run(debug=True)
```

The decorator automatically captures:
- HTTP request data (method, URL, headers, body)
- HTTP response data (status code, headers, body)
- OpenAI API calls and their results
- Any errors that occur during request processing

You can customize tracing with parameters like:
- `name`: Custom name for the trace
- `tags`: List or dict of tags for categorizing traces
- `capture_request`: Whether to capture request data (default: True)
- `capture_response`: Whether to capture response data (default: True) 

## docs/v2/usage/recording-operations.mdx

---
title: "Recording Operations"
description: "Track operations and LLM calls in your agent applications."
---

AgentOps makes it easy to track operations and interactions in your AI applications with minimal setup.

## Basic Setup

The simplest way to get started with AgentOps is to initialize it at the beginning of your application:

```python
import agentops

# Initialize AgentOps with your API key
agentops.init("your-api-key")
```

That's it! This single line of code will:
- Automatically create a session for tracking your application run
- Intercept and track all LLM calls to supported providers (OpenAI, Anthropic, etc.)
- Record relevant metrics such as token counts, costs, and response times

You can also set a custom trace name during initialization:

```python
import agentops

# Initialize with custom trace name
agentops.init("your-api-key", trace_name="my-custom-workflow")
```

## Automatic Instrumentation

AgentOps automatically instruments calls to popular LLM providers without requiring any additional code:

```python
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init("your-api-key")

# Make LLM calls as usual - AgentOps will track them automatically
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello, world!"}]
)
```

This works with many popular LLM providers including:
- OpenAI
- Anthropic
- Google (Gemini)
- Cohere
- And more

## Advanced: Using Decorators for Detailed Instrumentation

For more detailed tracking, AgentOps provides decorators that allow you to explicitly instrument your code. This is optional but can provide more context in the dashboard.

### `@operation` Decorator

The `@operation` decorator helps track specific operations in your application:

```python
from agentops.sdk.decorators import operation

@operation
def process_data(data):
    # Process the data
    return result
```

### `@agent` Decorator

If you use agent classes, you can track them with the `@agent` decorator:

```python
from agentops.sdk.decorators import agent, operation

@agent
class ResearchAgent:
    @operation
    def search(self, query):
        # Implementation of search
        return f"Results for: {query}"

def research_workflow(topic):
    agent = ResearchAgent()
    results = agent.search(topic)
    return results
    
results = research_workflow("quantum computing")
```

### `@tool` Decorator

Track tool usage and costs with the `@tool` decorator. You can specify costs to get total cost tracking directly in your dashboard summary:

```python
from agentops.sdk.decorators import tool

@tool(cost=0.05)
def web_search(query):
    # Tool implementation
    return f"Search results for: {query}"

@tool
def calculator(expression):
    # Tool without cost tracking
    return eval(expression)
```

### `@trace` Decorator

Create custom traces to group related operations using the `@trace` decorator. This is the recommended approach for most applications:

```python
import agentops
from agentops.sdk.decorators import trace, agent, operation

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@trace(name="customer-service-workflow", tags=["customer-support"])
def customer_service_workflow(customer_id):
    agent = ResearchAgent()
    results = agent.search(f"customer {customer_id}")
    return results
```


## Best Practices

1. **Keep it Simple**: For most applications, just initializing AgentOps with `agentops.init()` is sufficient.

2. **Use @trace for Custom Workflows**: When you need to group operations, use the `@trace` decorator instead of manual trace management.

3. **Meaningful Names and Tags**: When using decorators, choose descriptive names and relevant tags to make them easier to identify in the dashboard.

4. **Cost Tracking**: Use the `@tool` decorator with cost parameters to track tool usage costs in your dashboard.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/usage/manual-trace-control.mdx

---
title: "Manual Trace Control"
description: "Advanced trace management with start_trace and end_trace methods"
---

## Basic Manual Trace Control

### Starting and Ending Traces

The most basic form of manual trace control involves starting a trace, executing your code, and then ending the trace with a specific state:

```python
import agentops

# Initialize without automatic session creation
agentops.init("your-api-key", auto_start_session=False)

# Start a trace manually
trace = agentops.start_trace("my-workflow")

try:
    # Your application logic here
    result = perform_some_operation()
    
    # End the trace successfully
    agentops.end_trace(trace, "Success")
except Exception as e:
    # End the trace with failure state
    agentops.end_trace(trace, "Indeterminate")
```

### Trace Names and Tags

You can provide meaningful names and tags when starting traces:

```python
# Start a trace with custom name and tags
trace = agentops.start_trace(
    trace_name="customer-service-workflow",
    tags=["customer-123", "priority-high", "support"]
)
```

### Batch Processing with Selective Trace Ending

For batch processing scenarios, you can selectively end traces based on processing results:

```python
import agentops

# Initialize AgentOps
agentops.init("your-api-key", auto_start_session=False)

# Sample batch items to process
batch_items = [
    {"id": 1, "data": "item_1_data", "valid": True},
    {"id": 2, "data": "item_2_data", "valid": False},
    {"id": 3, "data": "item_3_data", "valid": True},
]
@agentops.operation(name="process_item")
def process_item(item):
    """Simulate processing an item"""
    if not item.get("valid", False):
        raise ValueError(f"Invalid item: {item['id']}")
    return {"processed": True, "result": f"Processed {item['data']}"}

# Start traces for batch items
for i, item in enumerate(batch_items):
    trace = agentops.start_trace(f"batch_item_{i+1}")
    try:
        result = process_item(item)
        if result.get("processed"):
            agentops.end_trace(trace, "Success")
        else:
            agentops.end_trace(trace, "Indeterminate")
    except Exception as e:
        agentops.end_trace(trace, "Error")
```

## Updating Trace Metadata During Execution

You can update metadata on running traces at any point during execution using the `update_trace_metadata` function. This is useful for adding context, tracking progress, or storing intermediate results.

### Basic Metadata Updates

```python
import agentops

# Initialize AgentOps
agentops.init("your-api-key", auto_start_session=False)

# Start a trace with initial tags
trace = agentops.start_trace("ai-agent-workflow", tags=["startup", "initialization"])

# Your AI agent code runs here...
process_user_request()

# Update metadata with results
agentops.update_trace_metadata({
    "operation_name": "AI Agent Processing Complete", 
    "stage": "completed",
    "response_quality": "high",
    "tags": ["ai-agent", "completed", "success"]  # Tags show current status
})

# End the trace
agentops.end_trace(trace, "Success")
```

### Semantic Convention Support

The function automatically maps user-friendly keys to semantic conventions when possible:

```python
# These keys will be mapped to semantic conventions
agentops.update_trace_metadata({
    "operation_name": "AI Agent Data Processing",
    "tags": ["production", "batch-job", "gpt-4"],  # Maps to core.tags
    "agent_name": "DataProcessorAgent",             # Maps to agent.name
    "workflow_name": "Intelligent ETL Pipeline",   # Maps to workflow.name
})

```

### Advanced Metadata with Custom Prefix

You can specify a custom prefix for your metadata attributes:

```python
# Use a custom prefix for business-specific metadata
agentops.update_trace_metadata({
    "customer_id": "CUST_456",
    "order_value": 99.99,
    "payment_method": "credit_card",
    "agent_interaction": "customer_support"
}, prefix="business")

# Results in:
# business.customer_id = "CUST_456"
# business.order_value = 99.99
# business.payment_method = "credit_card"
```

### Real-World Example: Progress Tracking

Here's how to use metadata updates to track progress through a complex workflow:

```python
import agentops
from agentops.sdk.decorators import operation

agentops.init(auto_start_session=False)

@operation
def process_batch(batch_data):
    # Simulate batch processing
    return f"Processed {len(batch_data)} items"

def run_etl_pipeline(data_batches):
    """ETL pipeline with progress tracking via metadata"""
    
    trace = agentops.start_trace("etl-pipeline", tags=["data-processing"])
    
    total_batches = len(data_batches)
    processed_records = 0
    
    # Initial metadata
    agentops.update_trace_metadata({
        "operation_name": "ETL Pipeline Execution",
        "pipeline_stage": "starting",
        "total_batches": total_batches,
        "processed_batches": 0,
        "processed_records": 0,
        "estimated_completion": "calculating...",
        "tags": ["etl", "data-processing", "async-operation"]
    })
    
    try:
        for i, batch in enumerate(data_batches):
            # Update progress
            agentops.update_trace_metadata({
                "pipeline_stage": "processing",
                "current_batch": i + 1,
                "processed_batches": i,
                "progress_percentage": round((i / total_batches) * 100, 2)
            })
            
            # Process the batch
            result = process_batch(batch)
            processed_records += len(batch)
            
            # Update running totals
            agentops.update_trace_metadata({
                "processed_records": processed_records,
                "last_batch_result": result
            })
        
        # Final metadata update
        agentops.update_trace_metadata({
            "operation_name": "ETL Pipeline Completed",
            "pipeline_stage": "completed",
            "processed_batches": total_batches,
            "progress_percentage": 100.0,
            "completion_status": "success",
            "total_execution_time": "calculated_automatically",
            "tags": ["etl", "completed", "success"]
        })
        
        agentops.end_trace(trace, "Success")
        
    except Exception as e:
        # Error metadata
        agentops.update_trace_metadata({
            "operation_name": "ETL Pipeline Failed",
            "pipeline_stage": "failed",
            "error_message": str(e),
            "completion_status": "error",
            "failed_at_batch": i + 1 if 'i' in locals() else 0,
            "tags": ["etl", "failed", "error"]
        })
        
        agentops.end_trace(trace, "Error")
        raise

# Example usage
data_batches = [
    ["record1", "record2", "record3"],
    ["record4", "record5"],
    ["record6", "record7", "record8", "record9"]
]

run_etl_pipeline(data_batches)
```

### Supported Data Types

The `update_trace_metadata` function supports various data types:

```python
agentops.update_trace_metadata({
    "operation_name": "Multi-type Data Example",
    "successful_operation": True,
    "tags": ["example", "demo", "multi-agent"],
    "processing_steps": ["validation", "transformation", "output"]
})

# Note: Lists are automatically converted to JSON strings for OpenTelemetry compatibility
```

## Integration with Decorators

Manual trace control works seamlessly with AgentOps decorators:

```python
import agentops
from agentops.sdk.decorators import agent, operation, tool

agentops.init("your-api-key", auto_start_session=False)

@agent
class CustomerServiceAgent:
    @operation
    def analyze_request(self, request):
        return f"Analyzed: {request}"
    
    @tool(cost=0.02)
    def lookup_customer(self, customer_id):
        return f"Customer data for {customer_id}"

# Manual trace with decorated components
trace = agentops.start_trace("customer-service")

try:
    agent = CustomerServiceAgent()
    customer_data = agent.lookup_customer("CUST_123")
    analysis = agent.analyze_request("billing issue")
    
    agentops.end_trace(trace, "Success")
except Exception as e:
    agentops.end_trace(trace, "Error")
```

## Real-World Example

Here's a comprehensive example showing manual trace control in a customer service application:

```python
import agentops
from agentops.sdk.decorators import agent, operation, tool
from openai import OpenAI

agentops.init(auto_start_session=False)
client = OpenAI()

@operation
def analyze_sentiment(text):
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": f"Analyze sentiment: {text}"}]
        )
        return response.choices[0].message.content.strip()
    
@tool(cost=0.01)
def lookup_order(order_id):
    return f"Order {order_id} details"

def process_customer_requests(requests):
    """Process multiple customer requests with individual trace tracking"""
    results = []
    for i, request in enumerate(requests):
        trace = agentops.start_trace(
            f"customer_request_{i+1}",
            tags=["customer-service", request.get("priority", "normal")]
        )
        try:
            sentiment = analyze_sentiment(request["message"])
            
            if "order" in request:
                order_info = lookup_order(request["order"])
            
            if "positive" in sentiment.lower() or "neutral" in sentiment.lower():
                agentops.end_trace(trace, "Success")
                results.append({"status": "resolved", "sentiment": sentiment})
            else:
                agentops.end_trace(trace, "Escalation_Required")
                results.append({"status": "escalated", "sentiment": sentiment})
                
        except Exception as e:
            agentops.end_trace(trace, "Error")
            results.append({"status": "error", "error": str(e)})
    
    return results

customer_requests = [
    {"message": "I love this product!", "priority": "low"},
    {"message": "My order is completely wrong!", "order": "12345", "priority": "high"},
    {"message": "When will my package arrive?", "order": "67890", "priority": "normal"}
]

results = process_customer_requests(customer_requests)
print(f"Processed {len(results)} customer requests")
```

This example demonstrates:
- Individual trace management for each customer request
- Integration with decorated agents and tools
- Different end states based on business logic
- Proper error handling with appropriate trace states
- Use of tags for categorization

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script> 

## docs/v2/usage/advanced-configuration.mdx

---
title: "Advanced Configuration"
---

In AgentOps fashion, you only need to add one line of "code" to your `.env` file üòä

<CodeGroup>
  ```python .env
  AGENTOPS_API_KEY=<YOUR API KEY>
  ```
</CodeGroup>

<Tip>
Find your AgentOps API Key in your Settings > [Projects & API Keys](https://app.agentops.ai/settings/projects) page.
</Tip>

#### Optional settings:

```python .env
# The AgentOps API endpoint. Defaults to https://api.agentops.ai
AGENTOPS_API_ENDPOINT=https://api.agentops.ai
# Logging level. <DEBUG, INFO, CRITICAL>. Defaults to INFO
AGENTOPS_LOG_LEVEL=INFO
# Write logs to file <TRUE, FALSE>. Defaults to TRUE
AGENTOPS_LOGGING_TO_FILE=TRUE
# Whether to opt out of recording environment data. <FALSE, TRUE>. Defaults to FALSE
AGENTOPS_ENV_DATA_OPT_OUT=FALSE
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/usage/typescript-sdk.mdx

---
title: "TypeScript SDK"
description: "Get started with the AgentOps TypeScript SDK for Node.js applications"
---

# TypeScript SDK

AgentOps provides TypeScript/JavaScript support through two SDK options:

## Modern TypeScript SDK (Recommended)

The modern TypeScript SDK is built on OpenTelemetry standards and provides comprehensive instrumentation for AI agents.

### Installation

```bash
npm install agentops
```

### Quick Start

```typescript
import { agentops } from 'agentops';

// Initialize with environment variable AGENTOPS_API_KEY
await agentops.init();

// Or pass API key explicitly
await agentops.init({
  apiKey: 'your-api-key'
});

// Your AI agent code here - instrumentation happens automatically!
```

### Features

- üîå **Plugin Architecture**: Dynamic loading and configuration of instrumentors
- ü§ñ **GenAI Support**: Built-in support for OpenTelemetry GenAI semantic conventions
- üìä **Standards Compliant**: Exports to any OpenTelemetry-compatible collector
- üõ†Ô∏è **Framework Agnostic**: Instrument multiple agent frameworks simultaneously
- üîß **TypeScript First**: Full TypeScript support with comprehensive type definitions

### OpenAI Agents Integration

The SDK provides first-class support for the [OpenAI Agents SDK](https://github.com/openai/openai-agents-js/):

```typescript
import { agentops } from 'agentops';
import { Agent, run } from '@openai/agents';

// Initialize AgentOps first
await agentops.init();

// Create your agent with tools and instructions
const agent = new Agent({
  name: 'My Assistant',
  instructions: 'You are a helpful assistant.',
  tools: [/* your tools */],
});

// Run the agent - instrumentation happens automatically
const result = await run(agent, "Hello, how can you help me?");
console.log(result.finalOutput);
```

Automatically captures:
- **Agent Lifecycle**: Track agent creation, execution, and completion
- **LLM Generation**: Capture model requests, responses, and token usage
- **Function Calls**: Monitor tool usage and function execution
- **Audio Processing**: Observe speech-to-text and text-to-speech operations
- **Handoffs**: Track agent-to-agent communication and workflow transitions

### Debug Logging

Enable detailed instrumentation logs:

```bash
DEBUG=agentops:* node your-app.js
```

## Legacy TypeScript SDK (Alpha)

<Warning>
The legacy TypeScript SDK has limited functionality compared to the Python SDK. The modern TypeScript SDK above is recommended for new projects.
</Warning>

### Installation

```bash
npm install agentops
```

### Usage

```typescript
import OpenAI from "openai";
import { Client } from 'agentops';

const openai = new OpenAI();

const agentops = new Client({
    apiKey: "your-agentops-api-key",
    tags: ["typescript", "example"],
    patchApi: [openai]  // Automatically record OpenAI calls
});

// Sample OpenAI call (automatically recorded)
async function chat() {
    const completion = await openai.chat.completions.create({
        messages: [
            { "role": "system", "content": "You are a helpful assistant." },
            { "role": "user", "content": "Hello!" }
        ],
        model: "gpt-3.5-turbo",
    });
    return completion;
}

// Track custom functions
function customFunction(x: string) {
    console.log(x);
    return 5;
}

const wrappedFunction = agentops.wrap(customFunction);
wrappedFunction("hello");

// Run your agent
chat().then(() => {
    agentops.endSession("Success");
});
```

## Repository Links

- **Modern SDK**: [agentops-ts](https://github.com/AgentOps-AI/agentops-ts)
- **Legacy SDK**: [agentops-node](https://github.com/AgentOps-AI/agentops-node)

## Getting Help

- [Discord Community](https://discord.gg/FagdcwwXRR)
- [GitHub Issues](https://github.com/AgentOps-AI/agentops-ts/issues)
- [Documentation](https://docs.agentops.ai)


## docs/v2/usage/trace-decorator.mdx

---
title: "Trace Decorator"
description: "Create custom traces with the @trace decorator"
---

## Basic Usage

### Simple Trace Creation

The `@trace` decorator automatically creates a trace span that encompasses the entire function execution. You can optionally specify custom names and tags to better organize and categorize your traces:

```python
from agentops.sdk.decorators import trace
import agentops

# Initialize AgentOps
agentops.init("your-api-key", auto_start_session=False)

@trace(name="customer-workflow", tags=["production", "customer-service"])
def my_workflow():
    """A simple workflow wrapped in a trace"""
    print("üöÄ Starting customer workflow...")
    print("üìã Processing customer request...")
    # Your application logic here
    print("‚úÖ Customer workflow completed successfully!")
    return "Workflow completed"

# Run the function - this creates and manages the trace automatically
print("üé¨ Running traced workflow...")
result = my_workflow()
print(f"üìä Result: {result}")
```

Both `name` and `tags` parameters are optional. If no name is provided, the function name will be used as the trace name.

### Custom Trace Names

You can specify custom names for your traces:

```python
@trace(name="customer-onboarding-flow")
def onboard_customer(customer_data):
    """Customer onboarding process"""
    print(f"üëã Onboarding customer: {customer_data['name']}")
    print("üìù Creating customer profile...")
    print("üìß Sending welcome email...")
    print("‚úÖ Customer onboarding complete!")
    return f"Onboarded customer: {customer_data['name']}"

@trace(name="data-processing-pipeline")
def process_data(input_data):
    """Data processing workflow"""
    print(f"üìä Processing {len(input_data)} data items...")
    print("üîÑ Applying transformations...")
    print("‚úÖ Data processing complete!")
    return f"Processed {len(input_data)} items"

# Usage examples
customer = {"name": "Alice Johnson", "email": "alice@example.com"}
result1 = onboard_customer(customer)
print(f"üìã Onboarding result: {result1}")

data_items = ["item1", "item2", "item3", "item4", "item5"]
result2 = process_data(data_items)
print(f"üìã Processing result: {result2}")
```

### Adding Tags to Traces

Tags help categorize and filter traces in your dashboard:

```python
@trace(tags=["production", "high-priority"])
def critical_workflow():
    """Critical production workflow"""
    print("üö® Executing critical production workflow...")
    print("‚ö° High priority processing...")
    print("‚úÖ Critical task completed successfully!")
    return "Critical task completed"

@trace(name="user-analysis", tags=["analytics", "user-behavior"])
def analyze_user_behavior(user_id):
    """Analyze user behavior patterns"""
    print(f"üîç Analyzing behavior for user: {user_id}")
    print("üìà Gathering user interaction data...")
    print("üß† Running behavior analysis algorithms...")
    print("‚úÖ User behavior analysis complete!")
    return f"Analysis complete for user {user_id}"

# Usage examples
print("üé¨ Running critical workflow...")
result1 = critical_workflow()
print(f"üìä Critical workflow result: {result1}")

print("\nüé¨ Running user analysis...")
result2 = analyze_user_behavior("user_12345")
print(f"üìä Analysis result: {result2}")
```

## Integration with Other Decorators

### Combining with Agent and Operation Decorators

The `@trace` decorator works seamlessly with other AgentOps decorators:

```python
import agentops
from agentops.sdk.decorators import trace, agent, operation, tool

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@agent
class DataAnalysisAgent:
    def __init__(self):
        print("ü§ñ DataAnalysisAgent initialized")
    
    @operation
    def collect_data(self, source):
        print(f"üìä Collecting data from {source}...")
        data = f"Data collected from {source}"
        print(f"‚úÖ Data collection complete: {data}")
        return data
    
    @tool(cost=0.05)
    def analyze_data(self, data):
        print(f"üß† Analyzing data: {data}")
        analysis = f"Analysis of {data}"
        print(f"‚úÖ Analysis complete: {analysis}")
        return analysis
    
    @operation
    def generate_report(self, analysis):
        print(f"üìù Generating report from: {analysis}")
        report = f"Report: {analysis}"
        print(f"‚úÖ Report generated: {report}")
        return report

@trace(name="complete-analysis-workflow")
def run_analysis_workflow(data_source):
    """Complete data analysis workflow"""
    print(f"üöÄ Starting analysis workflow for: {data_source}")
    print("=" * 50)
    
    agent = DataAnalysisAgent()
    
    # Collect data
    print("\nüìã Step 1: Data Collection")
    data = agent.collect_data(data_source)
    
    # Analyze data
    print("\nüìã Step 2: Data Analysis")
    analysis = agent.analyze_data(data)
    
    # Generate report
    print("\nüìã Step 3: Report Generation")
    report = agent.generate_report(analysis)
    
    print("\nüéâ Workflow completed successfully!")
    print("=" * 50)
    
    return {
        "source": data_source,
        "report": report
    }

# Usage
print("üé¨ Running complete analysis workflow...")
result = run_analysis_workflow("customer_database")
print(f"\nüìä Final Result:")
print(f"   Source: {result['source']}")
print(f"   Report: {result['report']}")
```

## Async Function Support

The `@trace` decorator fully supports async functions:

```python
import asyncio
import agentops
from agentops.sdk.decorators import trace, operation

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@operation
async def fetch_user_data(user_id):
    """Simulate async data fetching"""
    print(f"üåê Fetching data for user: {user_id}")
    await asyncio.sleep(1)  # Simulate API call
    data = f"User data for {user_id}"
    print(f"‚úÖ Data fetched: {data}")
    return data

@operation
async def process_user_data(user_data):
    """Simulate async data processing"""
    print(f"‚öôÔ∏è Processing user data: {user_data}")
    await asyncio.sleep(0.5)  # Simulate processing
    processed = f"Processed: {user_data}"
    print(f"‚úÖ Processing complete: {processed}")
    return processed

@trace(name="async-user-workflow")
async def async_user_workflow(user_id):
    """Async workflow for user processing"""
    print(f"üöÄ Starting async workflow for user: {user_id}")
    print("=" * 45)
    
    print("\nüìã Step 1: Fetching user data")
    user_data = await fetch_user_data(user_id)
    
    print("\nüìã Step 2: Processing user data")
    processed_data = await process_user_data(user_data)
    
    print("\nüéâ Async workflow completed!")
    print("=" * 45)
    
    return processed_data

# Usage
async def main():
    print("üé¨ Running async user workflow...")
    result = await async_user_workflow("user_123")
    print(f"\nüìä Final Result: {result}")
    print("‚ú® Check your AgentOps dashboard to see the traced async workflow!")

# Run the async workflow
print("üîÑ Starting async demo...")
asyncio.run(main())
```

## Error Handling and Trace States

### Automatic Error Handling

The `@trace` decorator automatically handles exceptions and sets appropriate trace states:

```python
import agentops
from agentops.sdk.decorators import trace

# Initialize AgentOps without auto-starting session since we use @trace
agentops.init("your-api-key", auto_start_session=False)

@trace(name="error-prone-workflow")
def risky_operation():
    """Operation that might fail"""
    import random
    
    print("üé≤ Running risky operation...")
    print("‚ö†Ô∏è This operation has a 50% chance of failure")
    
    if random.random() < 0.5:
        print("‚ùå Operation failed!")
        raise ValueError("Random failure occurred")
    
    print("‚úÖ Operation succeeded!")
    return "Operation succeeded"

# The trace will automatically be marked with failure state if an exception occurs
print("üé¨ Testing automatic error handling...")
for i in range(3):
    print(f"\nüîÑ Attempt {i+1}:")
    try:
        result = risky_operation()
        print(f"üìä Success: {result}")
        break
    except ValueError as e:
        print(f"üìä Operation failed: {e}")
        print("üîç Trace automatically ended with error state")
```

### Custom Error Handling

You can implement custom error handling within traced functions:

```python
@trace(name="robust-workflow")
def robust_operation(data):
    """Operation with custom error handling"""
    print(f"üöÄ Starting robust operation with data: {data}")
    
    try:
        # Risky operation
        if not data:
            print("‚ö†Ô∏è No data provided!")
            raise ValueError("No data provided")
        
        # Process data
        print("‚öôÔ∏è Processing data...")
        result = f"Processed: {data}"
        print(f"‚úÖ Processing successful: {result}")
        return {"success": True, "result": result}
        
    except ValueError as e:
        # Handle specific errors
        print(f"‚ùå Validation error: {e}")
        return {"success": False, "error": str(e)}
    except Exception as e:
        # Handle unexpected errors
        print(f"üí• Unexpected error: {e}")
        return {"success": False, "error": f"Unexpected error: {str(e)}"}

# Usage examples
print("\nüé¨ Testing custom error handling...")

print("\nüìã Test 1: Valid data")
result1 = robust_operation("valid_data")
print(f"üìä Result: {result1}")

print("\nüìã Test 2: Empty data")
result2 = robust_operation("")
print(f"üìä Result: {result2}")

print("\nüìã Test 3: None data")
result3 = robust_operation(None)
print(f"üìä Result: {result3}")
```

## Real-World Examples

### E-commerce Order Processing

```python
from agentops.sdk.decorators import trace, agent, operation, tool
from openai import OpenAI
import agentops

agentops.init("your-api-key", auto_start_session=False)

@agent
class OrderProcessor:
    def __init__(self):
        print("üõí OrderProcessor initialized")
    
    @tool(cost=0.01)
    def validate_payment(self, payment_info):
        """Payment validation service"""
        print(f"üí≥ Validating payment: {payment_info['card']}")
        result = {"valid": True, "transaction_id": "txn_123"}
        print(f"‚úÖ Payment validation successful: {result['transaction_id']}")
        return result
    
    @tool(cost=0.02)
    def check_inventory(self, product_id, quantity):
        """Inventory check service"""
        print(f"üì¶ Checking inventory for {product_id} (qty: {quantity})")
        result = {"available": True, "reserved": quantity}
        print(f"‚úÖ Inventory check complete: {quantity} units available")
        return result
    
    @operation
    def calculate_shipping(self, address, items):
        """Calculate shipping costs"""
        print(f"üöö Calculating shipping to {address['city']}, {address['state']}")
        result = {"cost": 9.99, "method": "standard"}
        print(f"‚úÖ Shipping calculated: ${result['cost']} ({result['method']})")
        return result
    
    @tool(cost=0.005)
    def send_confirmation_email(self, email, order_details):
        """Email service"""
        print(f"üìß Sending confirmation email to {email}")
        result = f"Confirmation sent to {email}"
        print(f"‚úÖ Email sent successfully")
        return result

@trace(name="order-processing", tags=["ecommerce", "orders"])
def process_order(order_data):
    """Complete order processing workflow"""
    print(f"üöÄ Starting order processing for {order_data['customer_email']}")
    print("=" * 60)
    
    processor = OrderProcessor()
    
    try:
        # Validate payment
        print("\nüìã Step 1: Payment Validation")
        payment_result = processor.validate_payment(order_data["payment"])
        if not payment_result["valid"]:
            print("‚ùå Payment validation failed!")
            return {"success": False, "error": "Payment validation failed"}
        
        # Check inventory for all items
        print("\nüìã Step 2: Inventory Check")
        for item in order_data["items"]:
            inventory_result = processor.check_inventory(
                item["product_id"], 
                item["quantity"]
            )
            if not inventory_result["available"]:
                print(f"‚ùå Item {item['product_id']} not available!")
                return {"success": False, "error": f"Item {item['product_id']} not available"}
        
        # Calculate shipping
        print("\nüìã Step 3: Shipping Calculation")
        shipping = processor.calculate_shipping(
            order_data["shipping_address"], 
            order_data["items"]
        )
        
        # Send confirmation
        print("\nüìã Step 4: Confirmation Email")
        confirmation = processor.send_confirmation_email(
            order_data["customer_email"],
            {
                "items": order_data["items"],
                "shipping": shipping,
                "payment": payment_result
            }
        )
        
        print("\nüéâ Order processing completed successfully!")
        print("=" * 60)
        
        return {
            "success": True,
            "order_id": "ORD_12345",
            "payment": payment_result,
            "shipping": shipping,
            "confirmation": confirmation
        }
        
    except Exception as e:
        print(f"üí• Order processing failed: {e}")
        return {"success": False, "error": str(e)}

# Usage
print("üé¨ Running e-commerce order processing demo...")

order = {
    "customer_email": "customer@example.com",
    "payment": {"card": "****1234", "amount": 99.99},
    "items": [{"product_id": "PROD_001", "quantity": 2}],
    "shipping_address": {"city": "New York", "state": "NY"}
}

result = process_order(order)

print(f"\nüìä ORDER PROCESSING RESULT:")
print(f"   Success: {result['success']}")
if result['success']:
    print(f"   Order ID: {result['order_id']}")
    print(f"   Transaction: {result['payment']['transaction_id']}")
    print(f"   Shipping: ${result['shipping']['cost']}")
else:
    print(f"   Error: {result['error']}")
```

### Data Analysis Workflow

```python
from agentops.sdk.decorators import trace, agent, operation, tool
from openai import OpenAI
import agentops

agentops.init("your-api-key", auto_start_session=False)

@agent
class DataAnalysisAgent:
    def __init__(self):
        self.client = OpenAI()
        print("ü§ñ DataAnalysisAgent initialized")
    
    @operation
    def collect_data(self, source):
        """Simulate data collection"""
        print(f"üìä Collecting data from {source}...")
        data = f"Raw data collected from {source}: [sample_data_1, sample_data_2, sample_data_3]"
        print(f"‚úÖ Data collection complete: {len(data)} characters collected")
        return data
    
    @operation
    def analyze_data_with_llm(self, data):
        """Use LLM to analyze the collected data"""
        print("üß† Analyzing data with LLM...")
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a data analyst. Analyze the provided data and give insights."},
                {"role": "user", "content": f"Please analyze this data: {data}"}
            ]
        )
        analysis = response.choices[0].message.content
        print(f"‚úÖ LLM analysis complete: {len(analysis)} characters generated")
        return analysis
    
    @tool(cost=0.05)
    def generate_visualization(self, analysis):
        """Generate data visualization"""
        print("üìà Generating visualization...")
        visualization = f"Chart generated for: {analysis[:50]}..."
        print(f"‚úÖ Visualization generated: {visualization}")
        return visualization
    
    @operation
    def generate_report(self, analysis, visualization):
        """Generate final report using LLM"""
        print("üìù Generating final report with LLM...")
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a report writer. Create a professional data analysis report."},
                {"role": "user", "content": f"Create a report based on this analysis: {analysis} and visualization: {visualization}"}
            ]
        )
        report = response.choices[0].message.content
        print(f"‚úÖ Final report generated: {len(report)} characters")
        return report

@trace(name="data-analysis-workflow", tags=["analytics", "reporting"])
def run_data_analysis(data_source):
    """Complete data analysis workflow with LLM integration"""
    print(f"üöÄ Starting data analysis workflow for: {data_source}")
    print("=" * 60)
    
    agent = DataAnalysisAgent()
    
    # Collect data
    print("\nüìã Step 1: Data Collection")
    raw_data = agent.collect_data(data_source)
    
    # Analyze data using LLM
    print("\nüìã Step 2: LLM Analysis")
    analysis = agent.analyze_data_with_llm(raw_data)
    
    # Generate visualization
    print("\nüìã Step 3: Visualization Generation")
    visualization = agent.generate_visualization(analysis)
    
    # Generate final report using LLM
    print("\nüìã Step 4: Report Generation")
    report = agent.generate_report(analysis, visualization)
    
    print("\nüéâ Workflow completed successfully!")
    print("=" * 60)
    
    return {
        "source": data_source,
        "raw_data": raw_data,
        "analysis": analysis,
        "visualization": visualization,
        "final_report": report
    }

# Usage
print("üé¨ Running data analysis workflow demo...")

result = run_data_analysis("customer_database")

print(f"\nüìä ANALYSIS RESULTS:")
print(f"   Data Source: {result['source']}")
print(f"   Raw Data: {result['raw_data'][:80]}...")
print(f"   Analysis Preview: {result['analysis'][:100]}...")
print(f"   Visualization: {result['visualization']}")
print(f"   Final Report Preview: {result['final_report'][:150]}...")

print(f"\n‚ú® Analysis complete! Check your AgentOps dashboard to see the traced workflow.")
```

## Best Practices

### 1. Use Meaningful Names

Choose descriptive names that clearly indicate what the trace represents:

```python
# Good
@trace(name="user-authentication-flow")
def authenticate_user(credentials):
    pass

@trace(name="payment-processing-pipeline")
def process_payment(payment_data):
    pass

# Less descriptive
@trace(name="trace1")
def some_function():
    pass
```

### 2. Add Relevant Tags

Use tags to categorize traces for easier filtering and analysis:

```python
@trace(name="order-fulfillment", tags=["ecommerce", "fulfillment", "high-priority"])
def fulfill_order(order_id):
    pass

@trace(name="data-sync", tags=["background-job", "data-processing"])
def sync_data():
    pass
```

### 3. Keep Traces Focused

Each trace should represent a logical unit of work:

```python
# Good - focused on a single workflow
@trace(name="customer-onboarding")
def onboard_customer(customer_data):
    validate_customer(customer_data)
    create_account(customer_data)
    send_welcome_email(customer_data)

# Less focused - mixing different concerns
@trace(name="mixed-operations")
def do_everything():
    onboard_customer(data1)
    process_orders(data2)
    generate_reports(data3)
```

### 4. Handle Errors Appropriately

Implement proper error handling within traced functions:

```python
@trace(name="data-processing")
def process_data(data):
    try:
        # Main processing logic
        result = complex_processing(data)
        return {"success": True, "result": result}
    except ValidationError as e:
        # Expected errors
        return {"success": False, "error": "validation_failed", "details": str(e)}
    except Exception as e:
        # Unexpected errors
        logger.error(f"Unexpected error in data processing: {e}")
        return {"success": False, "error": "processing_failed"}
```

The `@trace` decorator provides a powerful and flexible way to organize your application's telemetry data. By creating logical groupings of operations, you can better understand your application's behavior and performance characteristics in the AgentOps dashboard.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script> 

## docs/v2/usage/langchain-callback-handler.mdx

---
title: 'LangChain Callback Handler'
description: 'How to use AgentOps with LangChain'
---

By default, AgentOps is compatible with agents using LangChain with our LLM Instrumentor as long as they're using
supported models.

As an alternative to instrumenting, the LangChain Callback Handler is available.

## Constructor

- `api_key` (Optional, string): API Key for AgentOps services. If not provided, the key will be read from the `AGENTOPS_API_KEY` environment variable.
- `endpoint` (Optional, string): The endpoint for the AgentOps service. Defaults to 'https://api.agentops.ai'.
- `max_wait_time` (Optional, int): The maximum time to wait in milliseconds before flushing the queue. Defaults to 30,000 (30 seconds).
- `max_queue_size` (Optional, int): The maximum size of the event queue. Defaults to 100.
- `tags` (Optional, List[string]): Tags for the sessions for grouping or sorting (e.g., ["GPT-4"]).

## Usage

### Install Dependencies
<CodeGroup>
```bash pip 
pip install agentops
```
```bash poetry
poetry add agentops
```
</CodeGroup>

### Implement Callback Handler

Initialize the handler with its constructor and pass it into the callbacks array from LangChain.
```python
from agentops.integration.callbacks.langchain import LangchainCallbackHandler
ChatOpenAI(callbacks=[LangchainCallbackHandler()])
```

Example:
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from agentops import LangchainCallbackHandler

prompt = ChatPromptTemplate.from_messages(["Tell me a joke about {animal}"])

model = ChatOpenAI(callbacks=[LangchainCallbackHandler()])

chain = prompt | model
response = chain.invoke({"animal": "bears"})
```

## Why use the handler?

If your project uses LangChain for Agents, Events and Tools, it may be easier to use the callback Handler for observability.

If your project uses models with LangChain that are not yet supported by AgentOps, they can be supported by the Handler.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/concepts/core-concepts.mdx

---
title: 'Core Concepts'
description: 'Understanding the fundamental concepts of AgentOps'
---

# The AgentOps SDK Architecture

AgentOps is designed to provide comprehensive monitoring and analytics for AI agent workflows with minimal implementation effort. The SDK follows these key design principles:

## Automated Instrumentation

After calling `agentops.init()`, the SDK automatically identifies installed LLM providers and instruments their API calls. This allows AgentOps to capture interactions between your code and the LLM providers to collect data for your dashboard without requiring manual instrumentation for every call.

## Declarative Tracing with Decorators

The [decorators](/v2/concepts/decorators) system allows you to add tracing to your existing functions and classes with minimal code changes. Decorators create hierarchical spans that provide a structured view of your agent's operations for monitoring and analysis.

## OpenTelemetry Foundation

AgentOps is built on [OpenTelemetry](https://opentelemetry.io/), a widely-adopted standard for observability instrumentation. This provides a robust and standardized approach to collecting, processing, and exporting telemetry data.

# Sessions

A [Session](/v2/concepts/sessions) represents a single user interaction with your agent. When you initialize AgentOps using the `init` function, a session is automatically created for you:

```python
import agentops

# Initialize AgentOps with automatic session creation
agentops.init(api_key="YOUR_API_KEY")
```

By default, all events and API calls will be associated with this session. For more advanced use cases, you can control session creation manually:

```python
# Initialize without auto-starting a session
agentops.init(api_key="YOUR_API_KEY", auto_start_session=False)

# Later, manually start a session when needed
agentops.start_session(tags=["customer-query"])
```

# Span Hierarchy

In AgentOps, activities are organized into a hierarchical structure of spans:

- **SESSION**: The root container for all activities in a single execution of your workflow
- **AGENT**: Represents an autonomous entity with specialized capabilities
- **WORKFLOW**: A logical grouping of related operations
- **OPERATION/TASK**: A specific task or function performed by an agent
- **LLM**: An interaction with a language model
- **TOOL**: The use of a tool or API by an agent

This hierarchy creates a complete trace of your agent's execution:

```
SESSION
  ‚îú‚îÄ‚îÄ AGENT
  ‚îÇ     ‚îú‚îÄ‚îÄ OPERATION/TASK
  ‚îÇ     ‚îÇ     ‚îú‚îÄ‚îÄ LLM
  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ TOOL
  ‚îÇ     ‚îî‚îÄ‚îÄ WORKFLOW
  ‚îÇ           ‚îî‚îÄ‚îÄ OPERATION/TASK
  ‚îî‚îÄ‚îÄ LLM (unattributed to a specific agent)
```

# Agents

An **Agent** represents a component in your application that performs tasks. You can create and track agents using the `@agent` decorator:

```python
from agentops.sdk.decorators import agent, operation

@agent(name="customer_service")
class CustomerServiceAgent:
    @operation
    def answer_query(self, query):
        # Agent logic here
        pass
```

# LLM Events

AgentOps automatically tracks LLM API calls from supported providers, collecting valuable information like:

- **Model**: The specific model used (e.g., "gpt-4", "claude-3-opus")
- **Provider**: The LLM provider (e.g., "OpenAI", "Anthropic")
- **Prompt Tokens**: Number of tokens in the input
- **Completion Tokens**: Number of tokens in the output
- **Cost**: The estimated cost of the interaction
- **Messages**: The prompt and completion content

```python
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

# Initialize the OpenAI client
client = OpenAI()

# This LLM call is automatically tracked
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "What's the capital of France?"}]
)
```

# Tags

[Tags](/v2/concepts/tags) help you organize and filter your sessions. You can add tags when initializing AgentOps or when starting a session:

```python
# Add tags when initializing
agentops.init(api_key="YOUR_API_KEY", tags=["production", "web-app"])

# Or when manually starting a session
agentops.start_session(tags=["customer-service", "tier-1"])
```

# Host Environment

AgentOps automatically collects basic [information](/v2/concepts/host-env) about the environment where your agent is running:

- **Operating System**: The OS type and version
- **Python Version**: The version of Python being used
- **Hostname**: The name of the host machine (anonymized)
- **SDK Version**: The version of the AgentOps SDK being used

# Dashboard Views

The AgentOps dashboard provides several ways to visualize and analyze your agent's performance:

- **Session List**: Overview of all sessions with filtering options
- **Timeline View**: Chronological display of spans showing duration and relationships
- **Tree View**: Hierarchical representation of spans showing parent-child relationships
- **Message View**: Detailed view of LLM interactions with prompt and completion content
- **Analytics**: Aggregated metrics across sessions and operations

# Putting It All Together

A typical implementation looks like this:

```python
import agentops
from openai import OpenAI
from agentops.sdk.decorators import agent, operation

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY", tags=["production"])

# Define an agent
@agent(name="assistant")
class AssistantAgent:
    def __init__(self):
        self.client = OpenAI()
    
    @operation
    def answer_question(self, question):
        # This LLM call will be automatically tracked and associated with this agent
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": question}]
        )
        return response.choices[0].message.content

def workflow():
    # Use the agent
    assistant = AssistantAgent()
    answer = assistant.answer_question("What's the capital of France?")
    print(answer)

workflow()
# Session is automatically tracked until application terminates
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>

## docs/v2/concepts/tags.mdx

---
title: 'Tags'
description: 'Organize and filter your sessions with customizable tags'
---
## Adding Tags

You can add tags when initializing AgentOps, whicreh is the most common approach:

```python
import agentops

# Initialize AgentOps with tags
agentops.init(
    api_key="YOUR_API_KEY",
    default_tags=["production", "customer-service", "gpt-4"]
)
```

Alternatively, when using manual trace creation:

```python
# Initialize without auto-starting a session
agentops.init(api_key="YOUR_API_KEY", auto_start_session=False)

# Later start a trace with specific tags (modern approach)
trace = agentops.start_trace(trace_name="test_workflow", default_tags=["development", "testing", "claude-3"])
```

<Note>
Legacy approach using `agentops.start_session(default_tags=["development", "testing", "claude-3"])` is deprecated and will be removed in v4.0. Use `agentops.start_trace()` instead.
</Note>

## Tag Use Cases

Tags can be used for various purposes:

### Environment Identification

Tag sessions based on their environment:

```python
default_tags=["production"] # or ["development", "staging", "testing"]
```

### Feature Tracking

Tag sessions related to specific features or components:

```python
default_tags=["search-functionality", "user-authentication", "content-generation"]
```

### User Segmentation

Tag sessions based on user characteristics:

```python
default_tags=["premium-user", "new-user", "enterprise-customer"]
```

### Experiment Tracking

Tag sessions as part of specific experiments:

```python
default_tags=["experiment-123", "control-group", "variant-A"]
```

### Model Identification

Tag sessions with the models being used:

```python
default_tags=["gpt-4", "claude-3-opus", "mistral-large"]
```

## Viewing Tagged Sessions

In the AgentOps dashboard:

1. Use the tag filter to select specific tags
2. Combine multiple tags to refine your view
3. Save filtered views for quick access

## Best Practices

- Use a consistent naming convention for tags
- Include both broad categories and specific identifiers
- Avoid using too many tags per session (3-5 is typically sufficient)
- Consider using hierarchical tag structures (e.g., "env:production", "model:gpt-4")
- Update your tagging strategy as your application evolves

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>

## docs/v2/concepts/host-env.mdx

---
title: 'Host Environment'
description: 'Automatically collected information about the environment where your agent runs'
---
## Collected Information

The following information is automatically collected:

- **Operating System**: The OS type and version (e.g., "Linux", "Windows", "macOS")
- **Python Version**: The version of Python being used
- **Hostname**: The name of the host machine (anonymized)
- **AgentOps SDK Version**: The version of the AgentOps SDK being used
- **Process ID**: The ID of the process running the agent

## Usage in Analytics

Host environment information enables:

- Identifying environment-specific issues
- Tracking performance across different platforms
- Ensuring compatibility with specific OS versions
- Monitoring SDK version adoption

## Privacy Considerations

AgentOps is designed with privacy in mind:

- No personally identifiable information is collected
- Hostnames are anonymized
- No network scanning or detailed system analysis is performed
- Only the minimum necessary information for debugging is gathered

## Disabling Host Environment Collection

If needed, you can disable host environment collection when initializing AgentOps:

```python
import agentops

# Disable host environment collection
agentops.init(api_key="YOUR_API_KEY", env_data_opt_out=True)
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>


## docs/v2/concepts/traces.mdx

---
title: 'Traces'
description: 'Effectively manage traces in your agent workflow'
---
## Automatic Trace Management

The simplest way to create and manage traces is to use the `init` function with automatic trace creation:

```python
import agentops

# Initialize with automatic trace creation (default)
agentops.init(api_key="YOUR_API_KEY", default_tags=["production"])
```

This approach:
- Creates a trace automatically when you initialize the SDK
- Tracks all events in the context of this trace
- Manages the trace throughout the lifecycle of your application

## Manual Trace Creation
For more control, you can disable automatic trace creation and start traces manually:

```python
import agentops

# Initialize without auto-starting a trace
agentops.init(api_key="YOUR_API_KEY", auto_start_session=False)

# Later, manually start a trace when needed
trace_context = agentops.start_trace(
    trace_name="Customer Workflow", 
    tags=["customer-query", "high-priority"]
)

# End the trace when done
agentops.end_trace(trace_context, end_state="Success")
```

Manual trace management is useful when:
- You want to control exactly when trace tracking begins
- You need to associate different traces with different sets of tags
- Your application has distinct workflows that should be tracked separately

## Using the Trace Decorator

You can use the `@trace` decorator to create a trace for a specific function:

```python
import agentops

@agentops.trace
def process_customer_data(customer_id):
    # This entire function execution will be tracked as a trace
    return analyze_data(customer_id)

# Or with custom parameters
@agentops.trace(name="data_processing", tags=["analytics"])
def analyze_user_behavior(user_data):
    return perform_analysis(user_data)
```

## Trace Context Manager

TraceContext objects support Python's context manager protocol, making it easy to manage trace lifecycles:

```python
import agentops

# Using trace context as a context manager
with agentops.start_trace("user_session", tags=["web"]) as trace:
    # All operations here are tracked within this trace
    process_user_request()
    # Trace automatically ends when exiting the context
    # Success/Error state is set based on whether exceptions occurred
```

## Trace States

Every trace has an associated state that indicates its completion status. AgentOps provides multiple ways to specify trace end states for flexibility and backward compatibility.

### AgentOps TraceState Enum (Recommended)

The recommended approach is to use the `TraceState` enum from AgentOps:

```python
from agentops import TraceState

# Available states
agentops.end_trace(trace_context, end_state=TraceState.SUCCESS)  # Trace completed successfully
agentops.end_trace(trace_context, end_state=TraceState.ERROR)    # Trace encountered an error
agentops.end_trace(trace_context, end_state=TraceState.UNSET)    # Trace state is not determined
```

### OpenTelemetry StatusCode

For advanced users familiar with OpenTelemetry, you can use StatusCode directly:

```python
from opentelemetry.trace.status import StatusCode

agentops.end_trace(trace_context, end_state=StatusCode.OK)     # Same as TraceState.SUCCESS
agentops.end_trace(trace_context, end_state=StatusCode.ERROR)  # Same as TraceState.ERROR
agentops.end_trace(trace_context, end_state=StatusCode.UNSET)  # Same as TraceState.UNSET
```

### String Values

String values are also supported for convenience:

```python
# String representations
agentops.end_trace(trace_context, end_state="Success")        # Maps to SUCCESS
agentops.end_trace(trace_context, end_state="Error")          # Maps to ERROR  
agentops.end_trace(trace_context, end_state="Indeterminate")  # Maps to UNSET
```

### State Mapping

All state representations map to the same underlying OpenTelemetry StatusCode:

| AgentOps TraceState | OpenTelemetry StatusCode | String Values | Description |
|-------------------|-------------------------|---------------|-------------|
| `TraceState.SUCCESS` | `StatusCode.OK` | "Success" | Trace completed successfully |
| `TraceState.ERROR` | `StatusCode.ERROR` | "Error" | Trace encountered an error |
| `TraceState.UNSET` | `StatusCode.UNSET` | "Indeterminate" | Trace state is not determined |

### Default Behavior

If no end state is provided, the default is `TraceState.SUCCESS`:

```python
# These are equivalent
agentops.end_trace(trace_context)
agentops.end_trace(trace_context, end_state=TraceState.SUCCESS)
```

## Trace Attributes

Every trace collects comprehensive metadata to provide rich context for analysis. Trace attributes are automatically captured by AgentOps and fall into several categories:

### Core Trace Attributes

**Identity and Timing:**
- **Trace ID**: A unique identifier for the trace
- **Span ID**: Identifier for the root span of the trace
- **Start Time**: When the trace began
- **End Time**: When the trace completed (set automatically)
- **Duration**: Total execution time (calculated automatically)

**User-Defined Attributes:**
- **Trace Name**: Custom name provided when starting the trace
- **Tags**: Labels for filtering and grouping (list of strings or dictionary)
- **End State**: Success, error, or unset status

```python
# Tags can be provided as a list of strings or a dictionary
agentops.start_trace("my_trace", tags=["production", "experiment-a"])
agentops.start_trace("my_trace", tags={"environment": "prod", "version": "1.2.3"})
```

### Resource Attributes

AgentOps automatically captures system and environment information:

**Project and Service:**
- **Project ID**: AgentOps project identifier
- **Service Name**: Service name (defaults to "agentops")
- **Service Version**: Version of your service
- **Environment**: Deployment environment (dev, staging, prod)
- **SDK Version**: AgentOps SDK version being used

**Host System Information:**
- **Host Name**: Machine hostname
- **Host System**: Operating system (Windows, macOS, Linux)
- **Host Version**: OS version details
- **Host Processor**: CPU architecture information
- **Host Machine**: Machine type identifier

**Performance Metrics:**
- **CPU Count**: Number of available CPU cores
- **CPU Percent**: CPU utilization at trace start
- **Memory Total**: Total system memory
- **Memory Available**: Available system memory
- **Memory Used**: Currently used memory
- **Memory Percent**: Memory utilization percentage

**Dependencies:**
- **Imported Libraries**: List of Python packages imported in your environment

### Span Hierarchy

**Nested Operations:**
- **Spans**: All spans (operations, agents, tools, workflows) recorded during the trace
- **Parent-Child Relationships**: Hierarchical structure of operations
- **Span Kinds**: Types of operations (agents, tools, workflows, tasks)

### Accessing Trace Attributes

While most attributes are automatically captured, you can access trace information programmatically:

```python
import agentops

# Start a trace and get the context
trace_context = agentops.start_trace("my_workflow", tags={"version": "1.0"})

# Access trace information
trace_id = trace_context.span.get_span_context().trace_id
span_id = trace_context.span.get_span_context().span_id

print(f"Trace ID: {trace_id}")
print(f"Span ID: {span_id}")

# End the trace
agentops.end_trace(trace_context)
```

### Custom Attributes

You can add custom attributes to spans within your trace:

```python
import agentops

with agentops.start_trace("custom_workflow") as trace:
    # Add custom attributes to the current span
    trace.span.set_attribute("custom.workflow.step", "data_processing")
    trace.span.set_attribute("custom.batch.size", 100)
    trace.span.set_attribute("custom.user.id", "user_123")
    
    # Your workflow logic here
    process_data()
```

### Attribute Naming Conventions

AgentOps follows OpenTelemetry semantic conventions for attribute naming:

- **AgentOps Specific**: `agentops.*` (e.g., `agentops.span.kind`)
- **GenAI Operations**: `gen_ai.*` (e.g., `gen_ai.request.model`)
- **System Resources**: Standard names (e.g., `host.name`, `service.name`)
- **Custom Attributes**: Use your own namespace (e.g., `myapp.user.id`)

## Trace Context

Traces create a context for all span recording. When a span is recorded:

1. It's associated with the current active trace
2. It's automatically included in the trace's timeline
3. It inherits the trace's tags for filtering and analysis

## Viewing Traces in the Dashboard

The AgentOps dashboard provides several views for analyzing your traces:

1. **Trace List**: Overview of all traces with filtering options
2. **Trace Details**: In-depth view of a single trace
3. **Timeline View**: Chronological display of all spans in a trace
4. **Tree View**: Hierarchical representation of agents, operations, and events
5. **Analytics**: Aggregated metrics across traces

## Best Practices

- **Start traces at logical boundaries** in your application workflow
- **Use descriptive trace names** to easily identify them in the dashboard
- **Apply consistent tags** to group related traces
- **Use fewer, longer traces** rather than many short ones for better analysis
- **Use automatic trace management** unless you have specific needs for manual control
- **Leverage context managers** for automatic trace lifecycle management
- **Set appropriate end states** to track success/failure rates

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>


## docs/v2/concepts/spans.mdx

---
title: 'Spans'
description: 'Understanding the different types of spans in AgentOps'
---
## Core Span Types

AgentOps organizes all spans with specific kinds:

| Span Kind | Description |
|-----------|-------------|
| `SESSION` | The root container for all activities in a single execution of your workflow |
| `AGENT` | Represents an autonomous entity with specialized capabilities |
| `WORKFLOW` | A logical grouping of related operations |
| `OPERATION` | A specific task or function performed by an agent |
| `TASK` | Alias for OPERATION, used interchangeably |
| `LLM` | An interaction with a language model |
| `TOOL` | The use of a tool or API by an agent |

## Span Hierarchy

Spans in AgentOps are organized hierarchically:

```
SESSION
  ‚îú‚îÄ‚îÄ AGENT
  ‚îÇ     ‚îú‚îÄ‚îÄ OPERATION/TASK
  ‚îÇ     ‚îÇ     ‚îú‚îÄ‚îÄ LLM
  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ TOOL
  ‚îÇ     ‚îî‚îÄ‚îÄ WORKFLOW
  ‚îÇ           ‚îî‚îÄ‚îÄ OPERATION/TASK
  ‚îî‚îÄ‚îÄ LLM (unattributed to a specific agent)
```

Every span exists within the context of a session, and most spans (other than the session itself) have a parent span that provides context.

## Span Attributes

All spans in AgentOps include:

- **ID**: A unique identifier
- **Name**: A descriptive name
- **Kind**: The type of span (SESSION, AGENT, etc.)
- **Start Time**: When the span began
- **End Time**: When the span completed
- **Status**: Success or error status
- **Attributes**: Key-value pairs with additional metadata

Different span types have specialized attributes:

### LLM Spans

LLM spans track interactions with large language models and include:

- **Model**: The specific model used (e.g., "gpt-4", "claude-3-opus")
- **Provider**: The LLM provider (e.g., "OpenAI", "Anthropic")
- **Prompt Tokens**: Number of tokens in the input
- **Completion Tokens**: Number of tokens in the output
- **Cost**: The estimated cost of the interaction
- **Messages**: The prompt and completion content

### Tool Spans

Tool spans track the use of tools or APIs and include:

- **Tool Name**: The name of the tool used
- **Input**: The data provided to the tool
- **Output**: The result returned by the tool
- **Duration**: How long the tool operation took

### Operation/Task Spans

Operation spans track specific functions or tasks:

- **Operation Type**: The kind of operation performed
- **Parameters**: Input parameters to the operation
- **Result**: The output of the operation
- **Duration**: How long the operation took

## Creating Spans

There are several ways to create spans in AgentOps:

### Using Decorators

The recommended way to create spans is using decorators:

```python
from agentops.sdk.decorators import agent, operation, session, workflow, task

@session
def my_workflow():
    agent_instance = MyAgent()
    return agent_instance.perform_task()

@agent
class MyAgent:
    @operation
    def perform_task(self):
        # Perform the task
        return result
```

### Automatic Instrumentation

AgentOps automatically instruments LLM API calls from supported providers when `auto_instrument=True` (the default):

```python
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

# Initialize the OpenAI client
client = OpenAI()

# This LLM call will be automatically tracked
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Viewing Spans in the Dashboard

All recorded spans are visible in the AgentOps dashboard:

1. **Timeline View**: Shows the sequence and duration of spans
2. **Tree View**: Displays the hierarchical relationship between spans
3. **Details Panel**: Provides in-depth information about each span
4. **Analytics**: Aggregates statistics across spans

## Best Practices

- Use descriptive names for spans to make them easily identifiable
- Create a logical hierarchy with sessions, agents, and operations
- Record relevant parameters and results for better debugging
- Use consistent naming conventions for span types
- Track costs and token usage to monitor resource consumption

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>

## docs/v2/concepts/decorators.mdx

---
title: 'Decorators'
description: 'Use decorators to track activities in your agent system'
---
## Available Decorators

AgentOps provides the following decorators:

| Decorator | Purpose | Creates |
|-----------|---------|---------|
| `@session` | Track an entire user interaction | SESSION span |
| `@agent` | Track agent classes and their lifecycle | AGENT span |
| `@operation` | Track discrete operations performed by agents | OPERATION span |
| `@workflow` | Track a sequence of operations | WORKFLOW span |
| `@task` | Track smaller units of work (similar to operations) | TASK span |
| `@tool` | Track tool usage and cost in agent operations | TOOL span |
| `@guardrail` | Track guardrail input and output | GUARDRAIL span |

## Decorator Hierarchy

The decorators create spans that form a hierarchy:

```
SESSION
  ‚îú‚îÄ‚îÄ AGENT
  ‚îÇ     ‚îú‚îÄ‚îÄ OPERATION or TASK
  ‚îÇ     ‚îÇ     ‚îú‚îÄ‚îÄ LLM
  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ TOOL
  ‚îÇ     ‚îî‚îÄ‚îÄ WORKFLOW
  ‚îÇ           ‚îî‚îÄ‚îÄ OPERATION or TASK
  ‚îî‚îÄ‚îÄ AGENT
        ‚îî‚îÄ‚îÄ OPERATION or TASK
```

## Using Decorators

### @session

The `@session` decorator tracks an entire user interaction from start to finish:

```python
from agentops.sdk.decorators import session
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@session
def answer_question(question):
    # Create and use agents
    weather_agent = WeatherAgent()
    result = weather_agent.get_forecast(question)
    
    # Return the final result
    return result
```

Each `@session` function call creates a new session span that contains all the agents, operations, and workflows used during that interaction.

### @agent

The `@agent` decorator instruments a class to track its lifecycle and operations:

```python
from agentops.sdk.decorators import agent, operation
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@agent
class WeatherAgent:
    def __init__(self):
        self.api_key = "weather_api_key"
        
    @operation
    def get_forecast(self, location):
        # Get weather data
        return f"The weather in {location} is sunny."

def check_weather(city):
    weather_agent = WeatherAgent()
    forecast = weather_agent.get_forecast(city)
    return forecast

weather_info = check_weather("San Francisco")
```

When an agent-decorated class is instantiated within a session, an AGENT span is created automatically.

### @operation

The `@operation` decorator tracks discrete functions performed by an agent:

```python
from agentops.sdk.decorators import agent, operation
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@agent
class MathAgent:
    @operation
    def add(self, a, b):
        return a + b
        
    @operation
    def multiply(self, a, b):
        return a * b

def calculate(x, y):
    math_agent = MathAgent()
    sum_result = math_agent.add(x, y)
    product_result = math_agent.multiply(x, y)
    return {"sum": sum_result, "product": product_result}

results = calculate(5, 3)
```

Operations represent the smallest meaningful units of work in your agent system. Each operation creates an OPERATION span with:
- Inputs (function arguments)
- Output (return value)
- Duration
- Success/failure status

### @workflow

The `@workflow` decorator tracks a sequence of operations that work together:

```python
from agentops.sdk.decorators import agent, operation, workflow
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@agent
class TravelAgent:
    def __init__(self):
        self.flight_api = FlightAPI()
        self.hotel_api = HotelAPI()
    
    @workflow
    def plan_trip(self, destination, dates):
        # This workflow contains multiple operations
        flights = self.find_flights(destination, dates)
        hotels = self.find_hotels(destination, dates)
        
        return {
            "flights": flights,
            "hotels": hotels
        }
        
    @operation
    def find_flights(self, destination, dates):
        return self.flight_api.search(destination, dates)
        
    @operation
    def find_hotels(self, destination, dates):
        return self.hotel_api.search(destination, dates)
```

Workflows help you organize related operations and see their collective performance.

### @task

The `@task` decorator is similar to `@operation` but can be used for smaller units of work:

```python
from agentops.sdk.decorators import agent, task
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@agent
class DataProcessor:
    @task
    def normalize_data(self, data):
        # Normalize the data
        return [x / sum(data) for x in data]
    
    @task
    def filter_outliers(self, data, threshold=3):
        # Filter outliers
        mean = sum(data) / len(data)
        std_dev = (sum((x - mean) ** 2 for x in data) / len(data)) ** 0.5
        
        return [x for x in data if abs(x - mean) <= threshold * std_dev]
```

The `@task` and `@operation` decorators function identically (they are aliases in the codebase), and you can choose the one that best fits your semantic needs.

### @tool

The `@tool` decorator tracks tool usage within agent operations and supports cost tracking. It works with all function types: synchronous, asynchronous, generator, and async generator.

```python
from agentops.sdk.decorators import agent, tool
import asyncio

@agent
class ProcessingAgent:
    def __init__(self):
        pass

    @tool(cost=0.01)
    def sync_tool(self, item):
        """Synchronous tool with cost tracking."""
        return f"Processed {item}"

    @tool(cost=0.02)
    async def async_tool(self, item):
        """Asynchronous tool with cost tracking."""
        await asyncio.sleep(0.1)
        return f"Async processed {item}"

    @tool(cost=0.03)
    def generator_tool(self, items):
        """Generator tool with cost tracking."""
        for item in items:
            yield self.sync_tool(item)

    @tool(cost=0.04)
    async def async_generator_tool(self, items):
        """Async generator tool with cost tracking."""
        for item in items:
            await asyncio.sleep(0.1)
            yield await self.async_tool(item)
```

The tool decorator provides:
- Cost tracking for each tool call
- Proper span creation and nesting
- Support for all function types (sync, async, generator, async generator)
- Cost accumulation in generator and async generator operations

### @guardrail

The `@guardrail` decorator tracks guardrail input and output. You can specify the guardrail type (`"input"` or `"output"`) with the `spec` parameter.

```python
from agentops.sdk.decorators import guardrail
import agentops
import re

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@guardrail(spec="input")
def secret_key_guardrail(input):
    pattern = r'\bsk-[a-zA-Z0-9]{10,}\b'
    result = True if re.search(pattern, input) else False
    return {
        "tripwire_triggered" : result
    }
```

## Decorator Attributes

You can pass additional attributes to decorators:

```python
from agentops.sdk.decorators import agent, operation
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@agent(name="custom_agent_name", attributes={"version": "1.0"})
class CustomAgent:
    @operation(name="custom_operation", attributes={"importance": "high"})
    def process(self, data):
        return data
```

Common attributes include:

| Attribute | Description | Example |
|-----------|-------------|---------|
| `name` | Custom name for the span | `name="weather_forecast"` |
| `attributes` | Dictionary of custom attributes | `attributes={"model": "gpt-4"}` |

## Complete Example

Here's a complete example using all the decorators together:

```python
from agentops.sdk.decorators import session, agent, operation, workflow, task
import agentops

# Initialize AgentOps
agentops.init(api_key="YOUR_API_KEY")

@session
def assist_user(query):
    # Create the main assistant
    assistant = Assistant()
    
    # Process the query
    return assistant.process_query(query)

@agent
class Assistant:
    def __init__(self):
        pass
    
    @workflow
    def process_query(self, query):
        research_agent = ResearchAgent()
        writing_agent = WritingAgent()
        
        # Research phase
        research = research_agent.gather_information(query)
        
        # Writing phase
        response = writing_agent.generate_response(query, research)
        
        return response

@agent
class ResearchAgent:
    @operation
    def gather_information(self, query):
        # Perform web search
        search_results = self.search(query)
        
        # Analyze results
        return self.analyze_results(search_results)
    
    @task
    def search(self, query):
        # Simulate web search
        return [f"Result for {query}", f"Another result for {query}"]
    
    @task
    def analyze_results(self, results):
        # Analyze search results
        return {"summary": "Analysis of " + ", ".join(results)}

@agent
class WritingAgent:
    @operation
    def generate_response(self, query, research):
        # Generate a response based on the research
        return f"Answer to '{query}' based on: {research['summary']}"

assist_user("What is the capital of France?")
```

In this example:
1. The `@session` decorator wraps the entire interaction
2. The `@agent` decorator defines multiple agent classes
3. The `@workflow` decorator creates a workflow that coordinates agents
4. The `@operation` and `@task` decorators track individual operations
5. All spans are properly nested in the hierarchy

Note that LLM and TOOL spans are automatically created when you use compatible LLM libraries or tool integrations.

## Best Practices

- **Use @session for top-level functions** that represent complete user interactions
- **Apply @agent to classes** that represent distinct components of your system
- **Use @operation for significant functions** that represent complete units of work
- **Use @task for smaller functions** that are part of larger operations
- **Apply @workflow to methods** that coordinate multiple operations
- **Keep decorator nesting consistent** with the logical hierarchy of your code
- **Add custom attributes** to provide additional context for analysis
- **Use meaningful names** for all decorated components

## Dashboard Visualization

In the AgentOps dashboard, decorators create spans that appear in:

1. **Timeline View**: Shows the execution sequence and duration
2. **Hierarchy View**: Displays the parent-child relationships
3. **Detail Panels**: Shows inputs, outputs, and attributes
4. **Performance Metrics**: Tracks execution times and success rates

This visualization helps you understand the flow and performance of your agent system.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>

## docs/v2/integrations/ag2.mdx

---
title: AG2
description: "Track and analyze your AG2 agents with AgentOps"
---

## Installation

<CodeGroup>
  ```bash pip
  pip install agentops pyautogen
  ```
  ```bash poetry
  poetry add agentops pyautogen
  ```
  ```bash uv
  uv add agentops pyautogen
  ```
</CodeGroup>

## Setting Up API Keys

Before using AG2 with AgentOps, you need to set up your API keys. You can obtain:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
    ```bash Export to CLI
    export OPENAI_API_KEY="your_openai_api_key_here"
    export AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
    ```txt Set in .env file
    OPENAI_API_KEY="your_openai_api_key_here"
    AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all AG2 agent interactions:

<CodeGroup> 
```python Single Agent Conversation
import agentops
import autogen
import os

# Initialize AgentOps
agentops.init()

# Configure your AG2 agents
config_list = [
    {
        "model": "gpt-4",
        "api_key": os.getenv("OPENAI_API_KEY"),
    }
]

llm_config = {
    "config_list": config_list,
    "timeout": 60,
}

# Create a single agent
assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config=llm_config,
    system_message="You are a helpful AI assistant."
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"last_n_messages": 3, "work_dir": "coding"},
)

# Initiate a conversation
user_proxy.initiate_chat(
    assistant,
    message="How can I implement a basic web scraper in Python?"
)
```

```python Multi-Agent Conversation
import os
import agentops
import autogen

# Initialize AgentOps
agentops.init()

# Configure your AG2 agents
config_list = [
    {
        "model": "gpt-4",
        "api_key": os.getenv("OPENAI_API_KEY"),
    }
]

llm_config = {
    "config_list": config_list,
    "timeout": 60,
}

# Create a team of agents
researcher = autogen.AssistantAgent(
    name="researcher",
    llm_config=llm_config,
    system_message="You are a researcher who specializes in finding accurate information."
)

coder = autogen.AssistantAgent(
    name="coder",
    llm_config=llm_config,
    system_message="You are an expert programmer who writes clean, efficient code."
)

critic = autogen.AssistantAgent(
    name="critic",
    llm_config=llm_config,
    system_message="You review solutions and provide constructive feedback."
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"last_n_messages": 3, "work_dir": "coding"},
)

# Create a group chat
groupchat = autogen.GroupChat(
    agents=[user_proxy, researcher, coder, critic],
    messages=[],
    max_round=12
)

manager = autogen.GroupChatManager(
    groupchat=groupchat,
    llm_config=llm_config
)

# Initiate the group chat
user_proxy.initiate_chat(
    manager,
    message="Create a Python program to analyze sentiment from Twitter data."
)
```
</CodeGroup>

## Examples

<CardGroup cols={2}>
  <Card title="AG2 Memory Chat" icon="notebook" href="/v2/examples/ag2">
    Multi-agent conversation with memory capabilities
  </Card>
  <Card title="Async Human Input" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/ag2/async_human_input.ipynb" newTab={true}>
    Demonstrates asynchronous human input with AG2 agents.
  </Card>
  <Card title="Wikipedia Search Tool" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/ag2/tools_wikipedia_search.ipynb" newTab={true}>
    Example of AG2 agents using a Wikipedia search tool.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/mem0.mdx

---
title: 'Mem0'
description: 'Track and monitor Mem0 memory operations with AgentOps'
---

[Mem0](https://mem0.ai/) provides a smart memory layer for AI applications, enabling personalized interactions by remembering user preferences, conversation history, and context across sessions.

## Why Track Mem0 with AgentOps?

When building memory-powered AI applications, you need visibility into:
- **Memory Operations**: Track when memories are created, updated, or retrieved
- **Search Performance**: Monitor how effectively your AI finds relevant memories
- **Memory Usage Patterns**: Understand what information is being stored and accessed
- **Error Tracking**: Identify issues with memory storage or retrieval
- **Cost Analysis**: Track API calls to both Mem0 and your LLM provider

AgentOps automatically instruments Mem0 to provide complete observability of your memory operations.

## Installation

<CodeGroup>
```bash pip
pip install agentops mem0ai python-dotenv
```

```bash poetry
poetry add agentops mem0ai python-dotenv
```

```bash uv
uv add agentops mem0ai python-dotenv
```
</CodeGroup>

## Environment Configuration

Load environment variables and set up API keys. The MEM0_API_KEY is only required if you're using the cloud-based MemoryClient.
<CodeGroup>
  ```bash Export to CLI
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  export OPENAI_API_KEY="your_openai_api_key_here"
  ```
  ```txt Set in .env file
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  OPENAI_API_KEY="your_openai_api_key_here"
  ```
</CodeGroup>

## Tracking Memory Operations

<CodeGroup>
```python Local Memory
import agentops
from mem0 import Memory

# Start a trace to group related operations
agentops.start_trace("user_preference_learning",tags=["mem0_memory_example"])

try:
    # Initialize Memory - AgentOps tracks the configuration
    memory = Memory.from_config({
        "llm": {
            "provider": "openai",
            "config": {
                "model": "gpt-4o-mini",
                "temperature": 0.1
            }
        }
    })

    # Add memories - AgentOps tracks each operation
    memory.add(
        "I prefer morning meetings and dark roast coffee",
        user_id="user_123",
        metadata={"category": "preferences"}
    )

    # Search memories - AgentOps tracks search queries and results
    results = memory.search(
        "What are the user's meeting preferences?",
        user_id="user_123"
    )

    # End trace - AgentOps aggregates all operations
    agentops.end_trace(end_state="success")
    
except Exception as e:
    agentops.end_trace(end_state="error")
```

```python Cloud Memory
import agentops
from mem0 import MemoryClient

# Start trace for cloud operations
agentops.start_trace("cloud_memory_sync",tags=["mem0_memoryclient_example"])

try:
    # Initialize MemoryClient - AgentOps tracks API authentication
    client = MemoryClient(api_key="your_mem0_api_key")

    # Batch add memories - AgentOps tracks bulk operations
    messages = [
        {"role": "user", "content": "I work in software engineering"},
        {"role": "user", "content": "I prefer Python over Java"},
    ]

    client.add(messages, user_id="user_123")

    # Search with filters - AgentOps tracks complex queries
    filters = {"AND": [{"user_id": "user_123"}]}
    results = client.search(
        query="What programming languages does the user know?",
        filters=filters,
        version="v2"
    )

    # End trace - AgentOps aggregates all operations
    agentops.end_trace(end_state="success")

except Exception as e:
    agentops.end_trace(end_state="error")
```
</CodeGroup>

## What You'll See in AgentOps

When using Mem0 with AgentOps, your dashboard will show:

1. **Memory Operation Timeline**: Visual flow of all memory operations
2. **Search Analytics**: Query patterns and retrieval effectiveness  
3. **Memory Growth**: Track how user memories accumulate over time
4. **Performance Metrics**: Latency for adds, searches, and retrievals
5. **Error Tracking**: Failed operations with full error context
6. **Cost Attribution**: Token usage for memory extraction and searches

## Examples

<CardGroup cols={2}>
  <Card title="Memory Operations" icon="book" href="/v2/examples/mem0">
    Simple example showing memory storage and retrieval with AgentOps tracking
  </Card>
  
  <Card title="MemoryClient Operations" icon="cloud" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/mem0/mem0_memoryclient_example.ipynb">
    Track concurrent memory operations with async/await patterns
  </Card>

</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/integrations/ibm_watsonx_ai.mdx

---
title: IBM Watsonx.ai
description: "Track and analyze your IBM Watsonx.ai API calls with AgentOps"
---

AgentOps provides seamless integration with [IBM Watsonx.ai Python SDK](https://ibm.github.io/watsonx-ai-python-sdk/), allowing you to track and analyze all your Watsonx.ai model interactions automatically.

## Installation

<CodeGroup>
```bash pip
pip install agentops ibm-watsonx-ai
```
```bash poetry
poetry add agentops ibm-watsonx-ai
```
```bash uv
uv add agentops ibm-watsonx-ai
```
</CodeGroup>

## Setting Up API Keys

Before using IBM Watsonx.ai with AgentOps, you need to set up your API keys. You can obtain:
- **IBM_WATSONX_API_KEY**: From your [IBM Cloud account](https://cloud.ibm.com/)
- **IBM_WATSONX_URL**: The URL for your Watsonx.ai instance, typically found in your IBM Cloud dashboard.
- **IBM_WATSONX_PROJECT_ID**: The project ID for your Watsonx.ai project, which you can find in the Watsonx.ai console.
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export IBM_WATSONX_API_KEY="your_ibm_api_key_here"
export IBM_WATSONX_URL="your_ibm_url_here"
export IBM_WATSONX_PROJECT_ID="your_project_id_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
IBM_WATSONX_API_KEY="your_ibm_api_key_here"
IBM_WATSONX_URL="your_ibm_url_here"
IBM_WATSONX_PROJECT_ID="your_project_id_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["IBM_WATSONX_API_KEY"] = os.getenv("IBM_WATSONX_API_KEY")
os.environ["IBM_WATSONX_URL"] = os.getenv("IBM_WATSONX_URL")
os.environ["IBM_WATSONX_PROJECT_ID"] = os.getenv("IBM_WATSONX_PROJECT_ID")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all IBM Watsonx.ai API calls:

```python
import agentops
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference

# Initialize AgentOps
agentops.init(api_key="")

# Initialize credentials
credentials = Credentials(
    url=os.getenv("IBM_WATSONX_URL"),
    api_key=os.getenv("IBM_WATSONX_API_KEY"),
)

# Project ID
project_id = os.getenv("IBM_WATSONX_PROJECT_ID")

# Create a model instance
model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=credentials,
    project_id=project_id
)

# Make a completion request
response = model.generate_text("What is artificial intelligence?")
print(f"Generated Text:\n{response}")

# Don't forget to close connection when done
model.close_persistent_connection()
```

## Examples
<CardGroup cols={2}>
  <Card title="Watsonx Text Chat" icon="notebook" href="/v2/examples/watsonx">
    Basic text generation and chat
  </Card>
  <Card title="Watsonx Streaming" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/watsonx/watsonx-streaming.ipynb" newTab={true}>
    Demonstrates streaming responses with Watsonx.ai.
  </Card>
  <Card title="Watsonx Tokenization" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/watsonx/watsonx-tokeniation-model.ipynb" newTab={true}>
    Example of text tokenization with Watsonx.ai models.
  </Card>
</CardGroup>

## Additional Resources

- [IBM Watsonx.ai Python SDK Documentation](https://ibm.github.io/watsonx-ai-python-sdk/)
- [IBM Watsonx.ai Models](http://ibm.com/products/watsonx-ai/foundation-models)

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/openai_agents_js.mdx

---
title: OpenAI Agents JS
description: 'AgentOps integration with the OpenAI Agents SDK for TypeScript/JavaScript.'
---

[OpenAI Agents JS](https://github.com/openai/openai-agents-js) is a lightweight yet powerful SDK for building multi-agent workflows in TypeScript. AgentOps seamlessly integrates to provide observability into these workflows.

- [OpenAI Agents JS documentation](https://openai.github.io/openai-agents-js)
- [Python guide](/v2/integrations/openai_agents_python)

## Installation

```bash
npm install agentops @openai/agents
```

## Usage

```typescript
import { agentops } from 'agentops';
import { Agent, run } from '@openai/agents';

await agentops.init();

const agent = new Agent({
  name: 'Assistant',
  instructions: 'You are a helpful assistant.'
});

const result = await run(agent, 'Hello, world!');
console.log(result.finalOutput);
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/integrations/agno.mdx

---
title: Agno
description: "Track your Agno agents, teams, and workflows with AgentOps"
---

[Agno](https://docs.agno.com) is a modern AI agent framework for building intelligent agents, teams, and workflows. AgentOps provides automatic instrumentation to track all Agno operations including agent interactions, team coordination, tool usage, and workflow execution.

## Installation

Install AgentOps and Agno:

<CodeGroup>
  ```bash pip
  pip install agentops agno 
  ```
  ```bash poetry
  poetry add agentops agno 
  ```
  ```bash uv
  uv add agentops agno 
  ```
</CodeGroup>

## Setting Up API Keys

You'll need API keys for AgentOps and your chosen LLM provider:
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys) (if using OpenAI)
- **ANTHROPIC_API_KEY**: From [Anthropic Console](https://console.anthropic.com/) (if using Claude)

Set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  export OPENAI_API_KEY="your_openai_api_key_here"
  export ANTHROPIC_API_KEY="your_anthropic_api_key_here"  # Optional
  ```
  ```txt Set in .env file
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  OPENAI_API_KEY="your_openai_api_key_here"
  ANTHROPIC_API_KEY="your_anthropic_api_key_here"  # Optional
  ```
</CodeGroup>

## Quick Start

```python
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat

# Initialize AgentOps
import agentops
agentops.init(api_key=os.getenv("AGENTOPS_API_KEY"))

# Create and run an agent
agent = Agent(
    name="Assistant",
    role="Helpful AI assistant",
    model=OpenAIChat(id="gpt-4o-mini")
)

response = agent.run("What are the key benefits of AI agents?")
print(response.content)
```

## AgentOps Integration

### Basic Agent Tracking

AgentOps automatically instruments Agno agents and teams:

```python
import agentops
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat

# Initialize AgentOps - this enables automatic tracking
agentops.init(api_key=os.getenv("AGENTOPS_API_KEY"))

# Create agents - automatically tracked by AgentOps
agent = Agent(
    name="Assistant",
    role="Helpful AI assistant",
    model=OpenAIChat(id="gpt-4o-mini")
)

# Create teams - coordination automatically tracked
team = Team(
    name="Research Team", 
    mode="coordinate", 
    members=[agent]
)

# All operations are automatically logged to AgentOps
response = team.run("Analyze the current AI market trends")
print(response.content)
```

## What Gets Tracked

AgentOps automatically captures:

- **Agent Interactions**: All agent inputs, outputs, and configurations
- **Team Coordination**: Multi-agent collaboration patterns and results  
- **Tool Executions**: Function calls, parameters, and return values
- **Workflow Steps**: Session states, caching, and performance metrics
- **Token Usage**: Costs and resource consumption across all operations
- **Timing Metrics**: Response times and concurrent operation performance
- **Error Tracking**: Failures and debugging information


## Dashboard and Monitoring

Once your Agno agents are running with AgentOps, you can monitor them in the [AgentOps Dashboard](https://app.agentops.ai/):

- **Real-time Monitoring**: Live agent status and performance
- **Execution Traces**: Detailed logs of agent interactions
- **Performance Analytics**: Token usage, costs, and timing metrics
- **Team Collaboration**: Visual representation of multi-agent workflows
- **Error Tracking**: Comprehensive error logs and debugging information

## Examples

<CardGroup cols={2}>
  <Card title="Basic Agents and Teams" icon="users" href="/v2/examples/agno">
    Learn the fundamentals of creating AI agents and organizing them into collaborative teams
  </Card>
  
  <Card title="Async Operations" icon="bolt" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/agno/agno_async_operations.ipynb">
    Execute multiple AI tasks concurrently for improved performance using asyncio
  </Card>
  
  <Card title="Research Team Collaboration" icon="magnifying-glass" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/agno/agno_research_team.ipynb">
    Build sophisticated multi-agent teams with specialized tools for comprehensive research
  </Card>
  
  <Card title="RAG Tool Integration" icon="database" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/agno/agno_tool_integrations.ipynb">
    Implement Retrieval-Augmented Generation with vector databases and knowledge bases
  </Card>
  
  <Card title="Workflow Setup with Caching" icon="diagram-project" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/agno/agno_workflow_setup.ipynb">
    Create custom workflows with intelligent caching for optimized agent performance
  </Card>
</CardGroup>



## docs/v2/integrations/litellm.mdx

---
title: LiteLLM
description: "Track and analyze your LiteLLM calls across multiple providers with AgentOps"
---

AgentOps provides seamless integration with [LiteLLM](https://github.com/BerriAI/litellm), allowing you to automatically track all your LLM API calls across different providers through a unified interface.

## Installation

<CodeGroup>
```bash pip
pip install agentops litellm
```
```bash poetry
poetry add agentops litellm
```
```bash uv
uv add agentops litellm
```
</CodeGroup>

## Setting Up API Keys

Before using LiteLLM with AgentOps, you need to set up your API keys. You can obtain:
- **Provider API Keys**: From your chosen LLM provider (OpenAI, Anthropic, Google, etc.)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export OPENAI_API_KEY="your_openai_api_key_here"
export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
OPENAI_API_KEY="your_openai_api_key_here"
ANTHROPIC_API_KEY="your_anthropic_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["ANTHROPIC_API_KEY"] = os.getenv("ANTHROPIC_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

The simplest way to integrate AgentOps with LiteLLM is to set up the success_callback.

```python
import litellm
from litellm import completion

# Configure LiteLLM to use AgentOps
litellm.success_callback = ["agentops"]

# Make completion requests with LiteLLM
response = completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello, how are you?"}]
)

print(response.choices[0].message.content)
```

## Examples

<CodeGroup>
```python Streaming
import litellm
from litellm import completion

# Configure LiteLLM to use AgentOps
litellm.success_callback = ["agentops"]

# Make a streaming completion request
response = completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Write a short poem about AI."}],
    stream=True
)

# Process the streaming response
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

```python Multi-Provider
import litellm
from litellm import completion

# Configure LiteLLM to use AgentOps
litellm.success_callback = ["agentops"]

# OpenAI request
openai_response = completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "What are the advantages of GPT-4?"}]
)

print("OpenAI Response:", openai_response.choices[0].message.content)

# Anthropic request using the same interface
anthropic_response = completion(
    model="anthropic/claude-3-opus-20240229",
    messages=[{"role": "user", "content": "What are the advantages of Claude?"}]
)

print("Anthropic Response:", anthropic_response.choices[0].message.content)

# All requests across different providers are automatically tracked by AgentOps
```
</CodeGroup>


## More Examples
<CardGroup cols={2}>
  <Card title="LiteLLM Quickstart Notebook" icon="notebook" href="/v2/examples/litellm" />
</CardGroup>

For more information on integrating AgentOps with LiteLLM, refer to the [LiteLLM documentation on AgentOps integration](https://docs.litellm.ai/docs/observability/agentops_integration).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/xai.mdx

---
title: xAI (Grok)
description: "Track and analyze your xAI Grok API calls with AgentOps"
---

AgentOps can track Grok. Grok is this true?

## Installation

<CodeGroup>
  ```bash pip
  pip install agentops openai
  ```
  ```bash poetry
  poetry add agentops openai
  ``
  ```bash uv
  uv add agentops openai
  ```
</CodeGroup>

## Setting Up API Keys

Before using xAI with AgentOps, you need to set up your API keys. You can obtain:
- **XAI_API_KEY**: From the [xAI Developer Platform](https://console.x.ai/)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export XAI_API_KEY="your_xai_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
XAI_API_KEY="your_xai_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["XAI_API_KEY"] = os.getenv("XAI_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application. Then, use the OpenAI SDK with xAI's base URL to interact with Grok. AgentOps will automatically track all API calls.

<CodeGroup>
```python Simple Chat
import os
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client configured for xAI
client = OpenAI(
    api_key=os.getenv("XAI_API_KEY"),
    base_url="https://api.x.ai/v1",
)

# Basic chat completion
completion = client.chat.completions.create(
    model="grok-3-latest",
    messages=[
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": "Explain the concept of AI observability in simple terms."},
    ],
)

print(completion.choices[0].message.content)
```

```python Streaming Chat
import os
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client configured for xAI
client = OpenAI(
    api_key=os.getenv("XAI_API_KEY"),
    base_url="https://api.x.ai/v1",
)

# Streaming chat completion
stream = client.chat.completions.create(
    model="grok-3-latest",
    messages=[
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": "Tell me about the latest developments in AI."},
    ],
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```
</CodeGroup>

## Examples
<CardGroup cols={2}>
  <Card title="Grok Simple Example" icon="notebook" href="/v2/examples/xai">
    Basic usage patterns for Grok LLM
  </Card>
  <Card title="Grok Vision Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/xai/grok_vision_examples.ipynb" newTab={true}>
    Demonstrates using Grok with vision capabilities.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/llamaindex.mdx

---
title: 'LlamaIndex'
description: 'AgentOps works seamlessly with LlamaIndex, a framework for building context-augmented generative AI applications with LLMs.'
---

[LlamaIndex](https://www.llamaindex.ai/) is a framework for building context-augmented generative AI applications with LLMs. AgentOps provides comprehensive observability into your LlamaIndex applications through automatic instrumentation, allowing you to monitor LLM calls, track performance, and analyze your application's behavior.

## Installation

Install AgentOps and the LlamaIndex AgentOps instrumentation package:

<CodeGroup>
  ```bash pip
  pip install agentops llama-index-instrumentation-agentops
  ```
  ```bash poetry
  poetry add agentops llama-index-instrumentation-agentops
  ```
  ```bash uv
  uv add agentops llama-index-instrumentation-agentops
  ```
</CodeGroup>

## Setting Up API Keys

You'll need an AgentOps API key from your [AgentOps Dashboard](https://app.agentops.ai/):

<CodeGroup>
  ```bash Export to CLI
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>

## Usage

Simply set the global handler to "agentops" at the beginning of your LlamaIndex application. AgentOps will automatically instrument LlamaIndex to track your LLM interactions and application performance.

```python
from llama_index.core import set_global_handler
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Set the global handler to AgentOps
# NOTE: Feel free to set your AgentOps environment variables (e.g., 'AGENTOPS_API_KEY')
# as outlined in the AgentOps documentation, or pass the equivalent keyword arguments
# anticipated by AgentOps' AOClient as **eval_params in set_global_handler.
set_global_handler("agentops")

# Your LlamaIndex application code here
documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)

# Create a query engine
query_engine = index.as_query_engine()

# Query your data - AgentOps will automatically track this
response = query_engine.query("What is the main topic of these documents?")
print(response)
```

## What Gets Tracked

When you use AgentOps with LlamaIndex, the following operations are automatically tracked:

- **LLM Calls**: All interactions with language models including prompts, completions, and token usage
- **Embeddings**: Vector embedding generation and retrieval operations
- **Query Operations**: Search and retrieval operations on your indexes
- **Performance Metrics**: Response times, token costs, and success/failure rates

## Additional Resources

For more detailed information about LlamaIndex's observability features and AgentOps integration, check out the [LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/module_guides/observability/#agentops).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/smolagents.mdx

---
title: Smolagents
description: "Track and analyze your Smolagents AI agents with AgentOps"
---

AgentOps provides seamless integration with [Smolagents](https://github.com/huggingface/smolagents), HuggingFace's lightweight framework for building AI agents. Monitor your agent workflows, tool usage, and execution traces automatically.

## Core Concepts

Smolagents is designed around several key concepts:

- **Agents**: AI assistants that can use tools and reason through problems
- **Tools**: Functions that agents can call to interact with external systems
- **Models**: LLM backends that power agent reasoning (supports various providers via LiteLLM)
- **Code Execution**: Agents can write and execute Python code in sandboxed environments
- **Multi-Agent Systems**: Orchestrate multiple specialized agents working together

## Installation

Install AgentOps and Smolagents, along with any additional dependencies:

<CodeGroup>
  ```bash pip
  pip install agentops smolagents python-dotenv
  ```
  ```bash poetry
  poetry add agentops smolagents python-dotenv
  ```
  ```bash uv
  uv add agentops smolagents python-dotenv
  ```
</CodeGroup>

## Setting Up API Keys

Before using Smolagents with AgentOps, you need to set up your API keys:
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)
- **LLM API Keys**: Depending on your chosen model provider (e.g., OPENAI_API_KEY, ANTHROPIC_API_KEY)

Set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  export OPENAI_API_KEY="your_openai_api_key_here"
  ```
  ```txt Set in .env file
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  OPENAI_API_KEY="your_openai_api_key_here"
  ```
</CodeGroup>

Then load them in your Python code:
```python
from dotenv import load_dotenv
import os

load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

## Usage

Initialize AgentOps before creating your Smolagents to automatically track all agent interactions:

```python
import agentops
from smolagents import LiteLLMModel, ToolCallingAgent, DuckDuckGoSearchTool

# Initialize AgentOps
agentops.init()

# Create a model (supports various providers via LiteLLM)
model = LiteLLMModel("openai/gpt-4o-mini")

# Create an agent with tools
agent = ToolCallingAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
)

# Run the agent
result = agent.run("What are the latest developments in AI safety research?")
print(result)
```

## Examples

<CodeGroup>
```python Simple Math Agent
import agentops
from smolagents import LiteLLMModel, CodeAgent

# Initialize AgentOps
agentops.init()

# Create a model
model = LiteLLMModel("openai/gpt-4o-mini")

# Create a code agent that can perform calculations
agent = CodeAgent(
    tools=[],  # No external tools needed for math
    model=model,
    additional_authorized_imports=["math", "numpy"],
)

# Ask the agent to solve a math problem
result = agent.run(
    "Calculate the compound interest on $10,000 invested at 5% annual rate "
    "for 10 years, compounded monthly. Show your work."
)

print(result)
```

```python Research Agent with Tools
import agentops
from smolagents import (
    LiteLLMModel,
    ToolCallingAgent,
    DuckDuckGoSearchTool,
    tool
)

# Initialize AgentOps
agentops.init()

# Create a custom tool
@tool
def word_counter(text: str) -> str:
    """
    Counts the number of words in a given text.
    
    Args:
        text: The text to count words in.
        
    Returns:
        A string with the word count.
    """
    word_count = len(text.split())
    return f"The text contains {word_count} words."

# Create model and agent
model = LiteLLMModel("openai/gpt-4o-mini")

agent = ToolCallingAgent(
    tools=[DuckDuckGoSearchTool(), word_counter],
    model=model,
)

# Run a research task
result = agent.run(
    "Search for information about the James Webb Space Telescope's latest discoveries. "
    "Then count how many words are in your summary."
)

print(result)
```

```python Multi-Step Task Agent
import agentops
from smolagents import LiteLLMModel, CodeAgent, tool
import json

# Initialize AgentOps
agentops.init()

# Create tools for data processing
@tool
def save_json(data: dict, filename: str) -> str:
    """
    Saves data to a JSON file.
    
    Args:
        data: Dictionary to save
        filename: Name of the file to save to
        
    Returns:
        Success message
    """
    with open(filename, 'w') as f:
        json.dump(data, f, indent=2)
    return f"Data saved to {filename}"

@tool
def load_json(filename: str) -> dict:
    """
    Loads data from a JSON file.
    
    Args:
        filename: Name of the file to load from
        
    Returns:
        The loaded data as a dictionary
    """
    with open(filename, 'r') as f:
        return json.load(f)

# Create agent
model = LiteLLMModel("openai/gpt-4o-mini")

agent = CodeAgent(
    tools=[save_json, load_json],
    model=model,
    additional_authorized_imports=["pandas", "datetime"],
)

# Run a multi-step data processing task
result = agent.run("""
1. Create a dataset of 5 fictional employees with names, departments, and salaries
2. Save this data to 'employees.json'
3. Load the data back and calculate the average salary
4. Find the highest paid employee
5. Return a summary of your findings
""")

print(result)
```
</CodeGroup>
## More Examples

<CardGroup cols={2}>
  <Card title="Multi-Agent System" icon="notebook" href="/v2/examples/smolagents" newTab={true}>
    Complex multi-agent web browsing system
  </Card>
  <Card title="Text to SQL Agent" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/smolagents/text_to_sql.ipynb" newTab={true}>
    Convert natural language queries to SQL
  </Card>
</CardGroup>

Visit your [AgentOps Dashboard](https://app.agentops.ai) to see detailed traces of your Smolagents executions, tool usage, and agent reasoning steps.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>

## docs/v2/integrations/openai.mdx

---
title: OpenAI
description: "Track and analyze your OpenAI API calls with AgentOps"
---

AgentOps seamlessly integrates with [OpenAI's Python SDK](https://github.com/openai/openai-python), allowing you to track and analyze all your OpenAI API calls automatically.

## Installation

<CodeGroup>
```bash pip
pip install agentops openai
```
```bash poetry
poetry add agentops openai
```
```bash uv
uv add agentops openai
```
</CodeGroup>

## Setting Up API Keys

Before using OpenAI with AgentOps, you need to set up your API keys. You can obtain:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export OPENAI_API_KEY="your_openai_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
OPENAI_API_KEY="your_openai_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all OpenAI API calls:

```python
import agentops
from openai import OpenAI
      
# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Make API calls as usual - AgentOps will track them automatically
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)

print(response.choices[0].message.content)
```

## Examples

<CodeGroup>
```python Streaming
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Make a streaming API call
stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about AI."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

```python Function Calling
import json
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Define tools
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    }
                },
                "required": ["location"],
            },
        },
    }
]

# Function implementation
def get_weather(location):
    return json.dumps({"location": location, "temperature": "72", "unit": "fahrenheit", "forecast": ["sunny", "windy"]})

# Make a function call API request
messages = [
    {"role": "system", "content": "You are a helpful weather assistant."},
    {"role": "user", "content": "What's the weather like in Boston?"}
]

response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=tools,
    tool_choice="auto",
)

# Process response
response_message = response.choices[0].message
messages.append({"role": "assistant", "content": response_message.content, "tool_calls": response_message.tool_calls})

if response_message.tool_calls:
    # Process each tool call
    for tool_call in response_message.tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)
        
        if function_name == "get_weather":
            function_response = get_weather(function_args.get("location"))
            
            # Add tool response to messages
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": function_response,
                }
            )
    
    # Get a new response from the model
    second_response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
    )
    
    print(second_response.choices[0].message.content)
else:
    print(response_message.content)
```
</CodeGroup>

## More Examples
<CardGroup cols={2}>
  <Card title="Multi-Tool Orchestration" icon="notebook" href="/v2/examples/openai">
    Advanced multi-tool RAG example
  </Card>
  <Card title="Async OpenAI Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/openai_example_async.ipynb" newTab={true}>
    Demonstrates asynchronous calls with the OpenAI SDK.
  </Card>
  <Card title="Sync OpenAI Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/openai_example_sync.ipynb" newTab={true}>
    Shows synchronous calls with the OpenAI SDK.
  </Card>
  <Card title="Web Search Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/web_search.ipynb" newTab={true}>
    Example of integrating web search capabilities.
  </Card>
</CardGroup>


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/autogen.mdx

---
title: "AutoGen"
description: "Integrate AgentOps with Microsoft AutoGen for multi-agent workflow tracking"
---

[AutoGen](https://microsoft.github.io/autogen/stable/) is Microsoft's framework for building multi-agent conversational AI systems. AgentOps provides seamless integration with AutoGen to track and monitor your multi-agent workflows.

## Installation

<CodeGroup>
```bash pip
pip install agentops autogen-core python-dotenv
```
```bash poetry
poetry add agentops autogen-core python-dotenv
```
```bash uv
uv add agentops autogen-core python-dotenv
```
</CodeGroup>

## Setting Up API Keys

Before using AutoGen with AgentOps, you need to set up your API keys. You can obtain:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
    ```bash Export to CLI
    export OPENAI_API_KEY="your_openai_api_key_here"
    export AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
    ```txt Set in .env file
    OPENAI_API_KEY="your_openai_api_key_here"
    AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

AgentOps automatically instruments AutoGen agents and tracks their interactions. Simply initialize AgentOps before creating your AutoGen agents!

<CodeGroup>
```python Countdown
import asyncio
from dataclasses import dataclass
from typing import Callable
import agentops

from autogen_core import (
    DefaultTopicId, 
    MessageContext, 
    RoutedAgent, 
    default_subscription, 
    message_handler,
    AgentId,
    SingleThreadedAgentRuntime
)

# Initialize AgentOps
agentops.init()

@dataclass
class CountdownMessage:
    """Message containing a number for countdown operations"""
    content: int

@default_subscription
class ModifierAgent(RoutedAgent):
    """Agent that modifies numbers by applying a transformation function"""
    
    def __init__(self, modify_val: Callable[[int], int]) -> None:
        super().__init__("A modifier agent that transforms numbers.")
        self._modify_val = modify_val

    @message_handler
    async def handle_message(self, message: CountdownMessage, ctx: MessageContext) -> None:
        """Handle incoming messages and apply modification"""
        original_val = message.content
        modified_val = self._modify_val(original_val)
        
        print(f"üîß ModifierAgent: Transformed {original_val} ‚Üí {modified_val}")
        
        # Publish the modified value to continue the workflow
        await self.publish_message(
            CountdownMessage(content=modified_val), 
            DefaultTopicId()
        )

@default_subscription  
class CheckerAgent(RoutedAgent):
    """Agent that checks if a condition is met and decides whether to continue"""
    
    def __init__(self, stop_condition: Callable[[int], bool]) -> None:
        super().__init__("A checker agent that validates conditions.")
        self._stop_condition = stop_condition

    @message_handler
    async def handle_message(self, message: CountdownMessage, ctx: MessageContext) -> None:
        """Handle incoming messages and check stopping condition"""
        value = message.content
        
        if not self._stop_condition(value):
            print(f"‚úÖ CheckerAgent: {value} passed validation, continuing workflow")
            # Continue the workflow by publishing the message
            await self.publish_message(
                CountdownMessage(content=value), 
                DefaultTopicId()
            )
        else:
            print(f"üõë CheckerAgent: {value} failed validation, stopping workflow")
            print("üéâ Countdown completed successfully!")

async def run_countdown_workflow():
    """Run a countdown workflow from 10 to 1 using AutoGen agents"""
    
    print("üöÄ Starting AutoGen Countdown Workflow")
    print("=" * 50)
    
    # Create the AutoGen runtime
    runtime = SingleThreadedAgentRuntime()
    
    # Register the modifier agent (subtracts 1 from each number)
    await ModifierAgent.register(
        runtime,
        "modifier",
        lambda: ModifierAgent(modify_val=lambda x: x - 1),
    )
    
    # Register the checker agent (stops when value <= 1)
    await CheckerAgent.register(
        runtime,
        "checker", 
        lambda: CheckerAgent(stop_condition=lambda x: x <= 1),
    )
    
    # Start the runtime
    runtime.start()
    print("ü§ñ AutoGen runtime started")
    print("üì® Sending initial message with value: 10")
    
    # Send initial message to start the countdown
    await runtime.send_message(
        CountdownMessage(10), 
        AgentId("checker", "default")
    )
    
    # Wait for the workflow to complete
    await runtime.stop_when_idle()
    
    print("=" * 50)
    print("‚ú® Workflow completed! Check your AgentOps dashboard for detailed traces.")

# Run the workflow
if __name__ == "__main__":
    asyncio.run(run_countdown_workflow())
```

```python Multi-Agent
import asyncio
from dataclasses import dataclass
from typing import List, Dict, Any
import agentops

from autogen_core import (
    DefaultTopicId,
    MessageContext, 
    RoutedAgent,
    default_subscription,
    message_handler,
    AgentId,
    SingleThreadedAgentRuntime
)

# Initialize AgentOps
agentops.init()

@dataclass
class DataMessage:
    """Message containing data to be processed"""
    data: List[Dict[str, Any]]
    stage: str
    metadata: Dict[str, Any]

@default_subscription
class DataCollectorAgent(RoutedAgent):
    """Agent responsible for collecting and preparing initial data"""
    
    def __init__(self) -> None:
        super().__init__("Data collector agent that gathers initial dataset.")

    @message_handler
    async def handle_message(self, message: DataMessage, ctx: MessageContext) -> None:
        print(f"üìä DataCollector: Collecting data for {message.metadata.get('source', 'unknown')}")
        
        # Simulate data collection
        collected_data = [
            {"id": 1, "value": 100, "category": "A"},
            {"id": 2, "value": 200, "category": "B"}, 
            {"id": 3, "value": 150, "category": "A"},
            {"id": 4, "value": 300, "category": "C"},
        ]
        
        print(f"‚úÖ DataCollector: Collected {len(collected_data)} records")
        
        # Send to processor
        await self.publish_message(
            DataMessage(
                data=collected_data,
                stage="processing",
                metadata={**message.metadata, "collected_count": len(collected_data)}
            ),
            DefaultTopicId()
        )

@default_subscription
class DataProcessorAgent(RoutedAgent):
    """Agent that processes and transforms data"""
    
    def __init__(self) -> None:
        super().__init__("Data processor agent that transforms collected data.")

    @message_handler  
    async def handle_message(self, message: DataMessage, ctx: MessageContext) -> None:
        if message.stage != "processing":
            return
            
        print(f"‚öôÔ∏è DataProcessor: Processing {len(message.data)} records")
        
        # Process data - add calculated fields
        processed_data = []
        for item in message.data:
            processed_item = {
                **item,
                "processed_value": item["value"] * 1.1,  # 10% increase
                "status": "processed"
            }
            processed_data.append(processed_item)
        
        print(f"‚úÖ DataProcessor: Processed {len(processed_data)} records")
        
        # Send to analyzer
        await self.publish_message(
            DataMessage(
                data=processed_data,
                stage="analysis", 
                metadata={**message.metadata, "processed_count": len(processed_data)}
            ),
            DefaultTopicId()
        )

@default_subscription
class DataAnalyzerAgent(RoutedAgent):
    """Agent that analyzes processed data and generates insights"""
    
    def __init__(self) -> None:
        super().__init__("Data analyzer agent that generates insights.")

    @message_handler
    async def handle_message(self, message: DataMessage, ctx: MessageContext) -> None:
        if message.stage != "analysis":
            return
            
        print(f"üß† DataAnalyzer: Analyzing {len(message.data)} records")
        
        # Perform analysis
        total_value = sum(item["processed_value"] for item in message.data)
        avg_value = total_value / len(message.data)
        categories = set(item["category"] for item in message.data)
        
        analysis_results = {
            "total_records": len(message.data),
            "total_value": total_value,
            "average_value": avg_value,
            "unique_categories": len(categories),
            "categories": list(categories)
        }
        
        print(f"üìà DataAnalyzer: Analysis complete")
        print(f"   ‚Ä¢ Total records: {analysis_results['total_records']}")
        print(f"   ‚Ä¢ Average value: {analysis_results['average_value']:.2f}")
        print(f"   ‚Ä¢ Categories: {', '.join(analysis_results['categories'])}")
        
        # Send to reporter
        await self.publish_message(
            DataMessage(
                data=message.data,
                stage="reporting",
                metadata={
                    **message.metadata, 
                    "analysis": analysis_results
                }
            ),
            DefaultTopicId()
        )

@default_subscription
class ReportGeneratorAgent(RoutedAgent):
    """Agent that generates final reports"""
    
    def __init__(self) -> None:
        super().__init__("Report generator agent that creates final output.")

    @message_handler
    async def handle_message(self, message: DataMessage, ctx: MessageContext) -> None:
        if message.stage != "reporting":
            return
            
        print(f"üìù ReportGenerator: Generating final report")
        
        analysis = message.metadata.get("analysis", {})
        
        report = f"""
üéØ DATA PROCESSING REPORT
========================
Source: {message.metadata.get('source', 'Unknown')}
Processing Date: {message.metadata.get('timestamp', 'Unknown')}

üìä SUMMARY STATISTICS:
‚Ä¢ Total Records Processed: {analysis.get('total_records', 0)}
‚Ä¢ Total Value: ${analysis.get('total_value', 0):,.2f}
‚Ä¢ Average Value: ${analysis.get('average_value', 0):,.2f}
‚Ä¢ Unique Categories: {analysis.get('unique_categories', 0)}
‚Ä¢ Categories Found: {', '.join(analysis.get('categories', []))}

‚úÖ Processing pipeline completed successfully!
        """
        
        print(report)
        print("üéâ Multi-agent data processing workflow completed!")

async def run_data_processing_pipeline():
    """Run a complete data processing pipeline using multiple AutoGen agents"""
    
    print("üöÄ Starting AutoGen Data Processing Pipeline")
    print("=" * 60)
    
    # Create runtime
    runtime = SingleThreadedAgentRuntime()
    
    # Register all agents
    await DataCollectorAgent.register(
        runtime,
        "collector",
        lambda: DataCollectorAgent(),
    )
    
    await DataProcessorAgent.register(
        runtime,
        "processor", 
        lambda: DataProcessorAgent(),
    )
    
    await DataAnalyzerAgent.register(
        runtime,
        "analyzer",
        lambda: DataAnalyzerAgent(),
    )
    
    await ReportGeneratorAgent.register(
        runtime,
        "reporter",
        lambda: ReportGeneratorAgent(),
    )
    
    # Start runtime
    runtime.start()
    print("ü§ñ AutoGen runtime with 4 agents started")
    
    # Trigger the pipeline
    initial_message = DataMessage(
        data=[],
        stage="collection",
        metadata={
            "source": "customer_database",
            "timestamp": "2024-01-15T10:30:00Z",
            "pipeline_id": "data_proc_001"
        }
    )
    
    print("üì® Triggering data processing pipeline...")
    await runtime.send_message(
        initial_message,
        AgentId("collector", "default")
    )
    
    # Wait for completion
    await runtime.stop_when_idle()
    
    print("=" * 60)
    print("‚ú® Pipeline completed! Check AgentOps dashboard for detailed agent traces.")

# Run the pipeline
if __name__ == "__main__":
    asyncio.run(run_data_processing_pipeline())
```
</CodeGroup>

## Examples

<CardGroup cols={2}>
  <Card title="Agent Chat Example" icon="notebook" href="/v2/examples/autogen">
    Basic multi-agent chat functionality
  </Card>
  <Card title="Math Agent Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/autogen/MathAgent.ipynb" newTab={true}>
    Demonstrates an agent specialized for mathematical problem-solving.
  </Card>
</CardGroup>

Visit your [AgentOps Dashboard](https://app.agentops.ai) to see detailed traces of your AutoGen agent interactions, performance metrics, and workflow analytics.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/integrations/google_generative_ai.mdx

---
title: Google Generative AI
description: "Monitor and analyze your Google Gemini API calls with AgentOps"
---

AgentOps provides seamless integration with [Google's Generative AI API](https://ai.google.dev/), allowing you to monitor and analyze all your Gemini model interactions automatically.

## Installation

<CodeGroup>
  ```bash pip
  pip install agentops google-genai
  ```
  ```bash poetry
  poetry add agentops google-genai
  ```
  ```bash uv
  uv add agentops google-genai
  ```
</CodeGroup>

## Setting Up API Keys

Before using Google Gemini with AgentOps, you need to set up your API keys. You can obtain:
- **GOOGLE_API_KEY**: From the [Google AI Studio](https://aistudio.google.com/app/apikey)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export GOOGLE_API_KEY="your_google_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
GOOGLE_API_KEY="your_google_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all Gemini API calls.

<CodeGroup>
```python Streaming
import agentops
from google import genai

# Initialize AgentOps
agentops.init()

# Create a client
client = genai.Client(api_key="YOUR_GEMINI_API_KEY")

# Generate streaming content
for chunk in client.models.generate_content_stream(
    model='gemini-2.0-flash-001',
    contents='Explain quantum computing in simple terms.',
):
    print(chunk.text, end="", flush=True)
```

```python Simple Chat
import agentops
from google import genai

# Initialize AgentOps
agentops.init()

# Create a client
client = genai.Client(api_key="YOUR_GEMINI_API_KEY")

# Start a chat session
chat = client.chats.create(model='gemini-2.0-flash-001')

# Send messages and get responses
response = chat.send_message('Hello, how can you help me with AI development?')
print(response.text)

# Continue the conversation
response = chat.send_message('What are the best practices for prompt engineering?')
print(response.text)
```
</CodeGroup>

## Examples

<CardGroup cols={2}>
  <Card title="Gemini Quickstart Notebook" icon="notebook" href="/v2/examples/google_generative_ai">
    Basic Gemini usage with AgentOps
  </Card>
</CardGroup>

For more information on using the Google Gen AI SDK, refer to the [official documentation](https://googleapis.github.io/python-genai/).

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/langchain.mdx

---
title: LangChain
description: "Track your LangChain agents with AgentOps"
---

[LangChain](https://python.langchain.com/docs/tutorials/) is a framework for developing applications powered by language models. AgentOps automatically tracks your LangChain agents by integrating its callback handler.

## Installation

Install AgentOps and the necessary LangChain dependencies:

<CodeGroup>
  ```bash pip
  pip install agentops langchain langchain-community langchain-openai python-dotenv
  ```
  ```bash poetry
  poetry add agentops langchain langchain-community langchain-openai python-dotenv
  ```
  ```bash uv
  uv add agentops langchain langchain-community langchain-openai python-dotenv
  ```
</CodeGroup>

## Setting Up API Keys

You'll need API keys for AgentOps and OpenAI (as `ChatOpenAI` is commonly used with LangChain):
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export OPENAI_API_KEY="your_openai_api_key_here"
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  OPENAI_API_KEY="your_openai_api_key_here"
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>

Then load them in your Python code:
```python
from dotenv import load_dotenv
import os

load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

## Usage

Integrating AgentOps with LangChain involves using the `LangchainCallbackHandler`.
<Tip>
You don't need a separate `agentops.init()` call; the `LangchainCallbackHandler` initializes the AgentOps client automatically if an API key is provided to it or found in the environment.
</Tip>

Here's a basic example:

```python
from langchain_community.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool # Corrected Tool import
from langchain.tools import DuckDuckGoSearchRun # Example tool
from agentops.integration.callbacks.langchain import LangchainCallbackHandler


# 1. Initialize LangchainCallbackHandler
# AGENTOPS_API_KEY can be passed here or loaded from environment
handler = LangchainCallbackHandler(api_key=AGENTOPS_API_KEY, tags=['LangChain Example'])

# 2. Define tools for the agent
search_tool = DuckDuckGoSearchRun()
tools = [
    Tool( # Wrap DuckDuckGoSearchRun in a Tool object
        name="DuckDuckGo Search",
        func=search_tool.run,
        description="Useful for when you need to answer questions about current events or the current state of the world."
    )
]

# 3. Configure LLM with the AgentOps handler
# OPENAI_API_KEY can be passed here or loaded from environment
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
                 callbacks=[handler],
                 model='gpt-3.5-turbo',
                 temperature=0) # Added temperature for reproducibility

# 4. Initialize your agent, passing the handler to callbacks
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    callbacks=[handler], 
    handle_parsing_errors=True
)

# 5. Run your agent
try:
    response = agent.run("Who is the current CEO of OpenAI and what is his most recent public statement?")
    print(response)
except Exception as e:
    print(f"An error occurred: {e}")
```

Visit the [AgentOps Dashboard](https://app.agentops.ai/) to see your session.

## Examples
<CardGroup cols={1}>
  <Card title="LangChain Example" icon="notebook" href="/v2/examples/langchain">
    A detailed notebook demonstrating the LangChain callback handler integration.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/integrations/langgraph.mdx

---
title: LangGraph
description: "Track and analyze your LangGraph workflows with AgentOps"
---

[LangGraph](https://github.com/langchain-ai/langgraph) is a framework for building stateful, multi-step applications with LLMs as graphs. AgentOps automatically instruments LangGraph to provide comprehensive observability into your graph-based agent workflows.

## Core Concepts

LangGraph enables you to build complex agentic workflows as graphs with:
- **Nodes**: Individual steps in your workflow (agents, tools, functions)
- **Edges**: Connections between nodes that define flow
- **State**: Shared data that flows through the graph
- **Conditional Edges**: Dynamic routing based on state or outputs
- **Cycles**: Support for iterative workflows and feedback loops

## Installation

Install AgentOps and LangGraph along with LangChain dependencies:

<CodeGroup>
  ```bash pip
  pip install agentops langgraph langchain-openai python-dotenv
  ```
  ```bash poetry
  poetry add agentops langgraph langchain-openai python-dotenv
  ```
  ```bash uv
  uv add agentops langgraph langchain-openai python-dotenv
  ```
</CodeGroup>

## Setting Up API Keys

You'll need API keys for AgentOps and your LLM provider:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export OPENAI_API_KEY="your_openai_api_key_here"
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  OPENAI_API_KEY="your_openai_api_key_here"
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>

Then load them in your Python code:
```python
from dotenv import load_dotenv
import os

load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all LangGraph operations:

```python
import agentops
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI

# Initialize AgentOps
agentops.init()

# Define your graph state
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

# Create your LLM
model = ChatOpenAI(temperature=0)

# Define nodes
def agent_node(state: AgentState):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}

# Build the graph
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)
workflow.set_entry_point("agent")
workflow.add_edge("agent", END)

# Compile and run
app = workflow.compile()
result = app.invoke({"messages": [{"role": "user", "content": "Hello!"}]})
```

## What Gets Tracked

AgentOps automatically captures:

- **Graph Structure**: Nodes, edges, and entry points during compilation
- **Execution Flow**: The path taken through your graph
- **Node Executions**: Each node execution with inputs and outputs
- **LLM Calls**: All language model interactions within nodes
- **Tool Usage**: Any tools called within your graph
- **State Changes**: How state evolves through the workflow
- **Timing Information**: Duration of each node and total execution time

## Advanced Example

Here's a more complex example with conditional routing and tools:

```python
import agentops
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Initialize AgentOps
agentops.init()

# Define tools
@tool
def search(query: str) -> str:
    """Search for information."""
    return f"Search results for: {query}"

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    try:
        return str(eval(expression))
    except:
        return "Error in calculation"

# Configure model with tools
tools = [search, calculate]
model = ChatOpenAI(temperature=0).bind_tools(tools)

# Define state
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

# Define conditional logic
def should_continue(state: AgentState) -> Literal["tools", "end"]:
    messages = state["messages"]
    last_message = messages[-1]
    
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"

# Define nodes
def call_model(state: AgentState):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}

def call_tools(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_responses = []
    for tool_call in last_message.tool_calls:
        # Execute the appropriate tool
        if tool_call["name"] == "search":
            result = search.invoke(tool_call["args"])
        elif tool_call["name"] == "calculate":
            result = calculate.invoke(tool_call["args"])
        
        tool_responses.append({
            "role": "tool",
            "content": result,
            "tool_call_id": tool_call["id"]
        })
    
    return {"messages": tool_responses}

# Build the graph
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)
workflow.add_edge("tools", "agent")

# Compile and run
app = workflow.compile()
result = app.invoke({
    "messages": [{"role": "user", "content": "Search for AI news and calculate 25*4"}]
})
```

## Dashboard Insights

In your AgentOps dashboard, you'll see:

1. **Graph Visualization**: Visual representation of your compiled graph
2. **Execution Trace**: Step-by-step flow through nodes
3. **Node Metrics**: Performance data for each node
4. **LLM Analytics**: Token usage and costs across all model calls
5. **Tool Usage**: Which tools were called and their results
6. **Error Tracking**: Any failures in node execution

## Examples

<CardGroup cols={2}>
  <Card title="LangGraph Example" icon="notebook" href="/v2/examples/langgraph">
    Complete example showing agent workflows with tools
  </Card>
</CardGroup>

## Best Practices

1. **Initialize Early**: Call `agentops.init()` before creating your graph
2. **Use Descriptive Names**: Name your nodes clearly for better traces
3. **Handle Errors**: Implement error handling in your nodes
4. **Monitor State Size**: Large states can impact performance
5. **Leverage Conditional Edges**: Use them for dynamic workflows

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>

## docs/v2/integrations/google_adk.mdx

---
title: Google ADK
description: "Track and analyze your Google Agent Development Kit (ADK) AI agents with AgentOps"
---

AgentOps provides seamless integration with [Google Agent Development Kit (ADK)](https://google.github.io/adk-docs/), allowing you to track and analyze all your ADK agent interactions automatically.

## Installation

<CodeGroup>
```bash pip
pip install agentops google-adk
```
```bash poetry
poetry add agentops google-adk
```
```bash uv
uv add agentops google-adk
```
</CodeGroup>

## Setting Up API Keys

Before using Google ADK with AgentOps, you need to set up your API keys. You can obtain:
- **GOOGLE_API_KEY**: From the [Google AI Studio](https://aistudio.google.com/app/apikey)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export GOOGLE_API_KEY="your_google_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
GOOGLE_API_KEY="your_google_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all Google ADK agent interactions:

```python
import asyncio
import json
from pydantic import BaseModel, Field

import agentops
from google.adk.agents import LlmAgent
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types

agentops.init()

# --- 1. Define Constants ---
APP_NAME = "agent_comparison_app"
USER_ID = "test_user_456"
SESSION_ID_TOOL_AGENT = "session_tool_agent_xyz"
SESSION_ID_SCHEMA_AGENT = "session_schema_agent_xyz"
MODEL_NAME = "gemini-2.0-flash"

# --- 2. Define Schemas ---

# Input schema used by both agents
class CountryInput(BaseModel):
    country: str = Field(description="The country to get information about.")

# Output schema ONLY for the second agent
class CapitalInfoOutput(BaseModel):
    capital: str = Field(description="The capital city of the country.")
    # Note: Population is illustrative; the LLM will infer or estimate this
    # as it cannot use tools when output_schema is set.
    population_estimate: str = Field(description="An estimated population of the capital city.")

# --- 3. Define the Tool (Only for the first agent) ---
def get_capital_city(country: str) -> str:
    """Retrieves the capital city of a given country."""
    print(f"\n-- Tool Call: get_capital_city(country='{country}') --")
    country_capitals = {
        "united states": "Washington, D.C.",
        "canada": "Ottawa",
        "france": "Paris",
        "japan": "Tokyo",
    }
    result = country_capitals.get(country.lower(), f"Sorry, I couldn't find the capital for {country}.")
    print(f"-- Tool Result: '{result}' --")
    return result

# --- 4. Configure Agents ---

# Agent 1: Uses a tool and output_key
capital_agent_with_tool = LlmAgent(
    model=MODEL_NAME,
    name="capital_agent_tool",
    description="Retrieves the capital city using a specific tool.",
    instruction="""You are a helpful agent that provides the capital city of a country using a tool.
The user will provide the country name in a JSON format like {"country": "country_name"}.
1. Extract the country name.
2. Use the `get_capital_city` tool to find the capital.
3. Respond clearly to the user, stating the capital city found by the tool.
""",
    tools=[get_capital_city],
    input_schema=CountryInput,
    output_key="capital_tool_result", # Store final text response
)

# Agent 2: Uses output_schema (NO tools possible)
structured_info_agent_schema = LlmAgent(
    model=MODEL_NAME,
    name="structured_info_agent_schema",
    description="Provides capital and estimated population in a specific JSON format.",
    instruction=f"""You are an agent that provides country information.
The user will provide the country name in a JSON format like {{"country": "country_name"}}.
Respond ONLY with a JSON object matching this exact schema:
{json.dumps(CapitalInfoOutput.model_json_schema(), indent=2)}
Use your knowledge to determine the capital and estimate the population. Do not use any tools.
""",
    # *** NO tools parameter here - using output_schema prevents tool use ***
    input_schema=CountryInput,
    output_schema=CapitalInfoOutput, # Enforce JSON output structure
    output_key="structured_info_result", # Store final JSON response
)

# --- 5. Set up Session Management and Runners ---
session_service = InMemorySessionService()

# Create a runner for EACH agent
capital_runner = Runner(
    agent=capital_agent_with_tool,
    app_name=APP_NAME,
    session_service=session_service
)
structured_runner = Runner(
    agent=structured_info_agent_schema,
    app_name=APP_NAME,
    session_service=session_service
)

# --- 6. Define Agent Interaction Logic ---
async def call_agent_and_print(
    runner_instance: Runner,
    agent_instance: LlmAgent,
    session_id: str,
    query_json: str
):
    """Sends a query to the specified agent/runner and prints results."""
    print(f"\n>>> Calling Agent: '{agent_instance.name}' | Query: {query_json}")

    user_content = types.Content(role='user', parts=[types.Part(text=query_json)])

    final_response_content = "No final response received."
    async for event in runner_instance.run_async(user_id=USER_ID, session_id=session_id, new_message=user_content):
        # print(f"Event: {event.type}, Author: {event.author}") # Uncomment for detailed logging
        if event.is_final_response() and event.content and event.content.parts:
            # For output_schema, the content is the JSON string itself
            final_response_content = event.content.parts[0].text

    print(f"<<< Agent '{agent_instance.name}' Response: {final_response_content}")

    current_session = await session_service.get_session(app_name=APP_NAME,
                                                         user_id=USER_ID,
                                                         session_id=session_id)
    stored_output = current_session.state.get(agent_instance.output_key)

    # Pretty print if the stored output looks like JSON (likely from output_schema)
    print(f"--- Session State ['{agent_instance.output_key}']: ", end="")
    try:
        # Attempt to parse and pretty print if it's JSON
        parsed_output = json.loads(stored_output)
        print(json.dumps(parsed_output, indent=2))
    except (json.JSONDecodeError, TypeError):
         # Otherwise, print as string
        print(stored_output)
    print("-" * 30)


# --- 7. Run Interactions ---
async def main():
# Create sessions
    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID_TOOL_AGENT)
    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID_SCHEMA_AGENT)
    
    print("--- Testing Agent with Tool ---")
    await call_agent_and_print(capital_runner, capital_agent_with_tool, SESSION_ID_TOOL_AGENT, '{"country": "France"}')

    print("\n\n--- Testing Agent with Output Schema (No Tool Use) ---")
    await call_agent_and_print(structured_runner, structured_info_agent_schema, SESSION_ID_SCHEMA_AGENT, '{"country": "Japan"}')

asyncio.run(main())
```

## Examples

<CardGroup cols={2}>
  <Card title="Human Approval Example" icon="notebook" href="/v2/examples/google_adk" newTab={true}>
    Implement human-in-the-loop approval workflows with Google ADK agents
  </Card>
</CardGroup>

Visit your [AgentOps Dashboard](https://app.agentops.ai) to see detailed traces of your Google ADK agent interactions, tool usage, and session management.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/anthropic.mdx

---
title: Anthropic
description: "Track and analyze your Anthropic API calls with AgentOps"
---

AgentOps provides seamless integration with [Anthropic's Python SDK](https://github.com/anthropics/anthropic-sdk-python), allowing you to track and analyze all your Claude model interactions automatically.

## Installation



<CodeGroup>
  ```bash pip
  pip install agentops anthropic
  ```
  ```bash poetry
  poetry add agentops anthropic
  ```
  ```bash uv
  uv add agentops anthropic
  ```
</CodeGroup>

## Setting Up API Keys

Before using Anthropic with AgentOps, you need to set up your API keys. You can obtain:
- **ANTHROPIC_API_KEY**: From the [Anthropic Console](https://console.anthropic.com/)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
    ```bash Export to CLI
    export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
    export AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
    ```txt Set in .env file
    ANTHROPIC_API_KEY="your_anthropic_api_key_here"
    AGENTOPS_API_KEY="your_agentops_api_key_here"
    ```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["ANTHROPIC_API_KEY"] = os.getenv("ANTHROPIC_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all Anthropic API calls:

```python
import agentops
import anthropic

# Initialize AgentOps
agentops.init()

# Create Anthropic client
client = anthropic.Anthropic()

# Make a completion request - AgentOps will track it automatically
message = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[
        {"role": "user", "content": "What is artificial intelligence?"}
    ]
)

# Print the response received
print(message.content[0].text)
```


## Examples

<CodeGroup>
```python Streaming
import agentops
import anthropic
      
# Initialize AgentOps
agentops.init()

# Create Anthropic client
client = anthropic.Anthropic()

# Make a streaming request
with client.messages.stream(
    model="claude-sonnet-4-20250514",
    messages=[
        {"role": "user", "content": "Write a short poem about artificial intelligence."}
    ]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
    print()
```

```python Tool Use
import agentops
import anthropic
import json
from datetime import datetime

# Initialize AgentOps
agentops.init()

# Create Anthropic client
client = anthropic.Anthropic()

# Define tools
tools = [
    {
        "type": "custom",
        "name": "get_current_time",
        "description": "Get the current date and time",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    }
]

def get_current_time():
    return {"current_time": datetime.now().isoformat()}

# Make a request with tools
  message = client.messages.create(
    model="claude-opus-4-20250514",
    tools=tools,
    messages=[
        {"role": "user", "content": "What time is it now?"}
    ]
  )

# Handle tool use
if message.content[0].type == "tool_calls":
    tool_call = message.content[0].tool_calls[0]
    tool_name = tool_call.name
    
    if tool_name == "get_current_time":
        tool_response = get_current_time()
        
        # Continue the conversation with the tool response
        second_message = client.messages.create(
          model="claude-opus-4-20250514",
            messages=[
                {"role": "user", "content": "What time is it now?"},
                {
                    "role": "assistant",
                    "content": [
                        {
                            "type": "tool_calls",
                            "tool_calls": [
                                {
                                    "type": "custom",
                                    "name": "get_current_time",
                                    "input": {}
                                }
                            ]
                        }
                    ]
                },
                {
                    "role": "tool",
                    "content": json.dumps(tool_response),
                    "tool_call_id": tool_call.id
                }
            ]
      )

        print(second_message.content[0].text)
else:
    print(message.content[0].text)
```
</CodeGroup>

## More Examples

<CardGroup cols={2}>
  <Card title="Understanding Tools" icon="notebook" href="/v2/examples/anthropic">
    Claude integration with tool usage and advanced features
  </Card>
  <Card title="Sync Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/anthropic/anthropic-example-sync.ipynb" newTab={true}>
    Shows synchronous calls with the Anthropic SDK.
  </Card>
  <Card title="Async Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/anthropic/anthropic-example-async.ipynb" newTab={true}>
    Demonstrates asynchronous calls with the Anthropic SDK.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/openai_agents_python.mdx

---
title: OpenAI Agents SDK
description: 'AgentOps and OpenAI Agents SDK integration for powerful multi-agent workflow monitoring.'
---

[OpenAI Agents Python](https://github.com/openai/openai-agents-python) is a lightweight yet powerful SDK for building multi-agent workflows in Python. AgentOps seamlessly integrates to provide observability into these workflows.

- [OpenAI Agents Python documentation](https://openai.github.io/openai-agents-python/)
- [TypeScript guide](/v2/integrations/openai_agents_js)

## Core Concepts

- **Agents**: LLMs configured with instructions, tools, guardrails, and handoffs
- **Handoffs**: Allow agents to transfer control to other agents for specific tasks
- **Guardrails**: Configurable safety checks for input and output validation
- **Tracing**: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows

## Python

### Installation

Install AgentOps, the OpenAI Agents SDK, and `python-dotenv` for managing API keys:
<CodeGroup>
  ```bash pip 
  pip install agentops openai-agents python-dotenv
  ```
  ```bash poetry
  poetry add agentops openai-agents python-dotenv
  ```
  ```bash uv
  uv add agentops openai-agents python-dotenv
  ```
</CodeGroup>

### Setting Up API Keys

Before using the OpenAI Agents SDK with AgentOps, you need to set up your API keys:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

You can set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export OPENAI_API_KEY="your_openai_api_key_here"
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  OPENAI_API_KEY="your_openai_api_key_here"
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>

Then load them in your Python code:
```python
from dotenv import load_dotenv
import os

load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

### Usage

AgentOps will automatically instrument the OpenAI Agents SDK after being initialized. You can then create agents, run them, and track their interactions.

```python
import agentops
from agents import Agent, Runner

# Initialize AgentOps
agentops.init()

# Create an agent with instructions
agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)
```

## Examples

<CodeGroup>
```python Handoffs
from agents import Agent, Runner
import asyncio
import agentops
import os

agentops.init()

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, ¬øc√≥mo est√°s?")
    print(result.final_output)
    # Expected Output: ¬°Hola! Estoy bien, gracias por preguntar. ¬øY t√∫, c√≥mo est√°s?


if __name__ == "__main__":
    asyncio.run(main())
```

```python Function Calling
import asyncio
from agents import Agent, Runner, function_tool
import agentops
import os

agentops.init()

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Weather Agent", 
    instructions="You are a helpful agent that can get weather information.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # Expected Output: The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
```
</CodeGroup>

## More Examples
<CardGroup cols={2}>
  <Card title="Customer Service Agent" icon="notebook" href="/v2/examples/openai_agents">
    Demonstrates a customer service workflow
  </Card>
  <Card title="Agent Patterns" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_agents/agent_patterns.ipynb" newTab={true}>
    Illustrates various agent interaction patterns.
  </Card>
  <Card title="Agents with Tools" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_agents/agents_tools.ipynb" newTab={true}>
    Showcases agents utilizing different tools.
  </Card>
</CardGroup>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/integrations/crewai.mdx

---
title: 'CrewAI'
description: 'AgentOps and CrewAI teamed up to make monitoring Crew agents dead simple.'
---

[CrewAI](https://www.crewai.com/) is a framework for easily building multi-agent applications. AgentOps integrates with CrewAI to provide observability into your agent workflows. Crew has comprehensive [documentation](https://docs.crewai.com) available as well as a great [quickstart](https://docs.crewai.com/how-to/Creating-a-Crew-and-kick-it-off/) guide.

## Installation
Install AgentOps and CrewAI, along with `python-dotenv` for managing API keys:

<CodeGroup>
  ```bash pip
  pip install agentops crewai python-dotenv
  ```
  ```bash poetry
  poetry add agentops crewai python-dotenv
  ```
  ```bash uv
  uv add agentops crewai python-dotenv
  ```
</CodeGroup>

## Setting Up API Keys

You'll need API keys for AgentOps and OpenAI (since CrewAI's built-in `LLM` uses OpenAI models by default):
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Set these as environment variables or in a `.env` file.

<CodeGroup>
  ```bash Export to CLI
  export OPENAI_API_KEY="your_openai_api_key_here"
  export AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
  ```txt Set in .env file
  OPENAI_API_KEY="your_openai_api_key_here"
  AGENTOPS_API_KEY="your_agentops_api_key_here"
  ```
</CodeGroup>

Then load them in your Python code:
```python
from dotenv import load_dotenv
import os

load_dotenv()

AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

## Usage

Simply initialize AgentOps at the beginning of your CrewAI application. AgentOps automatically instruments CrewAI components‚Äîincluding its `LLM`‚Äîto track your agent interactions.

Here's how to set up a basic CrewAI application with AgentOps:

```python
import agentops
from crewai import Agent, Task, Crew, LLM

# Initialize AgentOps client
agentops.init()

# Define the LLM to use with CrewAI
llm = LLM(
    model="openai/gpt-4o",  # Or your preferred model
    temperature=0.7,
)

# Create an agent
researcher = Agent(
    role='Researcher',
    goal='Research and provide accurate information about cities and their history',
    backstory='You are an expert researcher with vast knowledge of world geography and history.',
    llm=llm,
    verbose=True
)

# Create a task
research_task = Task(
    description='What is the capital of France? Provide a detailed answer about its history, culture, and significance.',
    expected_output='A comprehensive response about Paris, including its status as the capital of France, historical significance, cultural importance, and key landmarks.',
    agent=researcher
)

# Create a crew with the researcher
crew = Crew(
    agents=[researcher],
    tasks=[research_task],
    verbose=True
)

# Execute the task
result = crew.kickoff()

print("\nCrew Research Results:")
print(result)
```

## Examples

<CardGroup cols={2}>
  <Card title="Job Posting" icon="notebook" href="/v2/examples/crewai" newTab={true}>
    Create job postings with a crew of specialized agents
  </Card>
  <Card title="Markdown Validator" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/crewai/markdown_validator.ipynb" newTab={true}>
    Validate and improve markdown content using CrewAI agents
  </Card>
</CardGroup>


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/integrations/ionet.mdx

---
title: IO Intelligence
description: "Track and analyze your IO Intelligence API calls with AgentOps"
---

AgentOps seamlessly integrates with IO Intelligence's OpenAI‚Äëcompatible API, allowing you to track and analyze every request without changing your workflow.

## Installation



<CodeGroup>
```bash pip
pip install openai
```
</CodeGroup>

## Basic Usage

Initialize AgentOps at the beginning of your application. Then create an **OpenAI** client that points at IO Intelligence's endpoint:

<CodeGroup>
```python Basic Usage
import agentops
from openai import OpenAI

# Initialise AgentOps (tracks every request automatically)
agentops.init("<YOUR_AGENTOPS_API_KEY>")

# Create IO Intelligence client (just add base_url)
client = OpenAI(
    api_key="<YOUR_IO_INTELLIGENCE_API_KEY>",
    base_url="https://api.intelligence.io.solutions/api/v1/"
)

response = client.chat.completions.create(
    model="meta-llama/Llama-3.3-70B-Instruct",
    messages=[{"role": "user", "content": "Say this is a test!"}]
)

print(response.choices[0].message.content)
```
</CodeGroup>

## Model Limits & Daily Quotas

Below are the free daily limits for each model (tokens are counted across input *and* output):

| LLM Model Name | Daily Chat quote | Daily API quote | Daily Embeddings quote | Context Length |
| --- | --- | --- | --- | --- |
| deepseek-ai/DeepSeek-R1 | 1,000,000 tk | 500,000 tk | N/A | 128,
| deepseek-ai/DeepSeek-R1-Distill-Llama-70B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| meta-llama/Llama-3.3-70B-Instruct | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| Qwen/QwQ-32B-Preview | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| databricks/dbrx-instruct | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| deepseek-ai/DeepSeek-R1-Distill-Llama-8B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-14B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-7B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| microsoft/phi-4 | 1,000,000 tk | 500,000 tk | N/A | 16,000 tk |
| mistralai/Mistral-Large-Instruct-2411 | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| neuralmagic/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| google/gemma-2-9b-it | 1,000,000 tk | 500,000 tk | N/A | 8,000 tk |
| nvidia/AceMath-7B-Instruct | 1,000,000 tk | 500,000 tk | N/A | 4,000 tk |
| CohereForAI/aya-expanse-32b | 1,000,000 tk | 500,000 tk | N/A | 8,000 tk |
| Qwen/Qwen2.5-Coder-32B-Instruct | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| THUDM/glm-4-9b-chat | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| CohereForAI/c4ai-command-r-plus-08-2024 | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| tiiuae/Falcon3-10B-Instruct | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| NovaSky-AI/Sky-T1-32B-Preview | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| bespokelabs/Bespoke-Stratos-32B | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| netease-youdao/Confucius-o1-14B | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| Qwen/Qwen2.5-1.5B-Instruct | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| mistralai/Ministral-8B-Instruct-2410 | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| openbmb/MiniCPM3-4B | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| jinaai/ReaderLM-v2 | 1,000,000 tk | 500,000 tk | N/A | 512,000 tk |
| ibm-granite/granite-3.1-8b-instruct | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| microsoft/Phi-3.5-mini-instruct | 1,000,000 tk | 500,000 tk | N/A | 128,000 tk |
| ozone-ai/0x-lite | 1,000,000 tk | 500,000 tk | N/A | 32,000 tk |
| mixedbread-ai/mxbai-embed-large-v1 | N/A | N/A | 500,000 tk | 512 tk |
| meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 | 1,000,000 tk | 500,000 tk | N/A | 430,000 tk |

## Quick Test (cURL)

<CodeGroup>
```bash cURL
curl https://api.intelligence.io.solutions/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <YOUR_IO_INTELLIGENCE_API_KEY>" \
  -d '{
    "model": "meta-llama/Llama-3.3-70B-Instruct",
    "messages": [{"role": "user", "content": "Say this is a test!"}],
    "temperature": 0.7
  }'
```
</CodeGroup>

## Streaming Completions

AgentOps also captures streaming responses:

<CodeGroup>
```python Streaming Example
import agentops
from openai import OpenAI

agentops.init("<YOUR_AGENTOPS_API_KEY>")

client = OpenAI(
    api_key="<YOUR_IO_INTELLIGENCE_API_KEY>",
    base_url="https://api.intelligence.io.solutions/api/v1/"
)

stream = client.chat.completions.create(
    model="meta-llama/Llama-3.3-70B-Instruct",
    messages=[{"role": "user", "content": "Stream please"}],
    stream=True
)
for chunk in stream:
    print(chunk.choices[0].delta.content, end="", flush=True)
```
</CodeGroup>

## Advanced: Tool Calls

All OpenAI‚Äëstyle tool calls work the same way‚ÄîAgentOps will record parameters and results automatically.

<CodeGroup>
```python Tool Calls
import agentops, json
from openai import OpenAI

agentops.init("<YOUR_AGENTOPS_API_KEY>")
client = OpenAI(
    api_key="<YOUR_IO_INTELLIGENCE_API_KEY>",
    base_url="https://api.intelligence.io.solutions/api/v1/"
)

def get_weather(location):
    return f"The weather in {location} is sunny."

messages = [
    {"role": "system", "content": "You are a weather bot."},
    {"role": "user", "content": "What's the weather like in Boston?"}
]

response = client.chat.completions.create(
    model="meta-llama/Llama-3.3-70B-Instruct",
    messages=messages,
    tools=[{
        "type":"function",
        "function":{
            "name":"get_weather",
            "description":"Get weather for a city",
            "parameters":{
                "type":"object",
                "properties":{
                    "location":{"type":"string"}
                },
                "required":["location"]
            }
        }
    }]
)

tool_calls = response.choices[0].message.tool_calls
for call in tool_calls:
    if call.function.name == "get_weather":
        function_response = get_weather(json.loads(call.function.arguments)["location"])
        messages.append({"role":"tool","tool_call_id":call.id,"content":function_response})

follow_up = client.chat.completions.create(
    model="meta-llama/Llama-3.3-70B-Instruct",
    messages=messages
)
print(follow_up.choices[0].message.content)
```
</CodeGroup>


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>


## docs/v2/examples/ag2.mdx

---
title: 'AG2 with Mem0 Example'
description: 'Observe an AG2 Agent with memory powered by Mem0 using AgentOps'
---
{/*  SOURCE_FILE: examples/ag2/agentchat_with_memory.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/ag2/agentchat_with_memory.ipynb'} target={'_blank'}>Github</a>_

# Observe an Agent with memory powered by Mem0

This notebook demonstrates an intelligent customer service chatbot system that combines:

- AG2 for conversational agents
- Mem0 for memory management

[Mem0](https://www.mem0.ai/) provides a smart, self-improving memory layer for Large Language Models (LLMs), enabling developers to create personalized AI experiences that evolve with each user interaction. Refer [docs](https://docs.mem0.ai/overview) for more information.

The implementation showcases how to initialize agents, manage conversation memory, and facilitate multi-agent conversations for enhanced problem-solving in customer support scenarios.

With AgentOps, you can observe the agent's memory and interactions in real-time, providing insights into how the agent learns and adapts over time.

## Pre-requisites
- AgentOps API key from [AgentOps](https://app.agentops.ai/).
- Mem0 API key from [Mem0 Platform](https://app.mem0.ai/).
- OpenAI API key from [OpenAI](https://platform.openai.com/).

## Installation

Install required dependencies:
<CodeGroup>
  ```bash pip
  pip install agentops "ag2[openai]" mem0ai python-dotenv
  ```
  ```bash poetry
  poetry add agentops ag2 mem0ai python-dotenv
  # Note: For ag2[openai] with poetry, you might need to specify openai as an extra or directly.
  # poetry add ag2 -E openai
  ```
  ```bash uv
  uv add agentops "ag2[openai]" mem0ai python-dotenv
  ```
</CodeGroup>

## Setup

```python
import os
from dotenv import load_dotenv
import agentops
from mem0 import MemoryClient
from autogen import ConversableAgent

load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
os.environ["MEM0_API_KEY"] = os.getenv("MEM0_API_KEY", "your_mem0_api_key_here")
```

## Initialize Agent and Memory

The conversational agent is set up using the 'gpt-4o' model and a mem0 client. We'll utilize the client's methods for storing and accessing memories.

```python
agentops.init(auto_start_session=False)
tracer = agentops.start_trace(trace_name="AG2 Agent using Mem0", tags=["ag2-mem0-example", "agentops-example"])

agent = ConversableAgent(
    "chatbot",
    llm_config={"config_list": [{"model": "gpt-4o", "api_key": os.environ.get("OPENAI_API_KEY")}]},
    code_execution_config=False,
    function_map=None,
    human_input_mode="NEVER",
)

memory = MemoryClient()
```

Initialize a conversation history for a Best Buy customer service chatbot. It contains a list of message exchanges between the user and the assistant, structured as dictionaries with 'role' and 'content' keys. The entire conversation is then stored in memory using the `memory.add()` method, associated with the identifier "customer_service_bot".

```python
conversation = [
    {
        "role": "assistant",
        "content": "Hi, I'm Best Buy's chatbot!\n\nThanks for being a My Best Buy TotalTM member.\n\nWhat can I help you with?",
    },
    {
        "role": "user",
        "content": 'Seeing horizontal lines on our tv. TV model: Sony - 77" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV',
    },
    {
        "role": "assistant",
        "content": "Thanks for being a My Best Buy Total‚Ñ¢ member. I can connect you to an expert immediately - just one perk of your membership!\n\nSelect the button below when you're ready to chat.",
    },
    {
        "role": "assistant",
        "content": "Good evening, thank you for choosing Best Buy, Fnu. My name is Lovely. I hope you are doing well. I'm sorry to hear that you're seeing horizontal lines on your TV.\n\nI'm absolutely committed to exploring all possible ways to assist you to fix this issue.\n\nTo ensure that we are on the right account, may I please have your email address registered with your Best Buy account?",
    },
    {"role": "user", "content": "dd@gmail.com"},
    {
        "role": "assistant",
        "content": "Perfect! Thank you for providing all the details, surely you have made my job easier by doing this. I really appreciate it.\n\nI also want to take a moment to express our heartfelt appreciation for your trust and loyalty. Thank you for being an amazing customer of BestBuy Total.\n\nCould you please help me with the order number or product's details to check it quickly?\n\nSamsung - 49\\" Odyssey OLED G9 (G95SC) DQHD 240Hz 0.03ms G-Sync Compatible Curved Smart Gaming Monitor - Silver - just to confirm this is the item, right?",
    },
    {"role": "user", "content": "Order number: 112217629"},
    {
        "role": "assistant",
        "content": "Superb! Thank you for confirmation.\n\nThank you for your patience. After exploring all possible solutions, I can help you to arrange a home repair appointment for your device. Our Geek Squad experts will visit your home to inspect and fix your device.\n\nIt's great that you have a protection plan - rest assured, we've got your back! As a valued Total member, you can avail this service at a minimal service fee. This fee, applicable to all repairs, covers the cost of diagnosing the issue and any small parts needed for the repair. It's part of our 24-month free protection plan.\n\nPlease click here to review the service fee and plan coverage details -\n\nhttps://www.bestbuy.com/site/best-buy-membership/best-buy-protection/pcmcat1608643232014.c?id=pcmcat1608643232014#jl-servicefees\n\nFnu - just to confirm shall I proceed to schedule the appointment?",
    },
    {"role": "user", "content": "Yes please"},
    {"role": "assistant", "content": "When should I schedule the appointment?"},
    {"role": "user", "content": "Schedule it for tomorrow please"},
]

memory.add(messages=conversation, user_id="customer_service_bot")
```

## Agent Inference

We ask a question to the agent, utilizing mem0 to retrieve relevant memories. The agent then formulates a response based on both the question and the retrieved contextual information.

```python
data = "I forgot the order number, can you quickly tell me?"

relevant_memories = memory.search(data, user_id="customer_service_bot")
flatten_relevant_memories = "\n".join([m["memory"] for m in relevant_memories])

prompt = f\"\"\"Answer the user question considering the memories. Keep answers clear and concise.
Memories:
{flatten_relevant_memories}
\n\n
Question: {data}
\"\"\"

reply = agent.generate_reply(messages=[{"content": prompt, "role": "user"}])
print(reply)
```

## Multi Agent Conversation

Initialize two AI agents: a "manager" for resolving customer issues and a "customer_bot" for gathering information on customer problems, both using GPT-4. It then retrieves relevant memories for a given question, combining them with the question into a prompt. This prompt can be used by either the manager or customer_bot to generate a contextually informed response.

```python
manager = ConversableAgent(
    "manager",
    system_message="You are a manager who helps in resolving customer issues.",
    llm_config={"config_list": [{"model": "gpt-4o-mini", "api_key": os.environ.get("OPENAI_API_KEY")}]},
    human_input_mode="NEVER",
)

customer_bot = ConversableAgent(
    "customer_bot",
    system_message="You are a customer service bot who gathers information on issues customers are facing. Keep answers clear and concise.",
    llm_config={"config_list": [{"model": "gpt-4", "api_key": os.environ.get("OPENAI_API_KEY")}]},
    human_input_mode="NEVER",
)

data = "When is the appointment?"

relevant_memories = memory.search(data, user_id="customer_service_bot")
flatten_relevant_memories = "\n".join([m["memory"] for m in relevant_memories])

prompt = f\"\"\"
Context:
{flatten_relevant_memories}
\n\n
Question: {data}
\"\"\"

result = manager.send(prompt, customer_bot, request_reply=True)

agentops.end_trace(tracer, end_state="Success")
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/examples/mem0.mdx

---
title: 'Mem0'
description: 'Memory Operations with Mem0'
---
{/*  SOURCE_FILE: examples/mem0/mem0_memory_example.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/mem0/mem0_memory_example.ipynb'} target={'_blank'}>Github</a>_

# Memory Operations with Mem0

This example demonstrates how to use Mem0's memory management capabilities with both synchronous and asynchronous operations to store, search, and manage conversational context and user preferences.

## Overview

This example showcases practical memory management operations where we:

1. **Initialize Mem0 Memory instances** for both sync and async operations
2. **Store conversation history** and user preferences with metadata
3. **Search memories** using natural language queries
4. **Compare performance** between synchronous and asynchronous memory operations

By using async operations, you can perform multiple memory operations simultaneously instead of waiting for each one to complete sequentially. This is particularly beneficial when dealing with multiple memory additions or searches.




## Setup and Imports
## Installation
<CodeGroup>
  ```bash pip
  pip install agentops mem0ai python-dotenv
  ```
  ```bash poetry
  poetry add agentops mem0ai python-dotenv
  ```
  ```bash uv
  uv add agentops mem0ai python-dotenv
  ```
</CodeGroup>


Import the required libraries for local memory management with Mem0. We'll use both Memory and AsyncMemory classes to demonstrate different execution patterns for memory operations.



```python

from mem0 import Memory, AsyncMemory
import os
import asyncio
import logging
from dotenv import load_dotenv
import agentops
```

## Environment Configuration

Set up environment variables for API keys. These are essential for authenticating with AgentOps for tracing and OpenAI for the language model used in memory operations.



```python
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
```

## Configuration and Sample Data

Set up the configuration for local memory storage and define sample user data. The configuration specifies the LLM provider and model settings for processing memories.



```python
local_config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "max_tokens": 2000,
        },
    }
}
user_id = "alice_demo"
agent_id = "assistant_demo"
run_id = "session_001"

sample_messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller? They can be quite engaging."},
    {"role": "user", "content": "I'm not a big fan of thriller movies but I love sci-fi movies."},
    {
        "role": "assistant",
        "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.",
    },
]

sample_preferences = [
    "I prefer dark roast coffee over light roast",
    "I exercise every morning at 6 AM",
    "I'm vegetarian and avoid all meat products",
    "I love reading science fiction novels",
    "I work in software engineering",
]
```

## Synchronous Memory Operations

This function demonstrates sequential memory operations using the synchronous Memory class. While straightforward to implement, each operation must complete before the next begins, which can impact performance.



```python
def demonstrate_sync_memory(local_config, sample_messages, sample_preferences, user_id):
    """
    Demonstrate synchronous Memory class operations.
    """

    agentops.start_trace("mem0_memory_example", tags=["mem0_memory_example"])
    try:
        
        memory = Memory.from_config(local_config)

        result = memory.add(
            sample_messages, user_id=user_id, metadata={"category": "movie_preferences", "session": "demo"}
        )

        for i, preference in enumerate(sample_preferences):
            result = memory.add(preference, user_id=user_id, metadata={"type": "preference", "index": i})
       
        search_queries = [
            "What movies does the user like?",
            "What are the user's food preferences?",
            "When does the user exercise?",
        ]

        for query in search_queries:
            results = memory.search(query, user_id=user_id)
        
            if results and "results" in results:
                for j, result in enumerate(results): 
                    print(f"Result {j+1}: {result.get('memory', 'N/A')}")
            else:
                print("No results found")

        all_memories = memory.get_all(user_id=user_id)
        if all_memories and "results" in all_memories:
            print(f"Total memories: {len(all_memories['results'])}")

        delete_all_result = memory.delete_all(user_id=user_id)
        print(f"Delete all result: {delete_all_result}")

        agentops.end_trace(end_state="success")
    except Exception as e:
        agentops.end_trace(end_state="error")
```

## Asynchronous Memory Operations

This function showcases concurrent memory operations using AsyncMemory. By leveraging asyncio.gather(), multiple operations execute simultaneously, significantly reducing total execution time for I/O-bound tasks.



```python
async def demonstrate_async_memory(local_config, sample_messages, sample_preferences, user_id):
    """
    Demonstrate asynchronous Memory class operations with concurrent execution.
    """

    agentops.start_trace("mem0_memory_async_example", tags=["mem0_memory_async_example"])
    try:

        async_memory = await AsyncMemory.from_config(local_config)

        result = await async_memory.add(
            sample_messages, user_id=user_id, metadata={"category": "async_movie_preferences", "session": "async_demo"}
        )

        async def add_preference(preference, index):
            """Helper function to add a single preference asynchronously."""
            return await async_memory.add(
                preference, user_id=user_id, metadata={"type": "async_preference", "index": index}
            )

        tasks = [add_preference(pref, i) for i, pref in enumerate(sample_preferences)]
        results = await asyncio.gather(*tasks)
        for i, result in enumerate(results):
            print(f"Added async preference {i+1}: {result}")

        search_queries = [
            "What movies does the user like?",
            "What are the user's dietary restrictions?",
            "What does the user do for work?",
        ]

        async def search_memory(query):
            """Helper function to perform async memory search."""
            return await async_memory.search(query, user_id=user_id), query

        search_tasks = [search_memory(query) for query in search_queries]
        search_results = await asyncio.gather(*search_tasks)

        for result, query in search_results:
            if result and "results" in result:
                for j, res in enumerate(result["results"]):
                    print(f"Result {j+1}: {res.get('memory', 'N/A')}")
            else:
                print("No results found")

        all_memories = await async_memory.get_all(user_id=user_id)
        if all_memories and "results" in all_memories:
            print(f"Total async memories: {len(all_memories['results'])}")

        delete_all_result = await async_memory.delete_all(user_id=user_id)
        print(f"Delete all result: {delete_all_result}")

        agentops.end_trace(end_state="success")

    except Exception as e:
        agentops.end_trace(end_state="error")
```

## Execute Demonstrations

Run both synchronous and asynchronous demonstrations to compare their execution patterns and performance. The async version demonstrates the benefits of concurrent execution for multiple memory operations.



```python
# Execute both sync and async demonstrations
demonstrate_sync_memory(local_config, sample_messages, sample_preferences, user_id)
await demonstrate_async_memory(local_config, sample_messages, sample_preferences, user_id)
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/examples.mdx

---
title: 'Examples'
description: 'Examples of AgentOps with various integrations'
---

## Explore our examples to see AgentOps in action!

### LLM Integrations

<CardGroup cols={3}>

  <Card title="Anthropic" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/anthropic/anthropic_icon_slate.png?raw=true" alt="Anthropic" />} iconType="image" href="/v2/examples/anthropic">
    Claude integration with tool usage and advanced features
  </Card>

  <Card title="Google Generative AI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/deepmind/gemini-logo.png?raw=true" alt="Gemini" />} iconType="image" href="/v2/examples/google_genai">
    Google Gemini models and their examples
  </Card>

  <Card title="LiteLLM" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/litellm/litellm.png?raw=true" alt="LiteLLM" />} iconType="image" href="/v2/examples/litellm">
    Unified LLM interface monitoring example
  </Card>

  <Card title="OpenAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI" />} iconType="image" href="/v2/examples/openai">
    Advanced multi-tool orchestration with GPT models
  </Card>

  <Card title="Watsonx" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ibm/ibm-logo.svg?raw=true" alt="IBM" />} iconType="image" href="/v2/examples/watsonx">
    Watsonx text chat integration example
  </Card>

  <Card title="x.AI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/xai/xai-logo.png?raw=true" alt="x.AI" />} iconType="image" href="/v2/examples/xai">
    Grok LLM basic usage patterns
  </Card>

</CardGroup>


### Agent Integrations

<CardGroup>

  <Card title="AG2" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/ag2/ag2-logo.svg?raw=true" alt="AG2" />} iconType="image" href="/v2/examples/ag2">
    Multi-agent conversations with memory capabilities
  </Card>

  <Card title="Agno" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/agno/agno.png?raw=true" alt="Agno" />} iconType="image" href="/v2/examples/agno">
    Modern AI agent framework with teams, workflows, and tool integration
  </Card>

  <Card title="Autogen" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/autogen/autogen-logo.svg?raw=true" alt="Autogen" />} iconType="image" href="/v2/examples/autogen">
    AG2 multi-agent workflow demonstration
  </Card>

  <Card title="CrewAI" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/crewai/crewai-logo.png?raw=true" alt="CrewAI" />} iconType="image" href="/v2/examples/crewai">
    CrewAI multi-agent framework example
  </Card>

  <Card title="Google ADK" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/google-adk/google-adk-logo.png?raw=true" alt="Google ADK" />} iconType="image" href="/v2/examples/google_adk">
    Google Agent Development Kit integration
  </Card>

  <Card title="OpenAI Agents SDK" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/openai/openai-logomark.png?raw=true" alt="OpenAI Agents SDK" />} iconType="image" href="/v2/examples/openai_agents">
    OpenAI Agents SDK workflow walkthrough
  </Card>

  <Card title="LangChain" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/langchain/langchain-logo.svg?raw=true" alt="LangChain" />} iconType="image" href="/v2/examples/langchain">
    LangChain callback handler integration
  </Card>

  <Card title="Mem0" icon={<img src="https://www.github.com/agentops-ai/agentops/blob/main/docs/images/external/mem0/mem0.png?raw=true" alt="Mem0" />} iconType="image" href="/v2/examples/mem0">
    Comprehensive memory operations with Mem0ai 
  </Card>

  <Card title="smolagents" icon={<img src="https://github.com/AgentOps-AI/agentops/blob/main/docs/images/external/huggingface/hf-logo.png?raw=true" alt="smolagents" />} iconType="image" href="/v2/integrations/smolagents">
    Track HuggingFace's smolagents with AgentOps seamlessly
  </Card>
</CardGroup>


<script type="module" src="/scripts/github_stars.js" />
<script type="module" src="/scripts/scroll-img-fadein-animation.js" />
<script type="module" src="/scripts/button_heartbeat_animation.js" />
<script type="module" src="/scripts/adjust_api_dynamically.js" />

## docs/v2/examples/agno.mdx

---
title: 'Agno'
description: 'Async Operations with Agno'
---
{/*  SOURCE_FILE: examples/agno/agno_async_operations.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/agno/agno_async_operations.ipynb'} target={'_blank'}>Github</a>_

# Async Operations with Agno

This notebook demonstrates how to leverage asynchronous programming with Agno agents to execute multiple AI tasks concurrently, significantly improving performance and efficiency.

## Overview
This notebook demonstrates a practical example of concurrent AI operations where we:

1. **Initialize an Agno agent** with OpenAI's GPT-4o-mini model
2. **Create multiple async tasks** that query the AI about different programming languages
3. **Compare performance** between concurrent and sequential execution

By using async operations, you can run multiple AI queries simultaneously instead of waiting for each one to complete sequentially. This is particularly beneficial when dealing with I/O-bound operations like API calls to AI models.





## Installation
<CodeGroup>
  ```bash pip
  pip install agentops agno python-dotenv
  ```
  ```bash poetry
  poetry add agentops agno python-dotenv
  ```
  ```bash uv
  uv add agentops agno python-dotenv
  ```
</CodeGroup>

```python
import os
import asyncio
from dotenv import load_dotenv

import agentops
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat
```


```python
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")
```


```python
agentops.init(auto_start_session=False, tags=["agno-example", "async-operation"])
```


```python
async def demonstrate_async_operations():
    """
    Demonstrate concurrent execution of multiple AI agent tasks.
    
    This function creates multiple async tasks that execute concurrently rather than sequentially.
    Each task makes an independent API call to the AI model, and asyncio.gather() 
    waits for all tasks to complete before returning results.
    
    Performance benefit: Instead of 3 sequential calls taking ~90 seconds total,
    concurrent execution typically completes in ~30 seconds.
    """
    tracer = agentops.start_trace(trace_name="Agno Async Operations Example",)

    try:
        # Initialize AI agent with specified model
        agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))
        
        async def task1():
            """Query AI about Python programming language."""
            response = await agent.arun("Explain Python programming language in one paragraph")
            return f"Python: {response.content}"

        async def task2():
            """Query AI about JavaScript programming language."""
            response = await agent.arun("Explain JavaScript programming language in one paragraph")
            return f"JavaScript: {response.content}"

        async def task3():
            """Query AI for comparison between programming languages."""
            response = await agent.arun("Compare Python and JavaScript briefly")
            return f"Comparison: {response.content}"

        # Execute all tasks concurrently using asyncio.gather()
        results = await asyncio.gather(task1(), task2(), task3())
        
        for i, result in enumerate(results, 1):
            print(f"\nTask {i} Result:")
            print(result)
            print("-" * 50)

        agentops.end_trace(tracer, end_state="Success")

    except Exception as e:
        print(f"An error occurred: {e}")
        agentops.end_trace(tracer, end_state="Error")
```


```python
await demonstrate_async_operations()
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/llamaindex_examples.mdx

---
title: 'Llamaindex Examples Example'
description: 'LlamaIndex AgentOps Integration Example'
---
{/*  SOURCE_FILE: examples/llamaindex_examples/llamaindex_example.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/llamaindex_examples/llamaindex_example.ipynb'} target={'_blank'}>Github</a>_

# LlamaIndex AgentOps Integration Example

This notebook demonstrates how to use AgentOps with LlamaIndex for observability and monitoring of your context-augmented generative AI applications.

## Setup

First, install the required packages:


```
# Install required packages
!pip install agentops llama-index-instrumentation-agentops llama-index-embeddings-huggingface llama-index-llms-huggingface python-dotenv
```

## Initialize AgentOps Handler

Set up the AgentOps handler for LlamaIndex instrumentation:


```
import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.instrumentation.agentops import AgentOpsHandler

# Initialize AgentOps handler
handler = AgentOpsHandler()
handler.init()

# Load environment variables
load_dotenv()

# Set API keys (replace with your actual keys)
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
```

## Configure Local Models (Optional)

For this example, we'll use local HuggingFace models to avoid requiring external API keys:


```
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM

# Configure local embeddings and LLM
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.llm = HuggingFaceLLM(model_name="microsoft/DialoGPT-medium")
print("Using local HuggingFace embeddings and LLM")
```

## Create Sample Documents and Index

Create some sample documents and build a vector index:


```
print("üöÄ Starting LlamaIndex AgentOps Integration Example")
print("=" * 50)

# Create sample documents
documents = [
    Document(text="LlamaIndex is a framework for building context-augmented generative AI applications with LLMs."),
    Document(text="AgentOps provides observability into your AI applications, tracking LLM calls, performance metrics, and more."),
    Document(text="The integration between LlamaIndex and AgentOps allows you to monitor your RAG applications seamlessly."),
    Document(text="Vector databases are used to store and retrieve embeddings for similarity search in RAG applications."),
    Document(text="Context-augmented generation combines retrieval and generation to provide more accurate and relevant responses.")
]

print("üìö Creating vector index from sample documents...")
index = VectorStoreIndex.from_documents(documents)
print("‚úÖ Vector index created successfully")
```

## Perform Queries

Now let's perform some queries to demonstrate the AgentOps integration:


```
# Create query engine
query_engine = index.as_query_engine()

print("üîç Performing queries...")

# Sample queries
queries = [
    "What is LlamaIndex?",
    "How does AgentOps help with AI applications?",
    "What are the benefits of using vector databases in RAG?"
]

for i, query in enumerate(queries, 1):
    print(f"\nüìù Query {i}: {query}")
    response = query_engine.query(query)
    print(f"üí¨ Response: {response}")
```

## Results

After running this notebook, you should see:

1. **AgentOps Session Link**: A URL to view the session in your AgentOps dashboard
2. **Cost Tracking**: Information about the cost of LLM calls (if using paid APIs)
3. **Operation Tracking**: All LlamaIndex operations are automatically tracked

Check your AgentOps dashboard to see detailed information about:
- LLM calls and responses
- Performance metrics
- Cost analysis
- Session replay

The session link will be printed in the output above by AgentOps.


```
print("\n" + "=" * 50)
print("üéâ Example completed successfully!")
print("üìä Check your AgentOps dashboard to see the recorded session with LLM calls and operations.")
print("üîó The session link should be printed above by AgentOps.")
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/google_genai.mdx

---
title: 'Google GenAI'
description: 'Google Generative AI Example with AgentOps'
---
{/*  SOURCE_FILE: examples/google_genai/gemini_example.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/google_genai/gemini_example.ipynb'} target={'_blank'}>Github</a>_

# Google Generative AI Example with AgentOps

This notebook demonstrates how to use AgentOps with Google's Generative AI package for observing both synchronous and streaming text generation.




## Installation
<CodeGroup>
  ```bash pip
  pip install agentops google-genai
  ```
  ```bash poetry
  poetry add agentops google-genai
  ```
  ```bash uv
  uv add agentops google-genai
  ```
</CodeGroup>

```python
from google import genai
import agentops
from dotenv import load_dotenv
import os
```


```python
load_dotenv()

os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["GEMINI_API_KEY"] = os.getenv("GEMINI_API_KEY", "your_gemini_api_key_here")
```


```python
# Initialize AgentOps and Gemini client
agentops.init(tags=["gemini-example", "agentops-example"])
client = genai.Client()
```


```python
# Test synchronous generation
print("Testing synchronous generation:")
response = client.models.generate_content(model="gemini-1.5-flash", contents="What are the three laws of robotics?")
print(response.text)
```


```python
# Test streaming generation
print("\nTesting streaming generation:")
response_stream = client.models.generate_content_stream(
    model="gemini-1.5-flash", contents="Explain the concept of machine learning in simple terms."
)

for chunk in response_stream:
    print(chunk.text, end="")
print()  # Add newline after streaming output

# Test another synchronous generation
print("\nTesting another synchronous generation:")
response = client.models.generate_content(
    model="gemini-1.5-flash", contents="What is the difference between supervised and unsupervised learning?"
)
print(response.text)
```


```python
# Example of token counting
print("\nTesting token counting:")
token_response = client.models.count_tokens(
    model="gemini-1.5-flash", contents="This is a test sentence to count tokens."
)
print(f"Token count: {token_response.total_tokens}")
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/litellm.mdx

---
title: 'LiteLLM'
description: 'AgentOps for observing LiteLLM'
---
{/*  SOURCE_FILE: examples/litellm/litellm_example.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/litellm/litellm_example.ipynb'} target={'_blank'}>Github</a>_

# AgentOps for observing LiteLLM

We can use AgentOps to observe LiteLLM, a lightweight library for working with large language models. This integration allows you to monitor and log the performance of your LiteLLM applications, providing insights into their behavior and efficiency.
LiteLLM integration extends observability to the different agent libraries which rely on LiteLLM and hence make it possible to observe the agents built using these libraries.

[See our LiteLLM docs](https://docs.agentops.ai/v1/integrations/litellm)

First let's install the required packages



## Installation
<CodeGroup>
  ```bash pip
  pip install -U agentops litellm python-dotenv
  ```
  ```bash poetry
  poetry add -U agentops litellm python-dotenv
  ```
  ```bash uv
  uv add -U agentops litellm python-dotenv
  ```
</CodeGroup>

Then import them


```python
import litellm
import agentops
import os
from dotenv import load_dotenv
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!

LiteLLM allows you to use several models including from OpenAI, Llama, Mistral, Claude, Gemini, Gemma, Dall-E, Whisper, and more all using the OpenAI format. To use a different model all you need to change are the API KEY and model (litellm.completion(model="...")).


```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv(
    "OPENAI_API_KEY", "your_openai_api_key_here"
)  # or the provider of your choosing
```


```python
agentops.init(auto_start_session=False)
tracer = agentops.start_trace(trace_name="LiteLLM Example", tags=["litellm-example", "agentops-example"])
```

Note: AgentOps requires that you call LiteLLM completions differently than the LiteLLM's docs mention
Instead of doing this -

```python
from litellm import completion
completion()
```

You should do this -

```python
import litellm
litellm.completion()
```


```python
messages = [{"role": "user", "content": "Write a 12 word poem about secret agents."}]
response = litellm.completion(model="gpt-4", messages=messages)  # or the model of your choosing
print(response.choices[0].message.content)
```


```python
agentops.end_trace(tracer, end_state="Success")
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/xai.mdx

---
title: 'xAI'
description: 'XAI Example'
---
{/*  SOURCE_FILE: examples/xai/grok_examples.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/xai/grok_examples.ipynb'} target={'_blank'}>Github</a>_

# XAI Example
This notebook demonstrates how to use XAI with AgentOps via the OpenAI python client. 

We are going to use the latest Grok model from XAI to create a transliteration chatbot that can understand the major languages of the world and translate them to a user's native language! We will use AgentOps to track the chatbot's performance.

First let's install the required packages



## Installation
<CodeGroup>
  ```bash pip
  pip install -U agentops openai
  ```
  ```bash poetry
  poetry add -U agentops openai
  ```
  ```bash uv
  uv add -U agentops openai
  ```
</CodeGroup>

Then import them


```python
from openai import OpenAI
import agentops
import os
from dotenv import load_dotenv
```

Next, we'll grab our API keys. You can use dotenv like below or however else you like to load environment variables


```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
```

Next we initialize the AgentOps client.


```python
agentops.init(auto_start_session=False)
tracer = agentops.start_trace(trace_name="XAI Example", tags=["xai-example", "grok", "agentops-example"])
```

And we are all set! Note the session URL above. We will use it to track the chatbot.

Let's initialize the OpenAI client with the XAI API key and base url.


```python
client = OpenAI(
    base_url="https://api.x.ai/v1",
    api_key=os.getenv("XAI_API_KEY", "your_xai_api_key_here"),
)
```

Now we will set the system and instruction prompts for the chatbot. We will set the native language to Spanish and the user prompt to transliterate an excerpt from Haruki Murakami's "Kafka On The Shore".


```python
SYSTEM_PROMPT = """
You are a highly intelligent, multilingual assistant designed to understand user prompts in English and respond in the user's specified native language. 
Your key responsibilities include:
1. Translating and generating meaningful, contextually appropriate responses in the user's native language.
2. Ensuring the output is accurate, coherent, and in Unicode format for proper display in the specified language.
3. Adhering to the nuances of the specified language's grammar, tone, and cultural context.

When asked to respond in a language, generate the response entirely in that language without using English unless explicitly requested.

If the specified language is unfamiliar or ambiguous, politely ask for clarification in English.
"""
```


```python
native_language = "Spanish"

USER_PROMPT = """
Sometimes fate is like a small sandstorm that keeps changing directions. You change direction but the sandstorm chases you. 
You turn again, but the storm adjusts. Over and over you play this out, like some ominous dance with death just before dawn. Why? 
Because this storm isn‚Äôt something that blew in from far away, something that has nothing to do with you. This storm is you. 
Something inside of you. So all you can do is give in to it, step right inside the storm, closing your eyes and plugging up your ears so the sand doesn‚Äôt get in, and walk through it, step by step. 
There‚Äôs no sun there, no moon, no direction, no sense of time. Just fine white sand swirling up into the sky like pulverized bones. 
That‚Äôs the kind of sandstorm you need to imagine.

And you really will have to make it through that violent, metaphysical, symbolic storm. 
No matter how metaphysical or symbolic it might be, make no mistake about it: it will cut through flesh like a thousand razor blades. People will bleed there, and you will bleed too. 
Hot, red blood. You‚Äôll catch that blood in your hands, your own blood and the blood of others.

And once the storm is over you won‚Äôt remember how you made it through, how you managed to survive. You won‚Äôt even be sure, in fact, whether the storm is really over. 
But one thing is certain. When you come out of the storm you won‚Äôt be the same person who walked in. That‚Äôs what this storm‚Äôs all about.
"""

INSTRUCTION_PROMPT = f"""
You are a multilingual chatbot. Take the user's prompt: "{USER_PROMPT}" and respond naturally in {native_language}. 
Ensure that the response is in Unicode characters appropriate for {native_language}.
"""
```

Now we will use the OpenAI client to generate the response by passing in the system and instruction prompts.


```python
response = client.chat.completions.create(
    model="grok-3-mini",
    messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": INSTRUCTION_PROMPT}],
)
```


```python
print(f"Original Prompt:\n{USER_PROMPT}")
generated_response = response.choices[0].message.content
print(f"Response in {native_language}:\n{generated_response}")
```

Awesome! We can now transliterate from English to any language! And all of this can be tracked with AgentOps by going to the session url above.


```python
agentops.end_trace(tracer, end_state="Success")
```

We end the session with a success state and a success reason. This is useful if you want to track the success or failure of the chatbot. In that case you can set the end state to failure and provide a reason. By default the session will have an indeterminate end state.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/smolagents.mdx

---
title: 'Smolagents'
description: 'Orchestrate a Multi-Agent System'
---
{/*  SOURCE_FILE: examples/smolagents/multi_smolagents_system.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/smolagents/multi_smolagents_system.ipynb'} target={'_blank'}>Github</a>_

# Orchestrate a Multi-Agent System

In this notebook, we will make a multi-agent web browser: an agentic system with several agents collaborating to solve problems using the web!

It will be a simple hierarchy, using a `ManagedAgent` object to wrap the managed web search agent:

```
+----------------+
| Manager agent  |
+----------------+
         |
_________|______________
|                        |
Code interpreter   +--------------------------------+
       tool        |         Managed agent          |
                   |      +------------------+      |
                   |      | Web Search agent |      |
                   |      +------------------+      |
                   |         |            |         |
                   |  Web Search tool     |         |
                   |             Visit webpage tool |
                   +--------------------------------+
```
Let‚Äôs set up this system.

Run the line below to install the required dependencies:



## Installation
<CodeGroup>
  ```bash pip
  pip install agentops duckduckgo-search markdownify smolagents
  ```
  ```bash poetry
  poetry add agentops duckduckgo-search markdownify smolagents
  ```
  ```bash uv
  uv add agentops duckduckgo-search markdownify smolagents
  ```
</CodeGroup>

üñáÔ∏è Now we initialize the AgentOps client and load the environment variables to use the API keys.


```python
import agentops
from dotenv import load_dotenv
import os
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import LiteLLMModel, tool, CodeAgent, ToolCallingAgent, DuckDuckGoSearchTool

load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
```

‚ö°Ô∏è Our agent will be powered by `openai/gpt-4o-mini` using the `LiteLLMModel` class.


```python
from smolagents import LiteLLMModel, tool ,CodeAgent, ToolCallingAgent, DuckDuckGoSearchTool
agentops.init(auto_start_session=False)
tracer = agentops.start_trace(
    trace_name="Orchestrate a Multi-Agent System", tags=["smolagents", "example", "multi-agent", "agentops-example"]
)
model = LiteLLMModel("openai/gpt-4o-mini")
```

## Create a Web Search Tool

For web browsing, we can already use our pre-existing `DuckDuckGoSearchTool`. However, we will also create a `VisitWebpageTool` from scratch using `markdownify`. Here‚Äôs how:


```python
@tool
def visit_webpage(url: str) -> str:
    """Visits a webpage at the given URL and returns its content as a markdown string.

    Args:
        url: The URL of the webpage to visit.

    Returns:
        The content of the webpage converted to Markdown, or an error message if the request fails.
    """
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Convert the HTML content to Markdown
        markdown_content = markdownify(response.text).strip()

        # Remove multiple line breaks
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

Let‚Äôs test our tool:


```python
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## Build Our Multi-Agent System

We will now use the tools `search` and `visit_webpage` to create the web agent.


```python
web_agent = ToolCallingAgent(
    tools=[DuckDuckGoSearchTool(), visit_webpage],
    model=model,
    name="search",
    description="Runs web searches for you. Give it your query as an argument.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

Let‚Äôs run our system with the following query:


```python
answer = manager_agent.run(
    "If LLM trainings continue to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What does that correspond to, compared to some countries? Please provide a source for any number used."
)

print(answer)
```

Awesome! We've successfully run a multi-agent system. Let's end the agentops session with a "Success" state. You can also end the session with a "Failure" or "Indeterminate" state, which is set as default.


```python
agentops.end_trace(tracer, end_state="Success")
```

You can view the session in the [AgentOps dashboard](https://app.agentops.ai/sessions) by clicking the link provided after ending the session.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/openai_agents.mdx

---
title: 'OpenAI Agents'
description: 'Airline Customer Service Agent'
---
{/*  SOURCE_FILE: examples/openai_agents/customer_service_agent.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/openai_agents/customer_service_agent.ipynb'} target={'_blank'}>Github</a>_

# Airline Customer Service Agent

This is a simple chatbot designed to assist airline customers with common queries. Here the agents are also used as tools to help the bot answer questions more effectively.

Using AgentOps we can track the flow of the conversation and the agents used. This is useful for debugging and understanding how the bot is performing.

## Prerequisites

Before running this notebook, you'll need:

1. **AgentOps Account**: Create a free account at [app.agentops.ai](https://app.agentops.ai)
2. **AgentOps API Key**: Obtain your API key from your AgentOps dashboard
3. **OpenAI API Key**: Get your API key from [platform.openai.com](https://platform.openai.com)

Make sure to set these as environment variables or create a `.env` file in your project root with:

```
AGENTOPS_API_KEY=your_agentops_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```




## Installation
<CodeGroup>
  ```bash pip
  pip install -q agentops openai-agents pydotenv
  ```
  ```bash poetry
  poetry add -q agentops openai-agents pydotenv
  ```
  ```bash uv
  uv add -q agentops openai-agents pydotenv
  ```
</CodeGroup>

```python
# Set the API keys for your AgentOps and OpenAI accounts.
import os
from dotenv import load_dotenv

load_dotenv()

os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
```


```python
from __future__ import annotations as _annotations  # noqa: F404

import random
import uuid

from pydantic import BaseModel
import agentops

from agents import (  # noqa: E402
    Agent,
    HandoffOutputItem,
    ItemHelpers,
    MessageOutputItem,
    RunContextWrapper,
    Runner,
    ToolCallItem,
    ToolCallOutputItem,
    TResponseInputItem,
    function_tool,
    handoff,
    trace,
)
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX  # noqa: E402
```


```python
agentops.init(tags=["customer-service-agent", "openai-agents", "agentops-example"])
tracer = agentops.start_trace(trace_name="Customer Service Agent")
```


```python
# Context model for the airline agent
class AirlineAgentContext(BaseModel):
    passenger_name: str | None = None
    confirmation_number: str | None = None
    seat_number: str | None = None
    flight_number: str | None = None
```


```python
# Tools for the airline agent
@function_tool(name_override="faq_lookup_tool", description_override="Lookup frequently asked questions.")
async def faq_lookup_tool(question: str) -> str:
    if "bag" in question or "baggage" in question:
        return (
            "You are allowed to bring one bag on the plane. "
            "It must be under 50 pounds and 22 inches x 14 inches x 9 inches."
        )
    elif "seats" in question or "plane" in question:
        return (
            "There are 120 seats on the plane. "
            "There are 22 business class seats and 98 economy seats. "
            "Exit rows are rows 4 and 16. "
            "Rows 5-8 are Economy Plus, with extra legroom. "
        )
    elif "wifi" in question:
        return "We have free wifi on the plane, join Airline-Wifi"
    return "I'm sorry, I don't know the answer to that question."


@function_tool
async def update_seat(context: RunContextWrapper[AirlineAgentContext], confirmation_number: str, new_seat: str) -> str:
    """
    Update the seat for a given confirmation number.

    Args:
        confirmation_number: The confirmation number for the flight.
        new_seat: The new seat to update to.
    """
    # Update the context based on the customer's input
    context.context.confirmation_number = confirmation_number
    context.context.seat_number = new_seat
    # Ensure that the flight number has been set by the incoming handoff
    assert context.context.flight_number is not None, "Flight number is required"
    return f"Updated seat to {new_seat} for confirmation number {confirmation_number}"


### HOOKS


async def on_seat_booking_handoff(context: RunContextWrapper[AirlineAgentContext]) -> None:
    flight_number = f"FLT-{random.randint(100, 999)}"
    context.context.flight_number = flight_number


### AGENTS

faq_agent = Agent[AirlineAgentContext](
    name="FAQ Agent",
    handoff_description="A helpful agent that can answer questions about the airline.",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    You are an FAQ agent. If you are speaking to a customer, you probably were transferred to from the triage agent.
    Use the following routine to support the customer.
    # Routine
    1. Identify the last question asked by the customer.
    2. Use the faq lookup tool to answer the question. Do not rely on your own knowledge.
    3. If you cannot answer the question, transfer back to the triage agent.""",
    tools=[faq_lookup_tool],
)

seat_booking_agent = Agent[AirlineAgentContext](
    name="Seat Booking Agent",
    handoff_description="A helpful agent that can update a seat on a flight.",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    You are a seat booking agent. If you are speaking to a customer, you probably were transferred to from the triage agent.
    Use the following routine to support the customer.
    # Routine
    1. Ask for their confirmation number.
    2. Ask the customer what their desired seat number is.
    3. Use the update seat tool to update the seat on the flight.
    If the customer asks a question that is not related to the routine, transfer back to the triage agent. """,
    tools=[update_seat],
)

triage_agent = Agent[AirlineAgentContext](
    name="Triage Agent",
    handoff_description="A triage agent that can delegate a customer's request to the appropriate agent.",
    instructions=(
        f"{RECOMMENDED_PROMPT_PREFIX} "
        "You are a helpful triaging agent. You can use your tools to delegate questions to other appropriate agents."
    ),
    handoffs=[
        faq_agent,
        handoff(agent=seat_booking_agent, on_handoff=on_seat_booking_handoff),
    ],
)
```


```python
faq_agent.handoffs.append(triage_agent)
seat_booking_agent.handoffs.append(triage_agent)
```


```python
async def main():
    current_agent: Agent[AirlineAgentContext] = triage_agent
    input_items: list[TResponseInputItem] = []
    context = AirlineAgentContext()

    # Normally, each input from the user would be an API request to your app, and you can wrap the request in a trace()
    # Here, we'll just use a random UUID for the conversation ID
    conversation_id = uuid.uuid4().hex[:16]

    while True:
        user_input = input("Enter your message: ")
        with trace("Customer service", group_id=conversation_id):
            input_items.append({"content": user_input, "role": "user"})
            result = await Runner.run(current_agent, input_items, context=context)

            for new_item in result.new_items:
                agent_name = new_item.agent.name
                if isinstance(new_item, MessageOutputItem):
                    print(f"{agent_name}: {ItemHelpers.text_message_output(new_item)}")
                elif isinstance(new_item, HandoffOutputItem):
                    print(f"Handed off from {new_item.source_agent.name} to {new_item.target_agent.name}")
                elif isinstance(new_item, ToolCallItem):
                    print(f"{agent_name}: Calling a tool")
                elif isinstance(new_item, ToolCallOutputItem):
                    print(f"{agent_name}: Tool call output: {new_item.output}")
                else:
                    print(f"{agent_name}: Skipping item: {new_item.__class__.__name__}")
            input_items = result.to_input_list()
            current_agent = result.last_agent
```


```python
await main()
agentops.end_trace(tracer, end_state="Success")
```

## Conclusion

**AgentOps makes observability effortless** - simply import the library and all your interactions are automatically tracked, visualized, and analyzed. This enables you to:

- Monitor tool performance across different use cases
- Optimize costs by understanding tool usage patterns
- Debug tool integration issues quickly
- Scale your AI applications with confidence in tool reliability

Visit [app.agentops.ai](https://app.agentops.ai) to explore your tool usage sessions and gain deeper insights into your AI application's tool interactions.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/openai.mdx

---
title: 'OpenAI'
description: 'Load the dataset (ensure you are logged in with huggingface-cli if needed)'
---
{/*  SOURCE_FILE: examples/openai/multi_tool_orchestration.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/multi_tool_orchestration.ipynb'} target={'_blank'}>Github</a>_

### Multi-Tool Orchestration with RAG approach using OpenAI's Responses API


This cookbook guides you through building dynamic, multi-tool workflows using OpenAI's Responses API. It demonstrates how to implement a Retrieval-Augmented Generation (RAG) approach that intelligently routes user queries to the appropriate in-built or external tools. Whether your query calls for general knowledge or requires accessing specific internal context from a vector database (like Pinecone), this guide shows you how to integrate function calls, web searches in-built tool, and leverage document retrieval to generate accurate, context-aware responses.




## Installation
<CodeGroup>
  ```bash pip
  pip install agentops datasets openai pandas pinecone python-dotenv tqdm
  ```
  ```bash poetry
  poetry add agentops datasets openai pandas pinecone python-dotenv tqdm
  ```
  ```bash uv
  uv add agentops datasets openai pandas pinecone python-dotenv tqdm
  ```
</CodeGroup>

```python
import os
from dotenv import load_dotenv
import time
from tqdm.auto import tqdm
from pandas import DataFrame
from datasets import load_dataset
import random
import string
from openai import OpenAI
import agentops
from pinecone import Pinecone
from pinecone import ServerlessSpec
```


```python
load_dotenv()

os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
os.environ["PINECONE_API_KEY"] = os.getenv("PINECONE_API_KEY", "your_pinecone_api_key_here")
```


```python
agentops.init(auto_start_session=True)
tracer = agentops.start_trace(
    trace_name="Multi-Tool Orchestration with RAG",
    tags=["multi-tool-orchestration-rag-demo", "openai-responses", "agentops-example"],
)
client = OpenAI()
```

In this example we use a sample medical reasoning dataset from Hugging Face. We convert the dataset into a Pandas DataFrame and merge the ‚ÄúQuestion‚Äù and ‚ÄúResponse‚Äù columns into a single string. This merged text is used for embedding and later stored as metadata.


```python
# Load the dataset (ensure you're logged in with huggingface-cli if needed)
ds = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", "en", split="train[:100]", trust_remote_code=True)
ds_dataframe = DataFrame(ds)

# Merge the Question and Response columns into a single string.
ds_dataframe["merged"] = ds_dataframe.apply(
    lambda row: f"Question: {row['Question']} Answer: {row['Response']}", axis=1
)
print("Example merged text:", ds_dataframe["merged"].iloc[0])
```


```python
ds_dataframe
```

### Create a Pinecone Index Based on the Dataset
Use the dataset itself to determine the embedding dimensionality. For example, compute one embedding from the merged column and then create the index accordingly.


```python
MODEL = "text-embedding-3-small"  # Replace with your production embedding model if needed
# Compute an embedding for the first document to obtain the embedding dimension.
sample_embedding_resp = client.embeddings.create(input=[ds_dataframe["merged"].iloc[0]], model=MODEL)
embed_dim = len(sample_embedding_resp.data[0].embedding)
print(f"Embedding dimension: {embed_dim}")
```


```python
# Initialize Pinecone using your API key.
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])

# Define the Pinecone serverless specification.
AWS_REGION = "us-east-1"
spec = ServerlessSpec(cloud="aws", region=AWS_REGION)

# Create a random index name with lower case alphanumeric characters and '-'
index_name = "pinecone-index-" + "".join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Create the index if it doesn't already exist.
if index_name not in pc.list_indexes().names():
    pc.create_index(index_name, dimension=embed_dim, metric="dotproduct", spec=spec)

# Connect to the index.
index = pc.Index(index_name)
time.sleep(1)
print("Index stats:", index.describe_index_stats())
```

#### Upsert the Dataset into Pinecone index

Process the dataset in batches, generate embeddings for each merged text, prepare metadata (including separate Question and Answer fields), and upsert each batch into the index. You may also update metadata for specific entries if needed.


```python
batch_size = 32
for i in tqdm(range(0, len(ds_dataframe["merged"]), batch_size), desc="Upserting to Pinecone"):
    i_end = min(i + batch_size, len(ds_dataframe["merged"]))
    lines_batch = ds_dataframe["merged"][i:i_end]
    ids_batch = [str(n) for n in range(i, i_end)]

    # Create embeddings for the current batch.
    res = client.embeddings.create(input=[line for line in lines_batch], model=MODEL)
    embeds = [record.embedding for record in res.data]

    # Prepare metadata by extracting original Question and Answer.
    meta = []
    for record in ds_dataframe.iloc[i:i_end].to_dict("records"):
        q_text = record["Question"]
        a_text = record["Response"]
        # Optionally update metadata for specific entries.
        meta.append({"Question": q_text, "Answer": a_text})

    # Upsert the batch into Pinecone.
    vectors = list(zip(ids_batch, embeds, meta))
    index.upsert(vectors=vectors)
```

### Query the Pinecone Index

Create a natural language query, compute its embedding, and perform a similarity search on the Pinecone index. The returned results include metadata that provides context for generating answers.


```python
def query_pinecone_index(client, index, model, query_text):
    # Generate an embedding for the query.
    query_embedding = client.embeddings.create(input=query_text, model=model).data[0].embedding

    # Query the index and return top 5 matches.
    res = index.query(vector=[query_embedding], top_k=5, include_metadata=True)
    print("Query Results:")
    for match in res["matches"]:
        print(
            f"{match['score']:.2f}: {match['metadata'].get('Question', 'N/A')} - {match['metadata'].get('Answer', 'N/A')}"
        )
    return res
```


```python
# Example usage with a different query from the train/test set
query = (
    "A 45-year-old man with a history of alcohol use presents with symptoms including confusion, ataxia, and ophthalmoplegia. "
    "What is the most likely diagnosis and the recommended treatment?"
)
query_pinecone_index(client, index, MODEL, query)
```

### Generate a Response Using the Retrieved Context

Select the best matching result from your query results and use the OpenAI Responses API to generate a final answer by combining the retrieved context with the original question.


```python
# Retrieve and concatenate top 3 match contexts.
matches = index.query(
    vector=[client.embeddings.create(input=query, model=MODEL).data[0].embedding], top_k=3, include_metadata=True
)["matches"]

context = "\n\n".join(
    f"Question: {m['metadata'].get('Question', '')}\nAnswer: {m['metadata'].get('Answer', '')}" for m in matches
)
# Use the context to generate a final answer.
response = client.responses.create(
    model="gpt-4o",
    input=f"Provide the answer based on the context: {context} and the question: {query} as per the internal knowledge base",
)
print("\nFinal Answer:")
print(response.output_text)
```

### Orchestrate Multi-Tool Calls

Now, we'll define the built-in function available through the Responses API, including the ability to invoke the external Vector Store - Pinecone as an example.

*Web Search Preview Tool*: Enables the model to perform live web searches and preview the results. This is ideal for retrieving real-time or up-to-date information from the internet.

*Pinecone Search Tool*: Allows the model to query a vector database using semantic search. This is especially useful for retrieving relevant documents‚Äîsuch as medical literature or other domain-specific content‚Äîthat have been stored in a vectorized format.


```python
# Tools definition: The list of tools includes:
# - A web search preview tool.
# - A Pinecone search tool for retrieving medical documents.

# Define available tools.
tools = [
    {
        "type": "web_search_preview",
        "user_location": {"type": "approximate", "country": "US", "region": "California", "city": "SF"},
        "search_context_size": "medium",
    },
    {
        "type": "function",
        "name": "PineconeSearchDocuments",
        "description": "Search for relevant documents based on the medical question asked by the user that is stored within the vector database using a semantic query.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "The natural language query to search the vector database."},
                "top_k": {"type": "integer", "description": "Number of top results to return.", "default": 3},
            },
            "required": ["query"],
            "additionalProperties": False,
        },
    },
]
```


```python
# Example queries that the model should route appropriately.
queries = [
    {"query": "Who won the cricket world cup in 1983?"},
    {"query": "What is the most common cause of death in the United States according to the internet?"},
    {
        "query": (
            "A 7-year-old boy with sickle cell disease is experiencing knee and hip pain, "
            "has been admitted for pain crises in the past, and now walks with a limp. "
            "His exam shows a normal, cool hip with decreased range of motion and pain with ambulation. "
            "What is the most appropriate next step in management according to the internal knowledge base?"
        )
    },
]
```


```python
# Process each query dynamically.
for item in queries:
    input_messages = [{"role": "user", "content": item["query"]}]
    print("\nüåü--- Processing Query ---üåü")
    print(f"üîç **User Query:** {item['query']}")

    # Call the Responses API with tools enabled and allow parallel tool calls.
    response = client.responses.create(
        model="gpt-4o",
        input=[
            {
                "role": "system",
                "content": "When prompted with a question, select the right tool to use based on the question.",
            },
            {"role": "user", "content": item["query"]},
        ],
        tools=tools,
        parallel_tool_calls=True,
    )

    print("\n‚ú® **Initial Response Output:**")
    print(response.output)

    # Determine if a tool call is needed and process accordingly.
    if response.output:
        tool_call = response.output[0]
        if tool_call.type in ["web_search_preview", "function_call"]:
            tool_name = tool_call.name if tool_call.type == "function_call" else "web_search_preview"
            print(f"\nüîß **Model triggered a tool call:** {tool_name}")

            if tool_name == "PineconeSearchDocuments":
                print("üîç **Invoking PineconeSearchDocuments tool...**")
                res = query_pinecone_index(client, index, MODEL, item["query"])
                if res["matches"]:
                    best_match = res["matches"][0]["metadata"]
                    result = f"**Question:** {best_match.get('Question', 'N/A')}\n**Answer:** {best_match.get('Answer', 'N/A')}"
                else:
                    result = "**No matching documents found in the index.**"
                print("‚úÖ **PineconeSearchDocuments tool invoked successfully.**")
            else:
                print("üîç **Invoking simulated web search tool...**")
                result = "**Simulated web search result.**"
                print("‚úÖ **Simulated web search tool invoked successfully.**")

            # Append the tool call and its output back into the conversation.
            input_messages.append(tool_call)
            input_messages.append({"type": "function_call_output", "call_id": tool_call.call_id, "output": str(result)})

            # Get the final answer incorporating the tool's result.
            final_response = client.responses.create(
                model="gpt-4o", input=input_messages, tools=tools, parallel_tool_calls=True
            )
            print("\nüí° **Final Answer:**")
            print(final_response.output_text)
        else:
            # If no tool call is triggered, print the response directly.
            print("üí° **Final Answer:**")
            print(response.output_text)
```

As shown above, depending on the query, appropriate tool is invoked in order to determine the optimal response.

For instance, looking at the third example, when the model triggers the tool named "PineconeSearchDocuments", the code calls‚ÄØ`query_pinecone_index`‚ÄØwith the current query and then extracts the best match (or an appropriate context) as the result. For non health related inqueries or queries where explicit internet search is asked, the code calls the‚ÄØweb_search_call‚ÄØfunction and for other queries, it may choose to not call any tool and rather provide a response based on the question under consideration.

Finally, the tool call and its output are appended to the conversation, and the final answer is generated by the Responses API.

### Multi-tool orchestration flow

Now let us try to modify the input query and the system instructions to the responses API in order to follow a tool calling sequence and generate the output. 


```python
# Process one query as an example to understand the tool calls and function calls as part of the response output
item = "What is the most common cause of death in the United States"

# Initialize input messages with the user's query.
input_messages = [{"role": "user", "content": item}]
print("\nüåü--- Processing Query ---üåü")
print(f"üîç **User Query:** {item}")

# Call the Responses API with tools enabled and allow parallel tool calls.
print("\nüîß **Calling Responses API with Tools Enabled**")
print("\nüïµÔ∏è‚Äç‚ôÇÔ∏è **Step 1: Web Search Call**")
print("   - Initiating web search to gather initial information.")
print("\nüìö **Step 2: Pinecone Search Call**")
print("   - Querying Pinecone to find relevant examples from the internal knowledge base.")

response = client.responses.create(
    model="gpt-4o",
    input=[
        {
            "role": "system",
            "content": "Every time it's prompted with a question, first call the web search tool for results, then call `PineconeSearchDocuments` to find real examples in the internal knowledge base.",
        },
        {"role": "user", "content": item},
    ],
    tools=tools,
    parallel_tool_calls=True,
)

# Print the initial response output.
print("input_messages", input_messages)

print("\n‚ú® **Initial Response Output:**")
print(response.output)
```


```python
# Understand the tool calls and function calls as part of the response output

import pandas as pd

# Create a list to store the tool call and function call details
tool_calls = []

# Iterate through the response output and collect the details
for i in response.output:
    tool_calls.append(
        {
            "Type": i.type,
            "Call ID": i.call_id if hasattr(i, "call_id") else i.id if hasattr(i, "id") else "N/A",
            "Output": str(i.output) if hasattr(i, "output") else "N/A",
            "Name": i.name if hasattr(i, "name") else "N/A",
        }
    )

# Convert the list to a DataFrame for tabular display
df_tool_calls = pd.DataFrame(tool_calls)

# Display the DataFrame
df_tool_calls
```


```python
tool_call_1 = response.output[0]
print(tool_call_1)
print(tool_call_1.id)

tool_call_2 = response.output[2]
print(tool_call_2)
print(tool_call_2.call_id)
```


```python
# append the tool call and its output back into the conversation.
input_messages.append(response.output[2])
input_messages.append({"type": "function_call_output", "call_id": tool_call_2.call_id, "output": str(result)})
print(input_messages)
```


```python
# Get the final answer incorporating the tool's result.
print("\nüîß **Calling Responses API for Final Answer**")

response_2 = client.responses.create(
    model="gpt-4o",
    input=input_messages,
)
print(response_2)
```


```python
# print the final answer
print(response_2.output_text)
agentops.end_trace(tracer, end_state="Success")
```


Here, we have seen  how to utilize OpenAI's Responses API to implement a Retrieval-Augmented Generation (RAG) approach with multi-tool calling capabilities. It showcases an example where the model selects the appropriate tool based on the input query: general questions may be handled by built-in tools such as web-search, while specific medical inquiries related to internal knowledge are addressed by retrieving context from a vector database (such as Pinecone) via function calls. Additonally, we have showcased how multiple tool calls can be sequentially combined to generate a final response based on our instructions provided to responses API. Happy coding! 


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/watsonx.mdx

---
title: 'IBM Watsonx.ai Example'
description: 'Using IBM Watsonx.ai for text generation and chat with AgentOps'
---
{/*  SOURCE_FILE: examples/watsonx/watsonx-text-chat.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/watsonx/watsonx-text-chat.ipynb'} target={'_blank'}>Github</a>_

# IBM Watsonx.ai Text Generation and Chat with AgentOps

This notebook demonstrates how to use IBM Watsonx.ai for basic text generation and chat completion tasks with AgentOps instrumentation.

## Installation
Install the required packages:
<CodeGroup>
  ```bash pip
  pip install agentops ibm-watsonx-ai python-dotenv
  ```
  ```bash poetry
  poetry add agentops ibm-watsonx-ai python-dotenv
  ```
  ```bash uv
  uv add agentops ibm-watsonx-ai python-dotenv
  ```
</CodeGroup>

## Setup

First, let's import the necessary libraries and initialize AgentOps:
```python
import agentops
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")

# Initialize AgentOps
agentops.init(tags=["watsonx-text-chat", "agentops-example"])
```

## Initialize IBM Watsonx.ai Credentials

To use IBM Watsonx.ai, you need to set up your credentials and project ID.
```python
# Initialize credentials - replace with your own API key
# Best practice: Store API keys in environment variables
# Ensure WATSONX_API_KEY, WATSONX_URL, and WATSONX_PROJECT_ID are set in your .env file or environment
os.environ["WATSONX_API_KEY"] = os.getenv("WATSONX_API_KEY", "your_watsonx_api_key_here")
os.environ["WATSONX_URL"] = os.getenv("WATSONX_URL", "https://eu-de.ml.cloud.ibm.com") # Example URL, ensure it's correct for your region
os.environ["WATSONX_PROJECT_ID"] = os.getenv("WATSONX_PROJECT_ID", "your-project-id-here")


credentials = Credentials(
    url=os.environ["WATSONX_URL"],
    api_key=os.environ["WATSONX_API_KEY"],
)

# Project ID for your IBM Watsonx project
project_id = os.environ["WATSONX_PROJECT_ID"]
```

## Text Generation

Let's use IBM Watsonx.ai to generate text based on a prompt:
```python
# Initialize text generation model
gen_model = ModelInference(model_id="google/flan-ul2", credentials=credentials, project_id=project_id)

# Generate text with a prompt
prompt = "Write a short poem about artificial intelligence:"
response = gen_model.generate_text(prompt)
print(f"Generated Text:\n{response}")
```

## Chat Completion

Now, let's use a different model for chat completion:
```python
# Initialize chat model
chat_model = ModelInference(
    model_id="meta-llama/llama-3-8b-instruct", # Using the model ID from the MDX as it might be more current/available
    credentials=credentials, 
    project_id=project_id
)

# Format messages for chat
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What are the three laws of robotics?"},
]

# Get chat response
chat_response = chat_model.chat(messages)
# Accessing response based on typical ibm-watsonx-ai SDK structure
print(f"Chat Response:\n{chat_response['results'][0]['generated_text']}") 
```

## Another Chat Example

Let's try a different type of query:
```python
# New chat messages
messages = [
    {"role": "system", "content": "You are an expert in machine learning."},
    {"role": "user", "content": "Explain the difference between supervised and unsupervised learning in simple terms."},
]

# Get chat response
chat_response = chat_model.chat(messages)
print(f"Chat Response:\n{chat_response['results'][0]['generated_text']}")
```

## Clean Up

Finally, let's close the persistent connection with the models if they were established and end the AgentOps session.
```python
# Close connections if persistent connections were used.
# This is good practice if the SDK version/usage implies persistent connections.
try:
    gen_model.close_persistent_connection()
    chat_model.close_persistent_connection()
except AttributeError:
    # Handle cases where this method might not exist (e.g. newer SDK versions or stateless calls)
    print("Note: close_persistent_connection not available or needed for one or more models.")
    pass

agentops.end_session("Success") # Manually end session
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/examples/autogen.mdx

---
title: 'AutoGen'
description: 'Microsoft Autogen Chat Example'
---
{/*  SOURCE_FILE: examples/autogen/AgentChat.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/autogen/AgentChat.ipynb'} target={'_blank'}>Github</a>_

# Microsoft Autogen Chat Example

AgentOps automatically configures itself when it's initialized meaning your agent run data will be tracked and logged to your AgentOps dashboard right away.

First let's install the required packages



## Installation
<CodeGroup>
  ```bash pip
  pip install "autogen-ext[openai]" -U agentops autogen-agentchat python-dotenv
  ```
  ```bash poetry
  poetry add "autogen-ext[openai]" -U agentops autogen-agentchat python-dotenv
  ```
  ```bash uv
  uv add "autogen-ext[openai]" -U agentops autogen-agentchat python-dotenv
  ```
</CodeGroup>

Then import them


```python
import os
from dotenv import load_dotenv
from IPython.core.error import (
    StdinNotImplementedError,
)
import asyncio

import agentops

from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
```


```python
# When initializing AgentOps, you can pass in optional tags to help filter sessions
agentops.init(auto_start_session=False)
tracer = agentops.start_trace(
    trace_name="Microsoft Agent Chat Example", tags=["autogen-chat", "microsoft-autogen", "agentops-example"]
)
```

AutoGen will now start automatically tracking

* LLM prompts and completions
* Token usage and costs
* Agent names and actions
* Correspondence between agents
* Tool usage
* Errors

# Simple Chat Example


```python
# Define model and API key
model_name = "gpt-4-turbo"  # Or "gpt-4o" / "gpt-4o-mini" as per migration guide examples
api_key = os.getenv("OPENAI_API_KEY")

# Create the model client
model_client = OpenAIChatCompletionClient(model=model_name, api_key=api_key)

# Create the agent that uses the LLM.
assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",  # Added system message for clarity
    model_client=model_client,
)

user_proxy_initiator = UserProxyAgent("user_initiator")


async def main():
    termination = MaxMessageTermination(max_messages=2)

    group_chat = RoundRobinGroupChat(
        [user_proxy_initiator, assistant],  # Corrected: agents as positional argument
        termination_condition=termination,
    )

    chat_task = "How can I help you today?"
    print(f"User Initiator: {chat_task}")

    try:
        stream = group_chat.run_stream(task=chat_task)
        await Console().run(stream)
        agentops.end_trace(tracer, end_state="Success")

    except StdinNotImplementedError:
        print("StdinNotImplementedError: This typically happens in non-interactive environments.")
        print("Skipping interactive part of chat for automation.")
        agentops.end_trace(tracer, end_state="Indeterminate")
    except Exception as e:
        print(f"An error occurred: {e}")
        agentops.end_trace(tracer, end_state="Error")
    finally:
        await model_client.close()


if __name__ == "__main__":
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        import nest_asyncio

        nest_asyncio.apply()
        asyncio.run(main())
    else:
        asyncio.run(main())
```

You can view data on this run at [app.agentops.ai](https://app.agentops.ai).

The dashboard will display LLM events for each message sent by each agent, including those made by the human user.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/langchain.mdx

---
title: 'LangChain'
description: 'AgentOps Langchain Agent Implementation'
---
{/*  SOURCE_FILE: examples/langchain/langchain_examples.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/langchain/langchain_examples.ipynb'} target={'_blank'}>Github</a>_

# AgentOps Langchain Agent Implementation

Using AgentOps monitoring with Langchain is simple. We've created a LangchainCallbackHandler that will do all of the heavy lifting!

First let's install the required packages



## Installation
<CodeGroup>
  ```bash pip
  pip install agentops langchain langchain_openai python-dotenv
  ```
  ```bash poetry
  poetry add agentops langchain langchain_openai python-dotenv
  ```
  ```bash uv
  uv add agentops langchain langchain_openai python-dotenv
  ```
</CodeGroup>

Then import them


```python
import os
from langchain_openai import ChatOpenAI
from langchain.agents import tool, AgentExecutor, create_openai_tools_agent
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
```

The only difference with using AgentOps is that we'll also import this special Callback Handler


```python
from agentops.integration.callbacks.langchain import (
    LangchainCallbackHandler as AgentOpsLangchainCallbackHandler,
)
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!


```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
```

This is where AgentOps comes into play. Before creating our LLM instance via Langchain, first we'll create an instance of the AO LangchainCallbackHandler. After the handler is initialized, a session will be recorded automatically.

Pass in your API key, and optionally any tags to describe this session for easier lookup in the AO dashboard.



```python
agentops_handler = AgentOpsLangchainCallbackHandler(tags=["Langchain Example", "agentops-example"])

llm = ChatOpenAI(callbacks=[agentops_handler], model="gpt-3.5-turbo")

# You must pass in a callback handler to record your agent
llm.callbacks = [agentops_handler]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Respond only in Spanish."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
        # ("tool_names", "find_movie")
    ]
)
```

Agents generally use tools. Let's define a simple tool here. Tool usage is also recorded.


```python
@tool
def find_movie(genre: str) -> str:
    """Find available movies"""
    if genre == "drama":
        return "Dune 2"
    else:
        return "Pineapple Express"


tools = [find_movie]
```

For each tool, you need to also add the callback handler


```python
for t in tools:
    t.callbacks = [agentops_handler]
```

Add the tools to our LLM


```python
llm_with_tools = llm.bind_tools([find_movie])
```

Finally, let's create our agent! Pass in the callback handler to the agent, and all the actions will be recorded in the AO Dashboard


```python
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
```


```python
agent_executor.invoke({"input": "What comedies are playing?"}, config={"callback": [agentops_handler]})
```

## Check your session
Finally, check your run on [AgentOps](https://app.agentops.ai). You will see a session recorded with the LLM calls and tool usage.


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/langgraph.mdx

---
title: 'Langgraph'
description: 'LangGraph Integration with AgentOps'
---
{/*  SOURCE_FILE: examples/langgraph/langgraph_example.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/langgraph/langgraph_example.ipynb'} target={'_blank'}>Github</a>_

# LangGraph Integration with AgentOps

This example demonstrates how to use LangGraph with AgentOps for comprehensive observability of your graph-based agent workflows.

LangGraph is a framework for building stateful, multi-step applications with LLMs. AgentOps automatically instruments LangGraph to track:
- Graph compilation and structure
- Node executions and transitions
- Tool usage within the graph
- LLM calls made by agents
- Complete execution flow with timing



## Installation
<CodeGroup>
  ```bash pip
  pip install agentops langchain-openai langgraph python-dotenv
  ```
  ```bash poetry
  poetry add agentops langchain-openai langgraph python-dotenv
  ```
  ```bash uv
  uv add agentops langchain-openai langgraph python-dotenv
  ```
</CodeGroup>

## Setup

First, let's import the necessary libraries and initialize AgentOps:


```python
import os
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
import agentops
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize AgentOps - this enables automatic instrumentation
agentops.init(os.getenv("AGENTOPS_API_KEY"), auto_start_session=False)
trace = agentops.start_trace("langgraph_example")
```

## Define Tools

Let's create some simple tools that our agent can use:


```python
@tool
def get_weather(location: str) -> str:
    """Get the weather for a given location."""
    # Simulated weather data
    weather_data = {
        "New York": "Sunny, 72¬∞F",
        "London": "Cloudy, 60¬∞F", 
        "Tokyo": "Rainy, 65¬∞F",
        "Paris": "Partly cloudy, 68¬∞F",
        "Sydney": "Clear, 75¬∞F"
    }
    return weather_data.get(location, f"Weather data not available for {location}")

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    try:
        result = eval(expression)
        return f"The result is: {result}"
    except Exception as e:
        return f"Error calculating expression: {str(e)}"

# Collect tools for binding to the model
tools = [get_weather, calculate]
```

## Define Agent State

In LangGraph, we need to define the state that will be passed between nodes:


```python
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
```

## Create the Model and Node Functions

We'll create a model with tool binding and define the functions that will be our graph nodes:


```python
# Create model with tool binding
model = ChatOpenAI(temperature=0, model="gpt-4o-mini").bind_tools(tools)

def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """Determine if we should continue to tools or end."""
    messages = state["messages"]
    last_message = messages[-1]
    
    # If the LLM wants to use tools, continue to the tools node
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    # Otherwise, we're done
    return "end"

def call_model(state: AgentState):
    """Call the language model."""
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}

def call_tools(state: AgentState):
    """Execute the tool calls requested by the model."""
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        
        # Find and execute the requested tool
        for tool in tools:
            if tool.name == tool_name:
                result = tool.invoke(tool_args)
                tool_messages.append(
                    ToolMessage(
                        content=str(result),
                        tool_call_id=tool_call["id"]
                    )
                )
                break
    
    return {"messages": tool_messages}
```

## Build the Graph

Now let's construct the LangGraph workflow:


```python
# Create the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)

# Set the entry point
workflow.set_entry_point("agent")

# Add conditional edges
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)

# Add edge from tools back to agent
workflow.add_edge("tools", "agent")

# Compile the graph
app = workflow.compile()
```

## Run Examples

Let's test our agent with different queries that require tool usage:


```python
# Example 1: Weather query
print("Example 1: Weather Query")
print("=" * 50)

messages = [HumanMessage(content="What's the weather in New York and Tokyo?")]
result = app.invoke({"messages": messages})

final_message = result["messages"][-1]
print(f"Response: {final_message.content}\n")
```


```python
# Example 2: Math calculation
print("Example 2: Math Calculation")
print("=" * 50)

messages = [HumanMessage(content="Calculate 25 * 4 + 10")]
result = app.invoke({"messages": messages})

final_message = result["messages"][-1]
print(f"Response: {final_message.content}\n")
```


```python
# Example 3: Combined query
print("Example 3: Combined Query")
print("=" * 50)

messages = [HumanMessage(content="What's the weather in Paris? Also calculate 100/5")]
result = app.invoke({"messages": messages})

final_message = result["messages"][-1]
print(f"Response: {final_message.content}\n")
```

## View in AgentOps Dashboard

After running this notebook, you can view the traces in your AgentOps dashboard. You'll see:

1. **Graph Compilation**: The structure of your LangGraph with nodes and edges
2. **Execution Flow**: How the graph executed, including:
   - Agent node calls
   - Tool node executions
   - State transitions
3. **LLM Calls**: Each ChatGPT call with prompts and completions
4. **Tool Usage**: Which tools were called and their results
5. **Timing Information**: How long each step took

The instrumentation captures the full context of your LangGraph application automatically!


```python
print("‚úÖ Check your AgentOps dashboard for comprehensive traces!")
print("üîç You'll see the graph structure, execution flow, and all LLM/tool calls.")
agentops.end_trace(trace)
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/google_adk.mdx

---
title: 'Google ADK'
description: 'Google ADK Example: Human Approval Workflow with AgentOps'
---
{/*  SOURCE_FILE: examples/google_adk/human_approval.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/google_adk/human_approval.ipynb'} target={'_blank'}>Github</a>_

# Google ADK Example: Human Approval Workflow with AgentOps

This notebook demonstrates a complete human approval workflow using the Google ADK (Agent Development Kit), integrated with AgentOps for observability.

**Key Features:**
- **Sequential Agent Processing:** The workflow uses multiple agents chained together to handle different stages of the approval process.
- **External Tool Integration:** An agent interacts with an external tool that simulates (or in this version, directly prompts for) human approval.
- **Session State Management:** Information is passed between agents and persisted using session state.
- **AgentOps Observability:** All agent actions, tool calls, and LLM interactions are traced and can be viewed in your AgentOps dashboard.
- **Interactive Human Input:** The approval step now requires direct input from the user.

## 1. Setup and Dependencies

First, let's install the necessary libraries if they are not already present and import them.



## Installation
<CodeGroup>
  ```bash pip
  pip install agentops asyncio google-adk nest_asyncio python-dotenv
  ```
  ```bash poetry
  poetry add agentops asyncio google-adk nest_asyncio python-dotenv
  ```
  ```bash uv
  uv add agentops asyncio google-adk nest_asyncio python-dotenv
  ```
</CodeGroup>


```python
import json
import os
import asyncio
from google.adk.agents import LlmAgent, SequentialAgent
from google.adk.tools import FunctionTool
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from pydantic import BaseModel, Field
import nest_asyncio
import agentops
from dotenv import load_dotenv
```

## 2. Configuration and Initialization

Load environment variables (especially `AGENTOPS_API_KEY` and your Google API key for Gemini) and initialize AgentOps.


```python
# Load environment variables from .env file
load_dotenv()
nest_asyncio.apply()
AGENTOPS_API_KEY = os.getenv("AGENTOPS_API_KEY") or "your_agentops_api_key_here"
# Initialize AgentOps - Just 2 lines!
agentops.init(AGENTOPS_API_KEY, trace_name="adk-human-approval-notebook", auto_start_session=False)
```

Define some constants for our application.


```python
APP_NAME = "human_approval_app_notebook"
USER_ID = "test_user_notebook_123"
SESSION_ID = "approval_session_notebook_456"
MODEL_NAME = "gemini-1.5-flash"
agentops.start_trace(trace_name=APP_NAME, tags=["google_adk", "notebook"])
```

## 3. Define Schemas

Pydantic models are used to define the structure of data for approval requests and decisions. This helps with validation and clarity.


```python
class ApprovalRequest(BaseModel):
    amount: float = Field(description="The amount requiring approval")
    reason: str = Field(description="The reason for the request")


class ApprovalDecision(BaseModel):
    decision: str = Field(description="The approval decision: 'approved' or 'rejected'")
    comments: str = Field(description="Additional comments from the approver")
```

## 4. External Approval Tool (with Human Interaction)

This tool now directly prompts the user for an approval decision. In a real-world scenario, this might involve sending a notification to an approver and waiting for their response through a UI or API.


```python
async def external_approval_tool(amount: float, reason: str) -> str:
    """
    Prompts for human approval and returns the decision as a JSON string.
    """
    print("üîî HUMAN APPROVAL REQUIRED:")
    print(f"   Amount: ${amount:,.2f}")
    print(f"   Reason: {reason}")
    decision = ""
    while decision.lower() not in ["approved", "rejected"]:
        decision = input("   Enter decision (approved/rejected): ").strip().lower()
        if decision.lower() not in ["approved", "rejected"]:
            print("   Invalid input. Please enter 'approved' or 'rejected'.")
    comments = input("   Enter comments (optional): ").strip()
    print(f"   Decision: {decision.upper()}")
    print(f"   Comments: {comments if comments else 'N/A'}")
    return json.dumps({"decision": decision, "comments": comments, "amount": amount, "reason": reason})


# Create the approval tool instance
approval_tool = FunctionTool(func=external_approval_tool)
```

## 5. Define Agents

We define three agents for our workflow:
1.  **`PrepareApprovalAgent`**: Extracts details from the user's request.
2.  **`RequestHumanApprovalAgent`**: Uses the `external_approval_tool` to get a decision.
3.  **`ProcessDecisionAgent`**: Processes the decision and formulates a final response.


```python
# Agent 1: Prepare the approval request
prepare_request = LlmAgent(
    model=MODEL_NAME,
    name="PrepareApprovalAgent",
    description="Extracts and prepares approval request details from user input",
    instruction="""You are an approval request preparation agent.
        Your task:
        1. Extract the amount and reason from the user's request
        2. Store these values in the session state with keys 'approval_amount' and 'approval_reason'
        3. Validate that both amount and reason are provided
        4. Respond with a summary of what will be submitted for approval
    If the user input is missing amount or reason, ask for clarification.
    """,
    output_key="request_prepared",
)

# Agent 2: Request human approval using the tool
request_approval = LlmAgent(
    model=MODEL_NAME,
    name="RequestHumanApprovalAgent",
    description="Calls the external approval system with prepared request details",
    instruction="""You are a human approval request agent.
        Your task:
        1. Get the 'approval_amount' and 'approval_reason' from the session state
        2. Use the external_approval_tool with these values
        3. Store the approval decision in session state with key 'human_decision'
        4. Respond with the approval status
    Always use the exact values from the session state for the tool call.
    """,
    tools=[approval_tool],
    output_key="approval_requested",
)

# Agent 3: Process the approval decision
process_decision = LlmAgent(
    model=MODEL_NAME,
    name="ProcessDecisionAgent",
    description="Processes the human approval decision and provides final response",
    instruction="""You are a decision processing agent.
        Your task:
        1. Check the 'human_decision' from session state
        2. Parse the approval decision JSON
        3. If approved: congratulate and provide next steps
        4. If rejected: explain the rejection and suggest alternatives
        5. Provide a clear, helpful final response to the user

    Be professional and helpful in your response.
    """,
    output_key="final_decision",
)
```

## 6. Create Sequential Workflow

Combine the agents into a sequential workflow. The `SequentialAgent` ensures that the sub-agents are executed in the specified order.


```python
approval_workflow = SequentialAgent(
    name="HumanApprovalWorkflowNotebook",
    description="Complete workflow for processing approval requests with human oversight",
    sub_agents=[prepare_request, request_approval, process_decision],
)
```

## 7. Session Management and Runner

Set up an in-memory session service and the workflow runner.


```python
session_service = InMemorySessionService()
# Create runner
workflow_runner = Runner(agent=approval_workflow, app_name=APP_NAME, session_service=session_service)
```

## 8. Helper Function to Run Workflow

This function encapsulates the logic to run the workflow for a given user request and session ID.


```python
async def run_approval_workflow_notebook(user_request: str, session_id: str):
    """Run the complete approval workflow with a user request in the notebook environment"""
    print(f"{'=' * 60}")
    print(f" Starting Approval Workflow for Session: {session_id}")
    print(f"{'=' * 60}")
    print(f"User Request: {user_request}")
    # Create user message
    user_content = types.Content(role="user", parts=[types.Part(text=user_request)])
    step_count = 0
    final_response = "No response received"
    # Run the workflow
    async for event in workflow_runner.run_async(
        user_id=USER_ID,
        session_id=session_id,
        new_message=user_content,
    ):
        if event.author and event.content:
            step_count += 1
            print(f"üìã Step {step_count} - {event.author}:")
            if event.content.parts:
                response_text = event.content.parts[0].text
                print(f"   {response_text}")
                if event.is_final_response():
                    final_response = response_text
    session = await session_service.get_session(
        app_name=APP_NAME,
        user_id=USER_ID,
        session_id=session_id,
    )
    print(f"{'=' * 60}")
    print(f"üìä Workflow Complete - Session State ({session_id}):")
    print(f"{'=' * 60}")
    for key, value in session.state.items():
        print(f"   {key}: {value}")
    print(f"üéØ Final Response: {final_response}")
    return final_response
```

## 9. Main Execution Logic

This cell contains the main logic to run the workflow with a few test cases. Each test case will run in its own session.


```python
async def main_notebook():
    test_requests = [
        "I need approval for $750 for team lunch and celebrations",
        "Please approve $3,000 for a conference ticket and travel expenses",
        "I need $12,000 approved for critical software licenses renewal",
    ]
    for i, request in enumerate(test_requests, 1):
        current_session_id = f"approval_session_notebook_{456 + i - 1}"
        # Create the session before running the workflow
        await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id)
        print(f"Created session: {current_session_id}")
        await run_approval_workflow_notebook(request, current_session_id)


try:
    asyncio.run(main_notebook())
    agentops.end_trace(end_state="Success")
except Exception as e:
    print(f"Error: {e}")
    agentops.end_trace(end_state="Error")
```


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>

## docs/v2/examples/anthropic.mdx

---
title: 'Anthropic Tools Example'
description: 'Understanding and using tools with the Anthropic SDK and AgentOps'
---
{/*  SOURCE_FILE: examples/anthropic/agentops-anthropic-understanding-tools.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/anthropic/agentops-anthropic-understanding-tools.ipynb'} target={'_blank'}>Github</a>_

# Anthropic Example for understanding Tools

Anthropic's tool returns are not as simple as getting a few strings! While this system is more complex than those before it, it's also simple enough to be used without problem once you understand how it works!

To get started, we will import Agentops and Anthropic.

## Installation
<CodeGroup>
  ```bash pip
  pip install agentops anthropic python-dotenv
  ```
  ```bash poetry
  poetry add agentops anthropic python-dotenv
  ```
  ```bash uv
  uv add agentops anthropic python-dotenv
  ```
</CodeGroup>

## Setup

Setup our generic default statements:
```python
import agentops
from anthropic import Anthropic
from dotenv import load_dotenv
import os
import random
```

And set our API keys.
```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")
os.environ["ANTHROPIC_API_KEY"] = os.getenv("ANTHROPIC_API_KEY", "your_anthropic_api_key_here")
```

Now let's set the client as Anthropic and make an AgentOps trace.
```python
agentops.init(tags=["anthropic-example-tool-tutorials", "agentops-example"])
client = Anthropic()
```

## Creating a Dummy Tool

Now to create a simple dummy tool! We are going to make a tool that will tell us about the demon infestation levels for 3 areas. From there, we will have VEGA, our AI determine the best place for the Doom Slayer to attack.

```python
locations = [
    {
        "Name": "Super Gore Nest",
        "Description": "A grotesque mass of demonic growth and organic structures infesting the ruins of an urban area on Earth. The Super Gore Nest serves as a massive, pulsating hive for Hell‚Äôs forces, complete with rivers of blood, twisted tendrils, and a dark, organic design that shows how deeply Hell has taken root in the city.",
    },
    {
        "Name": "Exultia",
        "Description": "An ancient, mystical world that holds the ruins of the Night Sentinels' kingdom, with gothic structures and arcane symbols throughout. This realm is filled with epic landscapes, medieval architecture, and hints of the powerful civilization that once defended against Hell‚Äôs forces.",
    },
    {
        "Name": "Cultist Base",
        "Description": "A grim fortress hidden within the icy mountains, where a fanatical cult worships demons. Filled with chilling sacrificial chambers, traps, and rituals, the Cultist Base is a hostile stronghold where Doom Slayer must confront the cult leaders aiding Hell's invasion of Earth.",
    },
    {
        "Name": "Taras Nabad",
        "Description": "A war-ravaged city on the homeworld of the Night Sentinels, showcasing grandiose, ancient architecture in the midst of destruction. Taras Nabad's sprawling structures and historical significance reveal glimpses into the Doom Slayer‚Äôs past and the once-thriving Sentinel civilization.",
    },
    {
        "Name": "Nekravol",
        "Description": "A hellish, industrial fortress where souls are processed into Argent energy. With conveyor belts moving the damned and a skyline dominated by fire and darkness, Nekravol is a nightmarish facility that powers Hell's armies and embodies the horrific machinery of Hell's cruelty.",
    },
    {
        "Name": "Urdak",
        "Description": "A surreal, high-tech realm that serves as the home of the angelic Maykrs. Urdak‚Äôs sleek, pristine architecture and ethereal ambiance sharply contrast with Hell‚Äôs brutal landscapes, yet this realm holds its own dark secrets and a critical role in Hell's invasion of Earth.",
    },
    {
        "Name": "UAC Base",
        "Description": "A futuristic military base on Earth controlled by the Union Aerospace Corporation (UAC), filled with high-tech weaponry and security systems. The UAC Base serves as a human foothold in the fight against Hell, though some within its ranks may have darker intentions.",
    },
]

combat_casualties = ["Nonexistent", "Low", "Medium", "High", "Extinction"]

missions = [
    "Locate and confront a key leader of Hell‚Äôs invasion forces.",
    "Clear out demonic infestations to secure a strategic foothold.",
    "Disrupt Hell's control over the area by eliminating critical targets.",
    "Enter a critical demonic stronghold to disrupt enemy operations.",
    "Locate and destroy the central power source to weaken enemy forces.",
    "Collect essential resources before the area becomes unstable.",
]
```

Now that that's done, we can make a function! We will generate three random missions and pass it off to the AI.
```python
def generate_missions():
    selectedmissions = []
    loop = 0

    while loop < 3:
        location = random.choice(locations)
        casualties = random.choice(combat_casualties)
        mission = random.choice(missions)
        final = (
            f"LocationName: {location['Name']}, "
            f"LocationInfo: {location['Description']}, "
            f"HumanCombatCasualties: {casualties}, "
            f"Mission: {mission}"
        )

        selectedmissions.append(final)
        loop += 1

    # Combine all mission strings into a single string with a separator (e.g., newline or comma)
    # Notebook had 'missions_string = "\n".join(missions)', which is a bug. 
    # Using 'selectedmissions' as it's the correct variable.
    missions_string = "\n".join(selectedmissions) 
    print(missions_string)
    return missions_string
```

```python
generate_missions()
```

## Constructing the Message Stream

Now to the real core of this; making our message stream! We create this as a function we can call later! I create examples since the LLM's context size can handle it (and it's generally good practice)!

We are also going to take several steps here; we must create an example of the tool being used as context. Next, we must add the generated lines to the messages list once done being generated. Finally, we will parse the text for the format we want and request another line.

Now we make a message! This time around we will skip making an initial message that has too much context, unlike in the past!
```python
# We make our history a separate block to be easier to add to later on! This is essentially our history
initial_messages = [
    {
        "role": "user",
        "content": "You are VEGA, the assistant to the DOOMGUY. Get three missions from the ship's API and tell me which mission is most to least important for quellng the forces of hell.  ",
    }
]
```

Now to construct a request!
```python
response = client.messages.create(
    max_tokens=5000,
    model="claude-3-7-sonnet-20250219", # Matched notebook
    tools=[
        {
            "name": "generate_missions",
            "description": "Retrieve three missions for the DoomSlayer",
            "input_schema": {"type": "object", "properties": {}, "required": []},
        }
    ],
    messages=initial_messages,
)

print(response.content)
```
Having trouble understanding this? The first block given is always AI dialogue! You can use `response.content[0].text` to get the AI's text! Let's try it below.
```python
message = response.content[0].text
print(message)
```
The code below finds the tool used!
```python
gen_mission_result = ""

# Print response content to see the data
# print(response.content) # Already printed in the cell above in notebook

# Assuming ToolUseBlock is at index 1 (matches notebook's direct access)
tool_use_block = response.content[1]

# Get the tool name and input
tool_name = tool_use_block.name
tool_input = tool_use_block.input

# We don't need to look to extract any inputs since we don't use any

# Check if the tool name is "generate_missions"
if tool_name == "generate_missions":
    # Call the function with the tool creator as an argument
    gen_mission_result = generate_missions()
```
Now we add these as context to the LLM through `initial_messages`!
```python
# Matches notebook cell 25 logic
initial_messages.append({"role": "assistant", "content": gen_mission_result})

initial_messages.append(
    {
        "role": "user",
        "content": "Based on these, which location should take priority and why?",
    }
)
```
And now to get a response!
```python
response = client.messages.create(
    max_tokens=5000,
    model="claude-3-7-sonnet-20250219", # Matched notebook
    tools=[
        {
            "name": "generate_missions",
            "description": "Retrieve three missions for the DoomSlayer",
            "input_schema": {"type": "object", "properties": {}, "required": []},
        }
    ],
    messages=initial_messages,
)

print(response)
```
Isolate again!
```python
message = response.content[0].text
print(message)
```
Hmmm, what if we wanted to include more tools and add inputs? Let's create two new functions to display this!

One will show the kind of demon we are facing, whereas another one will take our weapon input to determine what the best weapon chain to use is (You heard that right, we believe in quick weapon switches around these parts)
```python
demons = [
    {
        "Name": "Imp",
        "Description": "A fast, agile demon that hurls fireballs and uses its claws to tear apart its prey. Imps are commonly found in Hell‚Äôs army, notorious for their quickness and ability to climb walls, making them dangerous adversaries in any environment.",
    },
    {
        "Name": "Cacodemon",
        "Description": "A floating, spherical demon with a large mouth full of teeth and an ability to launch explosive projectiles. Cacodemons are often encountered in open areas, where their aerial agility and relentless attacks pose a constant threat.",
    },
    {
        "Name": "Hell Knight",
        "Description": "A towering, brutish demon with immense strength and durability. The Hell Knight is capable of charging at the Doom Slayer and delivering devastating melee attacks. Its tough hide makes it resistant to most forms of damage.",
    },
    {
        "Name": "Mancubus",
        "Description": "A grotesque, overweight demon that releases powerful fireballs from its massive arm cannons. Mancubus demons are slow-moving but dangerous due to their firepower and the ability to overwhelm enemies with their fiery onslaughts.",
    },
]


weapons = [
    {
        "Name": "Super Shotgun",
        "Description": "A powerful, double-barreled shotgun that delivers devastating close-range damage. Known for its sheer stopping power, the Super Shotgun can tear through enemies with ease, especially when equipped with the Meat Hook attachment, allowing for rapid mobility and devastating hits.",
    },
    {
        "Name": "Rocket Launcher",
        "Description": "A high-powered weapon that fires explosive rockets capable of dealing massive area damage. The Rocket Launcher is invaluable for taking down groups of enemies or dealing significant damage to larger demons, especially when upgraded with the Lock-On Burst mod.",
    },
    {
        "Name": "Chaingun",
        "Description": "A rapid-fire weapon that can unleash a torrent of bullets at a high rate of speed. The Chaingun is perfect for mowing down enemies and can be equipped with the Heat Blast mod, allowing for explosive energy rounds that can clear multiple enemies at once.",
    },
    {
        "Name": "BFG 9000",
        "Description": "One of the most iconic weapons in the *Doom* franchise, the BFG 9000 fires a massive energy beam that obliterates anything in its path. With its massive damage potential, the BFG 9000 is a game-changer, especially in dealing with large groups of enemies or the toughest foes.",
    },
    {
        "Name": "Ice Bomb",
        "Description": "A special grenade that freezes enemies in a wide area, giving the Doom Slayer a chance to deal with multiple foes at once. The Ice Bomb is effective for crowd control, allowing for easy Glory Kills or creating distance from overwhelming enemies.",
    },
]
```
Now we can keep the `initial_messages` from before actually! However let's change the context.
```python
# Matches notebook cell 33 (no conditional append of previous AI message)
initial_messages.append(
    {
        "role": "user",
        "content": "The first priority mission was selected. At the same time, scan for enemies and check inventory to determine the best combat strategy. You should use both tools at once.",
    }
)
```
And we of course make functions.
```python
def enemyscan(amount): # Matched notebook (no type hint)
    enemiesonscene = []
    loop = 0

    while loop < amount + 1: # Matched notebook
        scannedenemy = random.choice(demons)

        # Append name and description separately (Matches notebook)
        enemiesonscene.append(scannedenemy["Name"])
        enemiesonscene.append(scannedenemy["Description"])
        loop += 1

    # Combine all mission strings into a single string with a separator (e.g., newline or comma)
    enemies_string = "\n".join(enemiesonscene)
    print(enemies_string)
    return enemies_string
```
```python
enemyscan(5)
```
And now inventory.
```python
def inventoryscan():
    weapons_at_hand = []
    loop = 0

    while loop < 5: # Matched notebook (loop < 5)
        weapon = random.choice(weapons)

        # Append name and description separately (Matches notebook)
        weapons_at_hand.append(weapon["Name"])
        weapons_at_hand.append(weapon["Description"])
        loop += 1

    # Combine all mission strings into a single string with a separator (e.g., newline or comma)
    weapons_string = "\n".join(weapons_at_hand)
    print(weapons_string)
    return weapons_string
```
```python
inventoryscan()
```
With that, let's construct our new tools and run this!!
```python
response = client.messages.create(
    max_tokens=5000,
    model="claude-3-7-sonnet-20250219", # Matched notebook
    tools=[
        {
            "name": "enemyscan_tool",
            "description": "Retrieve a list of demons currently present in the area.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "amount": {
                        "type": "integer",
                        "description": "Number of enemies to scan.",
                    }
                },
                "required": ["amount"],
            },
        },
        {
            "name": "inventoryscan_tool",
            "description": "Retrieve a list of weapons the Doom Slayer has at hand.",
            "input_schema": {"type": "object", "properties": {}, "required": []},
        },
    ],
    messages=initial_messages,
)

print(response)
```
Display just the text.
```python
# Matches notebook cell 43
message = response.content[0].text
print(message)
```
```python
# Matches notebook cell 44
initial_messages.append({"role": "assistant", "content": f"{str(response.content[0].text)}"})
```
And now to get the information and put it all together! PLEASE read the comments!
```python
# Matches notebook cell 46 logic
inv_scan_res = ""
enemy_scan_res = ""

response_str = str(response) # As in notebook
tool_use_count = response_str.count(
    "ToolUseBlock"
)  # We know the ToolUseBlock will appear once for each tool request so we check how many time it appears

# You can use print(tool_use_count)to validate the ToolBlocks here if you wish

loop = 0

# We do this instead of a (foreach) because we need to skip the first block! This contains the message from the AI, not the tool! This way allows us to reference the item we want as easily as possible without complex logic needed!
while loop < tool_use_count:  # We will get the tools now
    tool_use_block = response.content[loop + 1]  # We start at 1 since 0 holds the AI mesage
    tool_name = tool_use_block.name
    tool_input = tool_use_block.input

    if tool_name == "inventoryscan_tool":
        # Call the inventoryscan function for inventoryscan_tool
        inv_scan_res = inventoryscan()
    elif tool_name == "enemyscan_tool":
        # Get the amount for enemyscan_tool
        amount = tool_input["amount"] # Direct access as in notebook
        # Call the enemyscan function with the amount
        enemy_scan_res = enemyscan(amount)

    loop = loop + 1 # Matches notebook
print(inv_scan_res)
print(enemy_scan_res)
```
And now we are basically done! We can give this to the AI and see what we get.
```python
# Matches notebook cell 48
initial_messages.append(
    {
        "role": "assistant", # Notebook uses assistant role here for tool results
        "content": f"Weapons Inventory Scan Result: {inv_scan_res}\nEnemy Scans Result: {enemy_scan_res}",
    }
)

initial_messages.append(
    {
        "role": "user",
        "content": "What is the combat plan for killing these demons? Based on the last message, tell me which demons to kill first, in which order and using which weapons as well as any sweakpoints.", # Typo 'sweakpoints' from notebook
    }
)
```
```python
# Matches notebook cell 49
response = client.messages.create(
    max_tokens=5000,
    model="claude-3-7-sonnet-20250219", # Matched notebook
    tools=[ 
        {
            "name": "enemyscan_tool",
            "description": "Retrieve a list of demons currently present in the area.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "amount": {
                        "type": "integer",
                        "description": "Number of enemies to scan.",
                    }
                },
                "required": ["amount"],
            },
        },
        {
            "name": "inventoryscan_tool",
            "description": "Retrieve a list of weapons the Doom Slayer has at hand.",
            "input_schema": {"type": "object", "properties": {}, "required": []},
        },
    ],
    messages=initial_messages,
)

message = response.content[0].text # Matches notebook
print(message)
```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## docs/v2/examples/crewai.mdx

---
title: 'CrewAI Example'
description: 'Using CrewAI with AgentOps to create a job posting'
---
{/*  SOURCE_FILE: examples/crewai/job_posting.ipynb  */}

_View Notebook on <a href={'https://github.com/AgentOps-AI/agentops/blob/main/examples/crewai/job_posting.ipynb'} target={'_blank'}>Github</a>_

# CrewAI Job Posting Example

This notebook demonstrates how to use CrewAI with AgentOps to automate the creation of a job posting.

First let's install the required packages.

## Installation
<CodeGroup>
  ```bash pip
  pip install -U 'crewai[tools]' agentops python-dotenv
  ```
  ```bash poetry
  poetry add crewai agentops python-dotenv
  # For tools, you might need to add them or specific extras:
  # poetry add crewai --extras tools 
  # or poetry add crewai[tools] depending on your poetry version
  ```
  ```bash uv
  uv add 'crewai[tools]' agentops python-dotenv
  ```
</CodeGroup>

## Setup
Then import them.
```python
from crewai import Crew, Agent, Task
from crewai_tools.tools import WebsiteSearchTool, SerperDevTool, FileReadTool # Corrected import in previous MDX version
import agentops
import os
from dotenv import load_dotenv
from textwrap import dedent # Added import
```

Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.

[Get an AgentOps API key](https://agentops.ai/settings/projects)

1. Create an environment variable in a `.env` file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...

2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!

```python
load_dotenv()
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY", "your_agentops_api_key_here")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY", "your_serper_api_key_here")
```

```python
# Initialize AgentOps client
agentops.init(auto_start_session=False)
```

## Define Agents and Tasks

```python
web_search_tool = WebsiteSearchTool()
serper_dev_tool = SerperDevTool()
file_read_tool = FileReadTool(
    file_path="job_description_example.md", # Ensure this file exists or handle appropriately
    description="A tool to read the job description example file.",
)


class Agents:
    def research_agent(self):
        return Agent(
            role="Research Analyst",
            goal="Analyze the company website and provided description to extract insights on culture, values, and specific needs.",
            tools=[web_search_tool, serper_dev_tool],
            backstory="Expert in analyzing company cultures and identifying key values and needs from various sources, including websites and brief descriptions.",
            verbose=True,
        )

    def writer_agent(self):
        return Agent(
            role="Job Description Writer",
            goal="Use insights from the Research Analyst to create a detailed, engaging, and enticing job posting.",
            tools=[web_search_tool, serper_dev_tool, file_read_tool],
            backstory="Skilled in crafting compelling job descriptions that resonate with the company's values and attract the right candidates.",
            verbose=True,
        )

    def review_agent(self):
        return Agent(
            role="Review and Editing Specialist",
            goal="Review the job posting for clarity, engagement, grammatical accuracy, and alignment with company values and refine it to ensure perfection.",
            tools=[web_search_tool, serper_dev_tool, file_read_tool],
            backstory="A meticulous editor with an eye for detail, ensuring every piece of content is clear, engaging, and grammatically perfect.",
            verbose=True,
        )
```

```python
class Tasks:
    def research_company_culture_task(self, agent, company_description, company_domain):
        return Task(
            description=dedent(
                f\"\"\"\\
								Analyze the provided company website and the hiring manager's company's domain {company_domain}, description: "{company_description}". Focus on understanding the company's culture, values, and mission. Identify unique selling points and specific projects or achievements highlighted on the site.
								Compile a report summarizing these insights, specifically how they can be leveraged in a job posting to attract the right candidates.\"\"\"
            ),
            expected_output=dedent(
                \"\"\"\\
								A comprehensive report detailing the company's culture, values, and mission, along with specific selling points relevant to the job role. Suggestions on incorporating these insights into the job posting should be included.\"\"\"
            ),
            agent=agent,
        )

    def research_role_requirements_task(self, agent, hiring_needs):
        return Task(
            description=dedent(
                f\"\"\"\\
								Based on the hiring manager's needs: "{hiring_needs}", identify the key skills, experiences, and qualities the ideal candidate should possess for the role. Consider the company's current projects, its competitive landscape, and industry trends. Prepare a list of recommended job requirements and qualifications that align with the company's needs and values.\"\"\"
            ),
            expected_output=dedent(
                \"\"\"\\
								A list of recommended skills, experiences, and qualities for the ideal candidate, aligned with the company's culture, ongoing projects, and the specific role's requirements.\"\"\"
            ),
            agent=agent,
        )

    def draft_job_posting_task(self, agent, company_description, hiring_needs, specific_benefits):
        return Task(
            description=dedent(
                f\"\"\"\\
								Draft a job posting for the role described by the hiring manager: "{hiring_needs}". Use the insights on "{company_description}" to start with a compelling introduction, followed by a detailed role description, responsibilities, and required skills and qualifications. Ensure the tone aligns with the company's culture and incorporate any unique benefits or opportunities offered by the company.
								Specific benefits: "{specific_benefits}"\"\"\"
            ),
            expected_output=dedent(
                \"\"\"\\
								A detailed, engaging job posting that includes an introduction, role description, responsibilities, requirements, and unique company benefits. The tone should resonate with the company's culture and values, aimed at attracting the right candidates.\"\"\"
            ),
            agent=agent,
        )

    def review_and_edit_job_posting_task(self, agent, hiring_needs):
        return Task(
            description=dedent(
                f\"\"\"\\
								Review the draft job posting for the role: "{hiring_needs}". Check for clarity, engagement, grammatical accuracy, and alignment with the company's culture and values. Edit and refine the content, ensuring it speaks directly to the desired candidates and accurately reflects the role's unique benefits and opportunities. Provide feedback for any necessary revisions.\"\"\"
            ),
            expected_output=dedent(
                \"\"\"\\
								A polished, error-free job posting that is clear, engaging, and perfectly aligned with the company's culture and values. Feedback on potential improvements and final approval for publishing. Formated in markdown.\"\"\"
            ),
            agent=agent,
            output_file="job_posting.md",
        )

    def industry_analysis_task(self, agent, company_domain, company_description):
        return Task(
            description=dedent(
                f\"\"\"\\
								Conduct an in-depth analysis of the industry related to the company's domain: "{company_domain}". Investigate current trends, challenges, and opportunities within the industry, utilizing market reports, recent developments, and expert opinions. Assess how these factors could impact the role being hired for and the overall attractiveness of the position to potential candidates.
								Consider how the company's position within this industry and its response to these trends could be leveraged to attract top talent. Include in your report how the role contributes to addressing industry challenges or seizing opportunities.\"\"\"
            ),
            expected_output=dedent(
                \"\"\"\\
								A detailed analysis report that identifies major industry trends, challenges, and opportunities relevant to the company's domain and the specific job role. This report should provide strategic insights on positioning the job role and the company as an attractive choice for potential candidates.\"\"\"
            ),
            agent=agent,
        )
```

## Run the Crew

```python
tracer = agentops.start_trace(trace_name="CrewAI Job Posting", tags=["crew-job-posting-example", "agentops-example"])

tasks = Tasks()
agents = Agents()

# Note: In a real scenario, you'd get these inputs dynamically.
# For this MDX example, we'll use placeholders.
company_description = "A fast-paced tech startup focused on AI-driven solutions for e-commerce."
company_domain = "agentops.ai" # Example domain
hiring_needs = "Senior Software Engineer with experience in Python, AI, and cloud platforms."
specific_benefits = "Competitive salary, stock options, remote work flexibility, and a great team culture."

# Create Agents
researcher_agent = agents.research_agent()
writer_agent = agents.writer_agent()
review_agent = agents.review_agent()

# Define Tasks for each agent
research_company_culture_task = tasks.research_company_culture_task(
    researcher_agent, company_description, company_domain
)
industry_analysis_task = tasks.industry_analysis_task(researcher_agent, company_domain, company_description)
research_role_requirements_task = tasks.research_role_requirements_task(researcher_agent, hiring_needs)
draft_job_posting_task = tasks.draft_job_posting_task(
    writer_agent, company_description, hiring_needs, specific_benefits
)
review_and_edit_job_posting_task = tasks.review_and_edit_job_posting_task(review_agent, hiring_needs)

# Instantiate the crew with a sequential process
crew = Crew(
    agents=[researcher_agent, writer_agent, review_agent],
    tasks=[
        research_company_culture_task,
        industry_analysis_task,
        research_role_requirements_task,
        draft_job_posting_task,
        review_and_edit_job_posting_task,
    ],
)


result = crew.kickoff()
print("Job Posting Creation Process Completed.")
print("Final Job Posting:")
print(result) # This will print the path to the output file job_posting.md

agentops.end_trace(tracer, end_state="Success")
```
The `job_posting.md` file will be created in the same directory where this script/notebook is run.

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>


## .github/CODE_OF_CONDUCT.md

# Code of Conduct

Be kind and be mindful. We're here to accelerate together.

All participants in our project's community are expected to follow this code of conduct, which covers our behavior as contributors, maintainers, and community members.

## Our Pledge


We pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

## .github/PULL_REQUEST_TEMPLATE.md

## üì• Pull Request

**üìò Description**
_Briefly describe the changes you've made._

**üß™ Testing**
_Describe the tests you performed to validate your changes._



## .github/CONTRIBUTING.md

## Open Source and Proprietary Components

The AgentOps SDK is open source, but the backend and UI are not. We are source available and also support on-prem/VPC hosting upon request.

That said, we love when community members contribute to our open source SDK-- we even mail over swag packs to developers who help out :)

## Contribution Guidelines

We use issues primarily to communicate our thoughts and plans to the community. However, not every issue is suitable for the open-source distribution, as some features may depend on non-open-source parts of our stack.

When looking for issues to work on look for following tags:
- `good first issue`
- `help wanted`

If you have feature requests or bug reports, please open an issue.

## Getting Started

The easiest way to contribute is to start a conversation with us. We highly recommend:

1. Joining our Discord community
2. Sending us a message through our contact form at [agentops.ai/contact](https://agentops.ai/contact)

We look forward to collaborating with you and appreciate your interest in improving AgentOps!

## agentops/client/http/README.md

# AgentOps HTTP Client Architecture

This directory contains the HTTP client architecture for the AgentOps SDK. The architecture follows a clean separation of concerns design principle.

## Components

### HttpClient

The `HttpClient` class provides low-level HTTP functionality:
- Connection pooling
- Retry logic
- Basic HTTP methods (GET, POST, PUT, DELETE)

### AuthManager

The `AuthManager` class handles authentication concerns:
- Token acquisition and storage
- Token refresh logic
- Authentication header preparation
- Thread-safe token operations

### HTTP Adapters

#### BaseHTTPAdapter
- Enhanced connection pooling and retry logic
- Used by the `HttpClient` for basic HTTP operations

#### AuthenticatedHttpAdapter
- Extends `BaseHTTPAdapter` with authentication capabilities
- Automatically adds authentication headers to requests
- Handles token refresh when authentication fails
- Can be mounted to any requests.Session

## Design Principles

1. **Separation of Concerns**
   - HTTP concerns are isolated from authentication concerns
   - Each component has a single responsibility

2. **Composition over Inheritance**
   - Components use composition rather than inheritance
   - `ApiClient` composes `HttpClient` and `AuthManager`

3. **Clear Interfaces**
   - Each component has a well-defined interface
   - Implementation details are hidden

4. **Dependency Flow**
   - Dependencies flow in one direction
   - Lower-level components (HTTP, Auth) don't depend on higher-level components

## Usage

### Basic API Client Usage

The HTTP client architecture is used by the `ApiClient` class, which provides a high-level interface for making API calls. Specific API versions (like `V3Client`) extend the `ApiClient` to provide version-specific functionality.

```python
# Example usage
from agentops.client.v3_client import V3Client

client = V3Client(endpoint="https://api.agentops.ai")
response = client.authenticated_request(
    method="get",
    path="/v3/some/endpoint",
    api_key="your-api-key"
)
```

### Using with External Libraries

The architecture also supports integration with external libraries that need authenticated HTTP sessions:

```python
# Example with OpenTelemetry exporter
from agentops.client.v3_client import V3Client
from agentops.client.exporters import AuthenticatedOTLPExporter

client = V3Client(endpoint="https://api.agentops.ai")
session = client.create_authenticated_session(api_key="your-api-key")

exporter = AuthenticatedOTLPExporter(
    endpoint="https://api.agentops.ai/v3/traces",
    api_client=client,
    api_key="your-api-key"
)
```


## agentops/instrumentation/OpenTelemetry.md

# OpenTelemetry Implementation Notes

This document outlines best practices and implementation details for OpenTelemetry in AgentOps instrumentations.

## Key Concepts

### Context Propagation

OpenTelemetry relies on proper context propagation to maintain parent-child relationships between spans. This is essential for:

- Creating accurate trace waterfalls in visualizations
- Ensuring all spans from the same logical operation share a trace ID
- Allowing proper querying and filtering of related operations

### Core Patterns

When implementing instrumentations that need to maintain context across different execution contexts:

1. **Store span contexts in dictionaries:**
   ```python
   # Use weakref dictionaries to avoid memory leaks
   self._span_contexts = weakref.WeakKeyDictionary()
   self._trace_root_contexts = weakref.WeakKeyDictionary()
   ```

2. **Create spans with explicit parent contexts:**
   ```python
   parent_context = self._get_parent_context(trace_obj)
   with trace.start_as_current_span(
       name=span_name,
       context=parent_context,
       kind=trace.SpanKind.CLIENT,
       attributes=attributes,
   ) as span:
       # Span operations here
       # Store the span's context for future reference
       context = trace.set_span_in_context(span)
       self._span_contexts[span_obj] = context
   ```

3. **Implement helper methods to retrieve appropriate parent contexts:**
   ```python
   def _get_parent_context(self, trace_obj):
       # Try to get the trace's root context if it exists
       if trace_obj in self._trace_root_contexts:
           return self._trace_root_contexts[trace_obj]
       
       # Otherwise, use the current context
       return context_api.context.get_current()
   ```

4. **Debug trace continuity:**
   ```python
   current_span = trace.get_current_span()
   span_context = current_span.get_span_context()
   trace_id = format_trace_id(span_context.trace_id)
   logging.debug(f"Current span trace ID: {trace_id}")
   ```

## Common Pitfalls

1. **Naming conflicts:** Avoid using `trace` as a parameter name when you're also importing the OpenTelemetry `trace` module
   ```python
   # Bad
   def on_trace_start(self, trace):
       # This will cause conflicts with the imported trace module
   
   # Good
   def on_trace_start(self, trace_obj):
       # No conflicts with OpenTelemetry's trace module
   ```

2. **Missing parent contexts:** Always explicitly provide parent contexts when available, don't rely on current context alone

3. **Memory leaks:** Use `weakref.WeakKeyDictionary()` for storing spans to allow garbage collection

4. **Lost context:** When calling async or callback functions, be sure to preserve and pass the context

## Testing Context Propagation

To verify proper context propagation:

1. Enable debug logging for trace IDs
2. Run a simple end-to-end test that generates multiple spans
3. Verify all spans share the same trace ID
4. Check that parent-child relationships are correctly established

```python
# Example debug logging
logging.debug(f"Span {span.name} has trace ID: {format_trace_id(span.get_span_context().trace_id)}")
```

## Timestamp Handling in OpenTelemetry

When working with OpenTelemetry spans and timestamps:

1. **Automatic Timestamp Tracking:** OpenTelemetry automatically tracks timestamps for spans. When a span is created with `tracer.start_span()` or `tracer.start_as_current_span()`, the start time is captured automatically. When `span.end()` is called, the end time is recorded.

2. **No Manual Timestamp Setting Required:** The standard instrumentation pattern does not require manually setting timestamp attributes on spans. Instead, OpenTelemetry handles this internally through the SpanProcessor and Exporter classes.

3. **Timestamp Representation:** In the OpenTelemetry data model, timestamps are stored as nanoseconds since the Unix epoch (January 1, 1970).

4. **Serialization Responsibility:** The serialization of timestamps from OTel spans to output formats like JSON is handled by the Exporter components. If timestamps aren't appearing correctly in output APIs, the issue is likely in the API exporter, not in the span creation code.

5. **Debugging Timestamps:** To debug timestamp issues, verify that spans are properly starting and ending, rather than manually setting timestamp attributes:

```python
# Good pattern - timestamps handled by OpenTelemetry automatically
with tracer.start_as_current_span("my_operation") as span:
    # Do work
    pass  # span.end() is called automatically
```

Note: If timestamps are missing in API output (e.g., empty "start_time" fields), focus on fixes in the exporter and serialization layer, not by manually tracking timestamps in instrumentation code.

## Attributes in OpenTelemetry

When working with span attributes in OpenTelemetry:

1. **Root Attributes Node:** The root `attributes` object in the API output JSON should always be empty. This is by design. All attribute data should be stored in the `span_attributes` object.

2. **Span Attributes:** The `span_attributes` object is where all user-defined and semantic attribute data should be stored. This allows for a structured, hierarchical representation of attributes.

3. **Structure Difference:** While the root `attributes` appears as an empty object in the API output, this is normal and expected. Do not attempt to populate this object directly or duplicate data from `span_attributes` into it.

4. **Setting Attributes:** Always set span attributes using the semantic conventions defined in the `agentops/semconv` module:

```python
from agentops.semconv import agent

# Good pattern - using semantic conventions
span.set_attribute(agent.AGENT_NAME, "My Agent")
```

## agentops/instrumentation/README.md

# AgentOps Instrumentation

This package provides OpenTelemetry instrumentation for various LLM providers and related services.

## Available Instrumentors

- **OpenAI** (`v0.27.0+` and `v1.0.0+`)
- **Anthropic** (`v0.7.0+`)
- **Google GenAI** (`v0.1.0+`)
- **IBM WatsonX AI** (`v0.1.0+`)
- **CrewAI** (`v0.56.0+`)
- **AG2/AutoGen** (`v0.3.2+`)
- **Google ADK** (`v0.1.0+`)
- **Agno** (`v0.0.1+`)
- **Mem0** (`v0.1.0+`)
- **smolagents** (`v0.1.0+`)

## Common Module Usage

The `agentops.instrumentation.common` module provides shared utilities for creating instrumentations:

### Base Instrumentor

Use `CommonInstrumentor` for creating new instrumentations:

```python
from agentops.instrumentation.common import CommonInstrumentor, InstrumentorConfig, WrapConfig

class MyInstrumentor(CommonInstrumentor):
    def __init__(self):
        config = InstrumentorConfig(
            library_name="my-library",
            library_version="1.0.0",
            wrapped_methods=[
                WrapConfig(
                    trace_name="my.method",
                    package="my_library.module",
                    class_name="MyClass",
                    method_name="my_method",
                    handler=my_attribute_handler
                )
            ],
            dependencies=["my-library >= 1.0.0"]
        )
        super().__init__(config)
```

### Attribute Handlers

Create attribute handlers to extract data from method calls:

```python
from agentops.instrumentation.common import AttributeMap

def my_attribute_handler(args=None, kwargs=None, return_value=None) -> AttributeMap:
    attributes = {}
    
    if kwargs and "model" in kwargs:
        attributes["llm.request.model"] = kwargs["model"]
    
    if return_value and hasattr(return_value, "usage"):
        attributes["llm.usage.total_tokens"] = return_value.usage.total_tokens
    
    return attributes
```

### Span Management

Use the span management utilities for consistent span creation:

```python
from agentops.instrumentation.common import create_span, SpanAttributeManager

# Create an attribute manager
attr_manager = SpanAttributeManager(service_name="my-service")

# Use the create_span context manager
with create_span(
    tracer,
    "my.operation",
    attributes={"my.attribute": "value"},
    attribute_manager=attr_manager
) as span:
    # Your operation code here
    pass
```

### Token Counting

Use the token counting utilities for consistent token usage extraction:

```python
from agentops.instrumentation.common import TokenUsageExtractor, set_token_usage_attributes

# Extract token usage from a response
usage = TokenUsageExtractor.extract_from_response(response)

# Set token usage attributes on a span
set_token_usage_attributes(span, response)
```

### Streaming Support

Use streaming utilities for handling streaming responses:

```python
from agentops.instrumentation.common import create_stream_wrapper_factory, StreamingResponseHandler

# Create a stream wrapper factory
wrapper = create_stream_wrapper_factory(
    tracer,
    "my.stream",
    extract_chunk_content=StreamingResponseHandler.extract_generic_chunk_content,
    initial_attributes={"stream.type": "text"}
)

# Apply to streaming methods
wrap_function_wrapper("my_module", "stream_method", wrapper)
```

### Metrics

Use standard metrics for consistency across instrumentations:

```python
from agentops.instrumentation.common import StandardMetrics, MetricsRecorder

# Create standard metrics
metrics = StandardMetrics.create_standard_metrics(meter)

# Use the metrics recorder
recorder = MetricsRecorder(metrics)
recorder.record_token_usage(prompt_tokens=100, completion_tokens=50)
recorder.record_duration(1.5)
```

## Creating a New Instrumentor

1. Create a new directory under `agentops/instrumentation/` for your provider
2. Create an `__init__.py` file with version information
3. Create an `instrumentor.py` file extending `CommonInstrumentor`
4. Create attribute handlers in an `attributes/` subdirectory
5. Add your instrumentor to the main `__init__.py` configuration

Example structure:
```
agentops/instrumentation/
‚îú‚îÄ‚îÄ my_provider/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ instrumentor.py
‚îÇ   ‚îî‚îÄ‚îÄ attributes/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ handlers.py
```

## Best Practices

1. **Use Common Utilities**: Leverage the common module for consistency
2. **Follow Semantic Conventions**: Use attributes from `agentops.semconv`
3. **Handle Errors Gracefully**: Wrap operations in try-except blocks
4. **Support Async**: Provide both sync and async method wrapping
5. **Document Attributes**: Comment on what attributes are captured
6. **Test Thoroughly**: Write unit tests for your instrumentor

## Examples

See the `examples/` directory for usage examples of each instrumentor.


## agentops/instrumentation/common/README.md

# AgentOps Instrumentation Common Module

The `agentops.instrumentation.common` module provides shared utilities for OpenTelemetry instrumentation across different LLM service providers.

## Core Components

### Attribute Handler Example

Attribute handlers extract data from method inputs and outputs:

```python
from typing import Optional, Any, Tuple, Dict
from agentops.instrumentation.common.attributes import AttributeMap
from agentops.semconv import SpanAttributes

def my_attribute_handler(args: Optional[Tuple] = None, kwargs: Optional[Dict] = None, return_value: Optional[Any] = None) -> AttributeMap:
    attributes = {}
    
    # Extract attributes from kwargs (method inputs)
    if kwargs:
        if "model" in kwargs:
            attributes[SpanAttributes.MODEL_NAME] = kwargs["model"]
        # ...
    
    # Extract attributes from return value (method outputs)
    if return_value:
        if hasattr(return_value, "model"):
            attributes[SpanAttributes.LLM_RESPONSE_MODEL] = return_value.model
        # ...
    
    return attributes
```

### `WrapConfig` Class

Config object defining how a method should be wrapped:

```python
from agentops.instrumentation.common.wrappers import WrapConfig
from opentelemetry.trace import SpanKind

config = WrapConfig(
    trace_name="llm.completion",    # Name that will appear in trace spans
    package="openai.resources",     # Path to the module containing the class
    class_name="Completions",       # Name of the class containing the method
    method_name="create",           # Name of the method to wrap
    handler=my_attribute_handler,   # Function that extracts attributes
    span_kind=SpanKind.CLIENT       # Type of span to create
)
```

### Wrapping/Unwrapping Methods

```python
from opentelemetry.trace import get_tracer
from agentops.instrumentation.common.wrappers import wrap, unwrap

# Create a tracer and wrap a method
tracer = get_tracer("openai", "0.0.0")
wrap(config, tracer)

# Later, unwrap the method
unwrap(config)
```



## agentops/instrumentation/providers/google_genai/README.md

# Google Generative AI (Gemini) Instrumentation

This module provides OpenTelemetry instrumentation for Google's Generative AI (Gemini) API. The instrumentation allows you to trace all API calls made using the `google-genai` Python SDK, capturing:

- Model parameters (temperature, max_tokens, etc.)
- Prompt content (with privacy controls)
- Response text and token usage
- Streaming metrics
- Token counting
- Performance and error data

## Supported Features

The instrumentation covers all major API methods including:

### Client-Based API
- `client.models.generate_content`
- `client.models.generate_content_stream`
- `client.models.count_tokens`
- `client.models.compute_tokens`
- And their corresponding async variants

## Metrics

The instrumentation captures the following metrics:

- Input tokens used
- Output tokens generated
- Total tokens consumed
- Operation duration
- Exception counts

These metrics are available as OpenTelemetry span attributes and can be viewed in your observability platform of choice when properly configured.

## agentops/instrumentation/agentic/crewai/NOTICE.md

This package contains code derived from the OpenLLMetry project, which is licensed under the Apache License, Version 2.0.

Original repository: https://github.com/traceloop/openllmetry

Copyright notice from the original project:
Copyright (c) Traceloop (https://traceloop.com)

The Apache 2.0 license can be found in the LICENSE file in this directory.

This code has been modified and adapted for use in the AgentOps project. 

## agentops/instrumentation/agentic/openai_agents/TRACING_API.md

# OpenAI Agents Tracing API Integration

This document provides an overview of how AgentOps integrates with the OpenAI Agents SDK tracing system.

## OpenAI Agents Tracing API Overview

The OpenAI Agents SDK provides a comprehensive tracing system that allows you to monitor and instrument agent activities. AgentOps integrates with this system to capture and forward trace data to its backend.

## Core Integration Methods

### 1. `add_trace_processor(processor)`

The main integration point that allows external systems like AgentOps to receive trace events:

```python
from agents import add_trace_processor
from agentops.instrumentation.openai_agents.processor import OpenAIAgentsProcessor

processor = OpenAIAgentsProcessor()
add_trace_processor(processor)
```

### 2. `set_trace_processors(processors)`

Replaces all current processors with a new list:

```python
from agents import set_trace_processors
set_trace_processors([my_processor1, my_processor2])
```

### 3. `set_tracing_disabled(disabled)`

Globally enables/disables tracing:

```python
from agents import set_tracing_disabled
set_tracing_disabled(True)  # Disable tracing
```

### 4. `set_tracing_export_api_key(api_key)`

Sets the API key for the backend exporter:

```python
from agents import set_tracing_export_api_key
set_tracing_export_api_key("your-api-key")
```

## Span Creation Methods

The SDK provides specialized methods for creating different types of spans:

1. **`agent_span(name, handoffs, tools, output_type, ...)`**
   - Creates spans for agent operations
   - Tracks agent name, available tools, potential handoffs

2. **`function_span(name, input, output, ...)`**
   - Creates spans for function/tool calls
   - Records function name, input arguments, and results

3. **`generation_span(input, output, model, model_config, usage, ...)`**
   - Creates spans for LLM generations
   - Records prompts, completions, model details, and token usage

4. **`response_span(response, ...)`**
   - Lightweight span for capturing OpenAI API response metadata

5. **`handoff_span(from_agent, to_agent, ...)`**
   - Tracks agent-to-agent handoffs

6. **`guardrail_span(name, triggered, ...)`**
   - Records guardrail evaluations

7. **`custom_span(name, data, ...)`**
   - Creates user-defined spans with arbitrary data

## Trace and Context Management

1. **`trace(workflow_name, trace_id, group_id, metadata, ...)`**
   - Creates and manages a trace context
   - Groups related spans into a logical trace/session

2. **`get_current_span()`**
   - Returns the current active span

3. **`get_current_trace()`**
   - Returns the current active trace

## How AgentOps Implements Integration

AgentOps integrates with this API through:

1. The `OpenAIAgentsProcessor` class that implements the `TracingProcessor` interface
2. The `create_span` context manager that ensures proper parent-child relationships between spans
3. The `AgentsInstrumentor` which registers the processor and adds additional instrumentation

This integration allows AgentOps to capture detailed information about agent execution, including:
- Agent operations and tool usage
- LLM requests and responses 
- Token usage metrics
- Error information
- Agent-to-agent handoffs

### Trace Context Propagation

Our implementation ensures proper parent-child relationships between spans through:

1. **Context Manager Pattern**: Using `start_as_current_span()` to maintain the OpenTelemetry span context
2. **Parent Reference Tracking**: Storing parent span relationships and using them to create proper span hierarchies
3. **Trace Correlation Attributes**: Adding consistent attributes to help with querying:
   - `agentops.original_trace_id`: Original trace ID from the Agents SDK
   - `agentops.original_span_id`: Original span ID from the Agents SDK
   - `agentops.parent_span_id`: Parent span ID for child spans
   - `agentops.trace_hash`: Consistent hash based on the original trace ID
   - `agentops.is_root_span`: "true" for spans without a parent

When querying spans for analysis:
1. Group spans by `agentops.original_trace_id` to find all spans in the same trace
2. Use `agentops.parent_span_id` to reconstruct the parent-child hierarchy

## Span Data Types

Several specialized span data types exist in the OpenAI Agents SDK to capture different operations:

- **AgentSpanData**: Captures agent execution data
- **FunctionSpanData**: Records tool/function calls
- **GenerationSpanData**: Records LLM generation details
- **ResponseSpanData**: Captures model response information
- **HandoffSpanData**: Tracks agent-to-agent handoffs
- **GuardrailSpanData**: Records guardrail evaluations
- **CustomSpanData**: For user-defined spans

## Processor Interface

The `TracingProcessor` interface defines methods processors must implement:
- `on_trace_start`: Called when a trace begins
- `on_trace_end`: Called when a trace ends
- `on_span_start`: Called when a span begins
- `on_span_end`: Called when a span completes
- `shutdown`: Called during application shutdown
- `force_flush`: Forces immediate processing of pending spans

The processor receives events from OpenAI Agents SDK's tracing system through these callback methods, translates them to OpenTelemetry spans, and sends them to the AgentOps backend for analysis and visualization.

## agentops/instrumentation/agentic/openai_agents/README.md

# OpenAI Agents SDK Instrumentation

This module provides automatic instrumentation for the OpenAI Agents SDK, adding telemetry that follows OpenTelemetry semantic conventions for Generative AI systems.

## Architecture Overview

The OpenAI Agents SDK instrumentor works by:

1. Intercepting the Agents SDK's trace processor interface to capture Agent, Function, Generation, and other span types
2. Monkey-patching the Agents SDK `Runner` class to capture the full execution lifecycle, including streaming operations
3. Converting all captured data to OpenTelemetry spans and metrics following semantic conventions

The instrumentation is organized into several key components:

1. **Instrumentor (`instrumentor.py`)**: The entry point that patches the Agents SDK and configures trace capture
2. **Processor (`processor.py`)**: Receives events from the SDK and prepares them for export
3. **Exporter (`exporter.py`)**: Converts SDK spans to OpenTelemetry spans and exports them
4. **Attributes Module (`attributes/`)**: Specialized modules for extracting and formatting span attributes

## Attribute Processing Modules

The attribute modules extract and format OpenTelemetry-compatible attributes from span data:

- **Common (`attributes/common.py`)**: Core attribute extraction functions for all span types and utility functions
- **Completion (`attributes/completion.py`)**: Handles different completion content formats (Chat Completions API, Response API, Agents SDK) 
- **Model (`attributes/model.py`)**: Extracts model information and parameters
- **Tokens (`attributes/tokens.py`)**: Processes token usage data and metrics
- **Response (`attributes/response.py`)**: Handles interpretation of Response API objects

Each getter function in these modules is focused on a single responsibility and does not modify global state. Functions are designed to be composable, allowing different attribute types to be combined as needed in the exporter.

## Span Types

The instrumentor captures the following span types:

- **Trace**: The root span representing an entire agent workflow execution
  - Created using `get_base_trace_attributes()` to initialize with standard fields
  - Captures workflow name, trace ID, and workflow-level metadata

- **Agent**: Represents an agent's execution lifecycle
  - Processed using `get_agent_span_attributes()` with `AGENT_SPAN_ATTRIBUTES` mapping
  - Uses `SpanKind.CONSUMER` to indicate an agent receiving a request
  - Captures agent name, input, output, tools, and other metadata

- **Function**: Represents a tool/function call
  - Processed using `get_function_span_attributes()` with `FUNCTION_SPAN_ATTRIBUTES` mapping
  - Uses `SpanKind.CLIENT` to indicate an outbound call to a function
  - Captures function name, input arguments, output results, and from_agent information

- **Generation**: Captures details of model generation
  - Processed using `get_generation_span_attributes()` with `GENERATION_SPAN_ATTRIBUTES` mapping
  - Uses `SpanKind.CLIENT` to indicate an outbound call to an LLM
  - Captures model name, configuration, usage statistics, and response content

- **Response**: Lightweight span for tracking model response data
  - Processed using `get_response_span_attributes()` with `RESPONSE_SPAN_ATTRIBUTES` mapping
  - Extracts response content and metadata from different API formats

- **Handoff**: Represents control transfer between agents
  - Processed using `get_handoff_span_attributes()` with `HANDOFF_SPAN_ATTRIBUTES` mapping
  - Tracks from_agent and to_agent information

## Span Lifecycle Management

The exporter (`exporter.py`) handles the full span lifecycle:

1. **Start Events**:
   - Create spans but DO NOT END them
   - Store span references in tracking dictionaries
   - Use OpenTelemetry's start_span to control when spans end
   - Leave status as UNSET to indicate in-progress

2. **End Events**:
   - Look up existing span by ID in tracking dictionaries
   - If found and not ended:
     - Update span with all final attributes
     - Set status to OK or ERROR based on task outcome
     - End the span manually
   - If not found or already ended:
     - Create a new complete span with all data
     - End it immediately

3. **Error Handling**:
   - Check if spans are already ended before attempting updates
   - Provide informative log messages about span lifecycle
   - Properly clean up tracking resources

This approach is essential because:
- Agents SDK sends separate start and end events for each task
- We need to maintain a single span for the entire task lifecycle to get accurate timing
- Final data (outputs, token usage, etc.) is only available at the end event
- We want to avoid creating duplicate spans for the same task
- Spans must be properly created and ended to avoid leaks

The span lifecycle management ensures spans have:
- Accurate start and end times (preserving the actual task duration)
- Complete attribute data from both start and end events
- Proper status reflecting task completion
- All final outputs, errors, and metrics
- Clean resource management with no memory leaks

## Key Design Patterns

### Semantic Conventions

All attribute names follow the OpenTelemetry semantic conventions defined in `agentops.semconv`:

```python
# Using constants from semconv module
attributes[CoreAttributes.TRACE_ID] = trace_id
attributes[WorkflowAttributes.WORKFLOW_NAME] = trace.name
attributes[SpanAttributes.LLM_SYSTEM] = "openai"
attributes[MessageAttributes.COMPLETION_CONTENT.format(i=0)] = content
```

### Target ‚Üí Source Attribute Mapping

We use a consistent pattern for attribute extraction with typed mapping dictionaries:

```python
# Attribute mapping example
AGENT_SPAN_ATTRIBUTES: AttributeMap = {
    # target_attribute: source_attribute
    AgentAttributes.AGENT_NAME: "name",
    WorkflowAttributes.WORKFLOW_INPUT: "input",
    WorkflowAttributes.FINAL_OUTPUT: "output",
    # ...
}
```

### Structured Attribute Handling

- Always use MessageAttributes semantic conventions for content and tool calls
- For chat completions, use MessageAttributes.COMPLETION_CONTENT.format(i=0) 
- For tool calls, use MessageAttributes.COMPLETION_TOOL_CALL_NAME.format(i=0, j=0), etc.
- Never try to combine or aggregate contents into a single attribute
- Each message component should have its own properly formatted attribute
- This ensures proper display in OpenTelemetry backends and dashboards

### Serialization Rules

1. We do not serialize data structures arbitrarily; everything has a semantic convention
2. Span attributes should use semantic conventions and avoid complex serialized structures
3. Keep all string data in its original form - do not parse JSON within strings
4. If a function has JSON attributes for its arguments, do not parse that JSON - keep as string
5. If a completion or response body text/content contains JSON, keep it as a string
7. Function arguments and tool call arguments should remain in their raw string form

### Critical Notes for Attribute Handling

- NEVER manually set the root completion attributes (`SpanAttributes.LLM_COMPLETIONS` or "gen_ai.completion")
- Let OpenTelemetry backend derive these values from the detailed attributes
- Setting root completion attributes creates duplication and inconsistency
- Tests should verify attribute existence using MessageAttributes constants
- Do not check for the presence of SpanAttributes.LLM_COMPLETIONS
- Verify individual content/tool attributes instead of root attributes

## agentops/instrumentation/agentic/openai_agents/SPANS.md

# OpenAI Agents Spans and Traces

This document describes the span types, naming conventions, and attribute patterns used by the AgentOps instrumentation for the OpenAI Agents SDK.

## Span Types and Classes

The instrumentation works with these specific span data classes:

1. **AgentSpanData**: Represents a single agent's operation
   - Has attributes for name, input, output, tools, and handoffs
   - Processed by `get_agent_span_attributes()` using `AGENT_SPAN_ATTRIBUTES` mapping

2. **FunctionSpanData**: Represents tool or function calls
   - Has attributes for name, input, output, and from_agent
   - Processed by `get_function_span_attributes()` using `FUNCTION_SPAN_ATTRIBUTES` mapping

3. **GenerationSpanData**: Represents LLM model invocations
   - Has attributes for model, input, output, tools, and from_agent
   - Processed by `get_generation_span_attributes()` using `GENERATION_SPAN_ATTRIBUTES` mapping

4. **HandoffSpanData**: Represents agent-to-agent handoffs
   - Has attributes for from_agent and to_agent
   - Processed by `get_handoff_span_attributes()` using `HANDOFF_SPAN_ATTRIBUTES` mapping

5. **ResponseSpanData**: Represents model response data
   - Has attributes for input and response
   - Processed by `get_response_span_attributes()` using `RESPONSE_SPAN_ATTRIBUTES` mapping

## Span Naming Conventions

Spans are named according to these conventions:

1. **Trace Spans**: `agents.trace.{workflow_name}`
   - Represents the entire agent workflow
   - Named after the workflow or trace name

2. **Agent Spans**: `agents.agent`
   - Represents a single agent's operation
   - Uses `SpanKind.CONSUMER`

3. **Function Spans**: `agents.function`
   - Represents tool or function calls
   - Uses `SpanKind.CLIENT`

4. **Generation Spans**: `agents.generation`
   - Represents LLM model invocations
   - Uses `SpanKind.CLIENT`

5. **Handoff Spans**: `agents.handoff`
   - Represents agent-to-agent handoffs
   - Uses `SpanKind.INTERNAL`

6. **Response Spans**: `agents.response`
   - Represents model response data
   - Uses `SpanKind.CLIENT`

## Span Hierarchy

The spans follow a parent-child relationship that reflects the execution flow:

```
agents.trace.{workflow_name}
  ‚îî‚îÄ‚îÄ agents.agent
      ‚îú‚îÄ‚îÄ agents.generation
      ‚îú‚îÄ‚îÄ agents.function
      ‚îú‚îÄ‚îÄ agents.response
      ‚îî‚îÄ‚îÄ agents.handoff
```

## Semantic Conventions and Attributes

Each span type has attributes following OpenTelemetry semantic conventions:

### Common Attributes (All Spans)

- `trace.id`: OpenTelemetry trace ID
- `span.id`: OpenTelemetry span ID
- `parent.id`: Parent span ID (if applicable)
- `instrumentation.name`: "agentops"
- `instrumentation.version`: AgentOps library version
- `instrumentation.library.name`: "openai_agents"
- `instrumentation.library.version`: Library version

### Workflow and Trace Attributes

- `workflow.name`: Name of the workflow or trace
- `workflow.step_type`: "trace" for trace spans
- `workflow.input`: Input to the workflow
- `workflow.final_output`: Final output from the workflow

### Agent Attributes

- `agent.name`: The name of the agent
- `agent.tools`: Comma-separated list of available tools
- `agent.handoffs`: Comma-separated list of handoff targets
- `agent.from`: Source agent in handoffs (used in HandoffSpanData)
- `agent.to`: Destination agent in handoffs (used in HandoffSpanData)

### LLM Attributes

- `gen_ai.system`: "openai" for all OpenAI spans
- `gen_ai.request.model`: Model used for generation
- `gen_ai.response.model`: Model that provided the response
- `gen_ai.prompt`: Input prompt or message
- `gen_ai.completion.0.role`: Role of the completion message (usually "assistant")
- `gen_ai.completion.0.content`: Content of the completion message
- `gen_ai.tool_call.0.0.name`: Name of the tool called (if applicable)
- `gen_ai.tool_call.0.0.arguments`: Arguments for the tool call (if applicable)

### Token Usage Attributes

- `gen_ai.usage.prompt_tokens`: Number of input tokens
- `gen_ai.usage.completion_tokens`: Number of output tokens
- `gen_ai.usage.total_tokens`: Total number of tokens
- `gen_ai.usage.reasoning_tokens`: Tokens used for reasoning (Response API)
- `gen_ai.usage.cache_read.input_tokens`: Cached input tokens (Response API)

## Span Lifecycle Management

The exporter handles span lifecycle with these stages:

1. **Start Events**:
   - Create spans with `start_span()` (not using context manager)
   - Store span references in tracking dictionaries
   - Leave status as UNSET to indicate in-progress

2. **End Events**:
   - Look up existing span by ID
   - Update with final attributes
   - Set appropriate status and end the span manually

3. **Error Handling**:
   - Set status to ERROR for spans with errors
   - Add error type and message as attributes
   - Record exceptions with `record_exception()`

## OpenTelemetry Span Kinds

Span kinds map to OpenTelemetry concepts:

- `AgentSpanData` ‚Üí `SpanKind.CONSUMER`
- `FunctionSpanData` ‚Üí `SpanKind.CLIENT`
- `GenerationSpanData` ‚Üí `SpanKind.CLIENT`
- `ResponseSpanData` ‚Üí `SpanKind.CLIENT`
- `HandoffSpanData` ‚Üí `SpanKind.INTERNAL`

## agentops/instrumentation/agentic/smolagents/README.md

# SmoLAgents Instrumentation

This module provides OpenTelemetry instrumentation for the SmoLAgents framework. It captures telemetry data from model operations, agent executions, and tool usage.

## Features

- Model operation tracking
  - Text generation
  - Token usage
  - Streaming responses
  - Latency metrics

- Agent execution monitoring
  - Step-by-step execution
  - Planning phases
  - Tool usage
  - Execution time

- Tool usage analytics
  - Tool call patterns
  - Success/failure rates
  - Execution time
  - Error tracking

## Usage

```python
from agentops import init
from agentops.instrumentation.smolagents import SmolagentsInstrumentor

# Initialize AgentOps with your API key
init(api_key="your-api-key")

# The instrumentation will be automatically activated
# All SmoLAgents operations will now be tracked
```

## Metrics Collected

1. Token Usage
   - Input tokens
   - Output tokens
   - Total tokens per operation

2. Timing Metrics
   - Operation duration
   - Time to first token (streaming)
   - Tool execution time
   - Planning phase duration

3. Agent Metrics
   - Step counts
   - Planning steps
   - Tools used
   - Success/failure rates

4. Error Tracking
   - Generation errors
   - Tool execution errors
   - Parsing errors

## Architecture

The instrumentation is built on OpenTelemetry and follows the same pattern as other AgentOps instrumentors:

1. Attribute Extractors
   - Model attributes
   - Agent attributes
   - Tool call attributes

2. Wrappers
   - Method wrappers for sync operations
   - Stream wrappers for async operations
   - Context propagation handling

3. Metrics
   - Histograms for distributions
   - Counters for events
   - Custom attributes for filtering

## Contributing

When adding new features or modifying existing ones:

1. Follow the established pattern for attribute extraction
2. Maintain context propagation
3. Add appropriate error handling
4. Update tests and documentation 

## agentops/semconv/README.md

# OpenTelemetry Semantic Conventions for Generative AI Systems

This module provides semantic conventions for telemetry data in AI and LLM systems, following OpenTelemetry GenAI conventions where applicable.

## Core Conventions

### Agent Attributes (`agent.py`)
```python
from agentops.semconv import AgentAttributes

AgentAttributes.AGENT_NAME     # Agent name
AgentAttributes.AGENT_ROLE     # Agent role/type
AgentAttributes.AGENT_ID       # Unique agent identifier
```

### Tool Attributes (`tool.py`)
```python
from agentops.semconv import ToolAttributes, ToolStatus

ToolAttributes.TOOL_NAME        # Tool name
ToolAttributes.TOOL_PARAMETERS  # Tool input parameters
ToolAttributes.TOOL_RESULT      # Tool execution result
ToolAttributes.TOOL_STATUS      # Tool execution status

# Tool status values
ToolStatus.EXECUTING   # Tool is executing
ToolStatus.SUCCEEDED   # Tool completed successfully
ToolStatus.FAILED      # Tool execution failed
```

### Workflow Attributes (`workflow.py`)
```python
from agentops.semconv import WorkflowAttributes

WorkflowAttributes.WORKFLOW_NAME      # Workflow name
WorkflowAttributes.WORKFLOW_TYPE      # Workflow type
WorkflowAttributes.WORKFLOW_STEP_NAME # Step name
WorkflowAttributes.WORKFLOW_STEP_STATUS # Step status
```

### LLM/GenAI Attributes (`span_attributes.py`)
Following OpenTelemetry GenAI conventions:

```python
from agentops.semconv import SpanAttributes

# Request attributes
SpanAttributes.LLM_REQUEST_MODEL        # Model name (e.g., "gpt-4")
SpanAttributes.LLM_REQUEST_TEMPERATURE  # Temperature setting
SpanAttributes.LLM_REQUEST_MAX_TOKENS   # Max tokens to generate

# Response attributes  
SpanAttributes.LLM_RESPONSE_MODEL       # Model that generated response
SpanAttributes.LLM_RESPONSE_FINISH_REASON # Why generation stopped

# Token usage
SpanAttributes.LLM_USAGE_PROMPT_TOKENS     # Input tokens
SpanAttributes.LLM_USAGE_COMPLETION_TOKENS # Output tokens
SpanAttributes.LLM_USAGE_TOTAL_TOKENS      # Total tokens
```

### Message Attributes (`message.py`)
For chat-based interactions:

```python
from agentops.semconv import MessageAttributes

# Prompt messages (indexed)
MessageAttributes.PROMPT_ROLE.format(i=0)     # Role at index 0
MessageAttributes.PROMPT_CONTENT.format(i=0)  # Content at index 0

# Completion messages (indexed)
MessageAttributes.COMPLETION_ROLE.format(i=0)    # Role at index 0
MessageAttributes.COMPLETION_CONTENT.format(i=0) # Content at index 0

# Tool calls (indexed)
MessageAttributes.TOOL_CALL_NAME.format(i=0)      # Tool name
MessageAttributes.TOOL_CALL_ARGUMENTS.format(i=0) # Tool arguments
```

### Core Attributes (`core.py`)
```python
from agentops.semconv import CoreAttributes

CoreAttributes.TRACE_ID    # Trace identifier
CoreAttributes.SPAN_ID     # Span identifier
CoreAttributes.PARENT_ID   # Parent span identifier
CoreAttributes.TAGS        # User-defined tags
```

## Usage Guidelines

1. **Follow OpenTelemetry conventions** - Use `gen_ai.*` prefixed attributes for LLM operations
2. **Use indexed attributes for collections** - Messages, tool calls, etc. should use `.format(i=index)`
3. **Prefer specific over generic** - Use `SpanAttributes.LLM_REQUEST_MODEL` over custom attributes
4. **Document custom attributes** - If you need provider-specific attributes, document them clearly

## Provider-Specific Conventions

### OpenAI
- `SpanAttributes.LLM_OPENAI_RESPONSE_SYSTEM_FINGERPRINT`
- `SpanAttributes.LLM_OPENAI_API_VERSION`

### LangChain  
- `LangChainAttributes.CHAIN_TYPE`
- `LangChainAttributes.TOOL_NAME`

## Metrics (`meters.py`)

Standard metrics for instrumentation:

```python
from agentops.semconv import Meters

Meters.LLM_TOKEN_USAGE         # Token usage histogram
Meters.LLM_OPERATION_DURATION  # Operation duration histogram
Meters.LLM_COMPLETIONS_EXCEPTIONS # Exception counter
```

## Best Practices

1. **Consistency** - Use the same attributes across instrumentations
2. **Completeness** - Capture essential attributes for debugging
3. **Performance** - Avoid capturing large payloads as attributes
4. **Privacy** - Be mindful of sensitive data in attributes

## agentops/legacy/crewai.md

# CrewAI Integration Reference

## Overview
This document provides information about CrewAI's integration with AgentOps and how our legacy compatibility layer supports different versions of CrewAI.

## CrewAI Integration Points

CrewAI has two distinct integration patterns with AgentOps:

### 1. Direct Integration (CrewAI < 0.105.0)
In CrewAI versions 0.98.0 through 0.102.0, integration is done directly in the core code:

- In `crew.py` (_finish_execution method):
  ```python
  if agentops:
      agentops.end_session(
          end_state="Success",
          end_state_reason="Finished Execution",
          is_auto_end=True,
      )
  ```

- In `tools/tool_usage.py`:
  ```python
  # Tool event creation
  tool_event = agentops.ToolEvent(name=calling.tool_name) if agentops else None
  
  # Error recording
  if agentops:
      agentops.record(agentops.ErrorEvent(exception=e, trigger_event=tool_event))
  
  # Tool usage recording
  if agentops:
      agentops.record(tool_event)
  ```

### 2. Event-Based Integration (CrewAI >= 0.105.0)
In CrewAI versions 0.105.0 and above, integration uses an event-based system:

```python
# In utilities/events/third_party/agentops_listener.py
class AgentOpsListener(BaseEventListener):
    # Called when a crew kickoff starts
    @crewai_event_bus.on(CrewKickoffStartedEvent)
    def on_crew_kickoff_started(source, event):
        self.session = agentops.init()
        for agent in source.agents:
            if self.session:
                self.session.create_agent(
                    name=agent.role,
                    agent_id=str(agent.id),
                )

    # Called when a crew kickoff completes
    @crewai_event_bus.on(CrewKickoffCompletedEvent)
    def on_crew_kickoff_completed(source, event):
        if self.session:
            self.session.end_session(
                end_state="Success",
                end_state_reason="Finished Execution",
            )

    # Tool usage and other events are also tracked
    # ...
```

## Required AgentOps Legacy API

To maintain compatibility with all CrewAI versions, our legacy API must support:

### Function Signatures

| Function | Parameters | Used By |
|----------|------------|---------|
| `agentops.init()` | - | All versions, returns a Session object |
| `agentops.end_session()` | Various (see below) | All versions |
| `agentops.record()` | Event object | CrewAI < 0.105.0 |
| `agentops.ToolEvent()` | `name` | CrewAI < 0.105.0 |
| `agentops.ErrorEvent()` | `exception`, `trigger_event` | CrewAI < 0.105.0 |
| `agentops.ActionEvent()` | `action_type` | Used in tests |

### Supported `end_session()` Calls

The `end_session()` function must handle:

1. A simple string status:
   ```python
   agentops.end_session("Success")
   ```

2. Named arguments from CrewAI < 0.105.0:
   ```python
   agentops.end_session(
       end_state="Success",
       end_state_reason="Finished Execution",
       is_auto_end=True
   )
   ```

3. Session object method calls from CrewAI >= 0.105.0:
   ```python
   session.end_session(
       end_state="Success",
       end_state_reason="Finished Execution"
   )
   ```

### Session Class Methods

The Session class must support:

1. `create_agent(name, agent_id)` - Used in CrewAI >= 0.105.0
2. `record(event)` - Used in CrewAI >= 0.105.0
3. `end_session(**kwargs)` - Used in CrewAI >= 0.105.0

## Implementation Guidelines

- All legacy interfaces should accept their parameters without errors but don't need to implement actual functionality.
- New code should use OpenTelemetry instrumentation instead of these legacy interfaces.
- This compatibility layer will be maintained until CrewAI migrates to using OpenTelemetry directly.
- Tests ensure backward compatibility with both integration patterns.

## agentops/integration/callbacks/langchain/README.md

# AgentOps LangChain Callback Handler

This callback handler enables seamless integration between LangChain and AgentOps for tracing and monitoring LLM applications.

## Features

- **Complete Coverage**: Supports all LangChain callback methods 
- **Session Tracking**: Creates a session span that serves as the root for all operations
- **Proper Hierarchy**: Maintains parent-child relationships between operations
- **Complete Instrumentation**: Tracks LLMs, chains, tools, and agent actions
- **Error Tracking**: Records errors from LLMs, chains, and tools
- **Streaming Support**: Handles token streaming for real-time insights
- **Attribute Capture**: Records inputs, outputs, and metadata for all operations
- **Error Resilience**: Handles errors gracefully to ensure spans are always properly closed

## Supported Callbacks

The handler implements all LangChain callback methods:

| Method | Description | Span Kind | Attributes |
|--------|-------------|-----------|------------|
| `on_llm_start` | Start of an LLM call | `llm` | Model, prompts, parameters |
| `on_llm_end` | End of an LLM call | `llm` | Completions, token usage |
| `on_llm_new_token` | Streaming token received | N/A | Token count, last token |
| `on_llm_error` | LLM call error | `llm` | Error details |
| `on_chat_model_start` | Start of a chat model call | `llm` | Model, messages, parameters |
| `on_chain_start` | Start of a chain | `task` | Chain type, inputs |
| `on_chain_end` | End of a chain | `task` | Outputs |
| `on_chain_error` | Chain execution error | `task` | Error details |
| `on_tool_start` | Start of a tool call | `tool` | Tool name, input |
| `on_tool_end` | End of a tool call | `tool` | Output |
| `on_tool_error` | Tool execution error | `tool` | Error details |
| `on_agent_action` | Agent taking an action | `agent` | Tool, input, log |
| `on_agent_finish` | Agent completing a task | `agent` | Output, log |
| `on_text` | Arbitrary text event | `text` | Text content |

All spans have appropriate attributes such as:
- Model information for LLM spans
- Input/output for all operations
- Tool names and types
- Chain types and configurations
- Error details for failed operations

## Troubleshooting

If you're not seeing data in AgentOps:

1. Check that your API key is correctly configured
2. Ensure you're passing the handler to all relevant components
3. Verify that all operations are properly ending/closing

## How It Works

The callback handler:
1. Creates a session span when initialized
2. Intercepts LangChain callbacks for various operations
3. Creates appropriate spans with meaningful attributes
4. Maintains proper parent-child relationships
5. Automatically cleans up and ends spans when operations complete 

## agentops/sdk/README.md

# AgentOps v0.4 Architecture

## Transition from Events to Spans

In AgentOps v0.4, we've transitioned from the "Event" concept to using "Spans" for all event tracking. This proposal outlines a new architecture that supports this transition and enables custom implementations through decorators.

## Core Concepts

1. **Session**: The master trace that serves as the root for all spans. No spans can exist without a session at the top.
2. **Spans**: Represent different types of operations (Agent, Tool, etc.) and are organized hierarchically.
3. **Decorators**: Allow users to easily mark their custom components with AgentOps-specific span types.
4. **TracingConfig**: A dedicated configuration structure for the tracing core, separate from the main application configuration.

## Architecture Diagram

```mermaid
flowchart TD
    %% Core Tracing Components
    subgraph "Core Tracing Infrastructure"
        TracingCore[Tracing Core]
        TracingConfig[Tracing Config]
        SpanFactory[Span Factory]
        SpanProcessor[Span Processor]
        SpanExporter[Span Exporter]
        
        TracingConfig --> TracingCore
        TracingCore --> SpanFactory
        TracingCore --> SpanProcessor
        SpanProcessor --> SpanExporter
    end
    
    %% Span Base Classes
    subgraph "Span Base Classes"
        TracedObject[TracedObject]
    end
    
    %% Span Types
    subgraph "Span Types"
        SessionSpan[SessionSpan]
        AgentSpan[AgentSpan]
        ToolSpan[ToolSpan]
        LLMSpan[LLMSpan]
        CustomSpan[CustomSpan]
        
        TracedObject --> SessionSpan
        TracedObject --> AgentSpan
        TracedObject --> ToolSpan
        TracedObject --> LLMSpan
        TracedObject --> CustomSpan
    end
    
    %% Decorators
    subgraph "Decorators"
        SessionDecorator[session]
        AgentDecorator[agent]
        ToolDecorator[tool]
        LLMDecorator[llm]
        
        AgentDecorator --> AgentSpan
        ToolDecorator --> ToolSpan
        SessionDecorator --> SessionSpan
        LLMDecorator --> LLMSpan
    end
    
    %% User-Facing Classes
    subgraph "User-Facing Classes"
        Session[Session]
        Agent[Agent]
        Tool[Tool]
        
        SessionSpan --> Session
        AgentSpan --> Agent
        ToolSpan --> Tool
    end
    
    %% Relationships
    SpanFactory --> TracedObject
    Session -.->|"Master Trace"| Agent
    Session -.->|"Master Trace"| Tool
    
    %% Context Management
    subgraph "Context Management"
        SpanContext[Span Context]
        Registry[Registry]
        
        SpanContext <--> Registry
    end
    
    TracingCore <--> SpanContext
    
    class TracingCore,SpanFactory,SpanProcessor,SpanExporter core
    class TracedObject base
    class SessionSpan,AgentSpan,ToolSpan,LLMSpan,CustomSpan span
    class SessionDecorator,AgentDecorator,ToolDecorator,LLMDecorator decorator
    class Session,Agent,Tool user
    class SpanContext,Registry context
```

## Component Descriptions

### Core Tracing Infrastructure

- **Tracing Core**: Central component that manages the creation, processing, and export of spans.
- **Tracing Config**: Configuration specific to the tracing infrastructure, separate from the main application configuration.
- **Span Factory**: Creates spans of different types based on context and decorator information.
- **Span Processor**: Processes spans (adds attributes, manages context, etc.) before they are exported.
- **Span Exporter**: Exports spans to the configured destination (e.g., AgentOps backend).

### Span Base Classes

- **TracedObject**: Base class that provides core tracing functionality (trace ID, span ID, etc.) and common span operations (start, end, attributes).

### Span Types

- **SessionSpan**: Represents a session (master trace).
- **AgentSpan**: Represents an agent operation.
- **ToolSpan**: Represents a tool operation.
- **LLMSpan**: Represents an LLM operation.
- **CustomSpan**: Allows for custom span types.

### Decorators

- **@session**: Creates a new session span.
- **@agent**: Creates a new agent span.
- **@tool**: Creates a new tool span.
- **@llm**: Creates a new LLM span.

### User-Facing Classes

- **Session**: User-facing session class that wraps SessionSpan.
- **Agent**: User-facing agent class that wraps AgentSpan.
- **Tool**: User-facing tool class that wraps ToolSpan.

### Context Management

- **Span Context**: Manages the current span context (parent-child relationships).
- **Registry**: Keeps track of active spans and their relationships.

## Implementation Considerations

1. **Decorator Implementation**:
   ```python
   def agent(cls=None, **kwargs):
       def decorator(cls):
           # Wrap methods with span creation/management
           original_init = cls.__init__
           
           def __init__(self, *args, **init_kwargs):
               # Get current session from context
               session = get_current_session()
               if not session:
                   raise ValueError("No active session found. Create a session first.")
               
               # Create agent span as child of session
               self._span = create_span("agent", parent=session.span, **kwargs)
               
               # Call original init
               original_init(self, *args, **init_kwargs)
           
           cls.__init__ = __init__
           return cls
       
       if cls is None:
           return decorator
       return decorator(cls)
   ```

2. **Session as Master Trace**:
   - All spans must have a session as their root ancestor.
   - Session creation should be explicit and precede any agent or tool operations.

3. **Context Propagation**:
   - Span context should be propagated automatically through the call stack.
   - Context should be accessible globally but thread-safe.

## Example Usage

```python
from agentops import Session, agent, tool, tracer
from agentops.sdk import TracingConfig

# Initialize the global tracer with a dedicated configuration
tracer.initialize(
    service_name="my-service",
    exporter_endpoint="https://my-exporter-endpoint.com",
    max_queue_size=1000,
    max_wait_time=10000
)

# Create a session (master trace)
with Session() as session:
    # Create an agent
    @agent
    class MyAgent:
        def __init__(self, name):
            self.name = name
        
        def run(self):
            # Agent operations are automatically traced
            result = self.use_tool()
            return result
        
        @tool
        def use_tool(self):
            # Tool operations are automatically traced
            return "Tool result"
    
    # Use the agent
    agent = MyAgent("Agent1")
    result = agent.run()
```

## Benefits

1. **Simplified API**: Users can easily mark their components with decorators.
2. **Hierarchical Tracing**: All operations are organized hierarchically with the session as the root.
3. **Automatic Context Propagation**: Context is propagated automatically through the call stack.
4. **Extensibility**: Custom span types can be added easily.
5. **Separation of Concerns**: Tracing configuration is separate from the main application configuration.



## examples/README.md

# AgentOps Examples

This directory contains comprehensive examples demonstrating how to integrate AgentOps with various AI/ML frameworks, libraries, and providers. Each example is provided as a Jupyter notebook and a Python script with detailed explanations and code samples.

## üìÅ Directory Structure

- **[`ag2/`](./ag2/)** - Examples for AG2 (AutoGen 2.0) multi-agent conversations
  - `agentchat_with_memory` - Agent chat with persistent memory
  - `async_human_input` - Asynchronous human input handling
  - `tools_wikipedia_search` - Wikipedia search tool integration

- **[`anthropic/`](./anthropic/)** - Anthropic Claude API integration examples
  - `agentops-anthropic-understanding-tools` - Deep dive into tool usage
  - `anthropic-example-async` - Asynchronous API calls
  - `anthropic-example-sync` - Synchronous API calls
  - `antrophic-example-tool` - Tool calling examples
  - `README.md` - Detailed Anthropic integration guide

- **[`autogen/`](./autogen/)** - Microsoft AutoGen framework examples
  - `AgentChat` - Basic agent chat functionality
  - `MathAgent` - Mathematical problem-solving agent

- **[`crewai/`](./crewai/)** - CrewAI multi-agent framework examples
  - `job_posting` - Job posting automation workflow
  - `markdown_validator` - Markdown validation agent

- **[`gemini/`](./gemini/)** - Google Gemini API integration
  - `gemini_example` - Basic Gemini API usage with AgentOps

- **[`google_adk/`](./google_adk/)** - Google AI Development Kit examples
  - `human_approval` - Human-in-the-loop approval workflows

- **[`langchain/`](./langchain/)** - LangChain framework integration
  - `langchain_examples` - Comprehensive LangChain usage examples

- **[`litellm/`](./litellm/)** - LiteLLM proxy integration
  - `litellm_example` - Multi-provider LLM access through LiteLLM

- **[`openai/`](./openai/)** - OpenAI API integration examples
  - `multi_tool_orchestration` - Complex tool orchestration
  - `openai_example_async` - Asynchronous OpenAI API calls
  - `openai_example_sync` - Synchronous OpenAI API calls
  - `web_search` - Web search functionality

- **[`openai_agents/`](./openai_agents/)** - OpenAI Agents SDK examples
  - `agent_patterns` - Common agent design patterns
  - `agents_tools` - Agent tool integration
  - `customer_service_agent` - Customer service automation

- **[`smolagents/`](./smolagents/)** - SmolAgents framework examples
  - `multi_smolagents_system` - Multi-agent system coordination
  - `text_to_sql` - Natural language to SQL conversion

- **[`watsonx/`](./watsonx/)** - IBM Watsonx AI integration
  - `watsonx-streaming` - Streaming text generation
  - `watsonx-text-chat` - Text generation and chat completion
  - `watsonx-tokeniation-model` - Tokenization and model details
  - `README.md` - Detailed Watsonx integration guide

- **[`xai/`](./xai/)** - xAI (Grok) API integration
  - `grok_examples` - Basic Grok API usage
  - `grok_vision_examples` - Vision capabilities with Grok

### Utility Scripts

- **[`generate_documentation.py`](./generate_documentation.py)** - Script to convert Jupyter notebooks to MDX documentation files
  - Converts notebooks from `examples/` to `docs/v2/examples/`
  - Handles frontmatter, GitHub links, and installation sections
  - Transforms `%pip install` commands to CodeGroup format

## üìì Prerequisites

1. **AgentOps Account**: Sign up at [agentops.ai](https://agentops.ai)
2. **Python Environment**: Python 3.10+ recommended
3. **API Keys**: Obtain API keys for the services you want to use

## üìñ Documentation Generation

The `generate_documentation.py` script automatically converts these Jupyter notebook examples into documentation for the AgentOps website. It:

- Extracts notebook content and converts to Markdown
- Adds proper frontmatter and metadata
- Transforms installation commands into user-friendly format
- Generates GitHub links for source notebooks
- Creates MDX files in `docs/v2/examples/`

### Usage
```bash
python examples/generate_documentation.py examples/langchain/langchain_examples.ipynb
```

## ü§ù Contributing

When adding new examples:

1. Create a new subdirectory for the framework/provider
2. Include comprehensive Jupyter notebooks with explanations
3. Add a README.md if the integration is complex
4. Ensure examples are self-contained and runnable
5. Follow the existing naming conventions
6. Use the `generate_documentation.py` script to create documentation files
7. Add the example notebook to the main `README.md` for visibility
8. Add the generated documentation to the `docs/v2/examples/` directory for website visibility
9. Submit a pull request with a clear description of your changes

## üìö Additional Resources

- [AgentOps Documentation](https://docs.agentops.ai)
- [AgentOps Dashboard](https://app.agentops.ai)
- [GitHub Repository](https://github.com/AgentOps-AI/agentops)
- [Community Discord](https://discord.gg/agentops)

## üìÑ License

These examples are provided under the same license as the AgentOps project. See the main repository for license details.


## examples/README_TESTING.md

# AgentOps Examples Integration Testing

This directory contains example scripts demonstrating how to use AgentOps with various LLM providers and frameworks. Each example includes automatic validation to ensure that LLM spans are properly tracked by AgentOps.

## What's Being Tested

Each example script now includes automated span validation that:

1. **Runs the example** - Executes the normal example code
2. **Validates span tracking** - Uses the AgentOps integrated validation to verify that:
   - Spans were successfully sent to AgentOps
   - LLM calls were properly instrumented and tracked
   - Token counts and costs were recorded

## How It Works

### 1. Integrated Validation

AgentOps now includes built-in validation functionality (`agentops.validate_trace_spans`) that:
- Exchanges API keys for JWT tokens using the public API
- Queries the AgentOps API for trace and span data
- Validates that expected spans are present
- Retrieves metrics like token usage and costs

### 2. Example Structure

Each example follows this pattern:

```python
import agentops

# Initialize AgentOps
agentops.init()

# Start a trace
tracer = agentops.start_trace("example-name")

# ... perform operations with LLMs ...

# End the trace
agentops.end_trace(tracer, end_state="Success")

# Validate spans were tracked
try:
    result = agentops.validate_trace_spans(trace_context=tracer)
    agentops.print_validation_summary(result)
except agentops.ValidationError as e:
    print(f"‚ùå Error validating spans: {e}")
    raise
```

### 3. CI/CD Integration

The GitHub Actions workflow (`examples-integration-test.yml`) runs all examples automatically on:
- Push to main/develop branches
- Pull requests
- Manual workflow dispatch

Each example is run in isolation with proper error handling and reporting.

## Running Tests Locally

To run a specific example with validation:

```bash
cd examples/openai
python openai_example_sync.py
```

To run all examples:

```bash
# From the examples directory
for script in $(find . -name "*.py" -type f | grep -v "__pycache__"); do
    echo "Running $script..."
    python "$script"
done
```

## Adding New Examples

When adding a new example:

1. Include the standard validation at the end:
   ```python
   # Validate spans were tracked
   try:
       result = agentops.validate_trace_spans(trace_context=tracer)
       agentops.print_validation_summary(result)
   except agentops.ValidationError as e:
       print(f"‚ùå Error validating spans: {e}")
       raise
   ```

2. Add the example to the GitHub Actions matrix in `.github/workflows/examples-integration-test.yml`

3. Ensure the example has proper error handling

## Requirements

- Valid `AGENTOPS_API_KEY` environment variable
- API keys for the specific LLM provider being tested
- Python 3.12+ with required dependencies 

## examples/google_adk/README.md

# Google ADK Examples with AgentOps

This directory contains examples of using the Google AI Development Kit (ADK) with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops google-adk
  ```

## Examples

### 1. Human Approval

Example: `human_approval`
This example demonstrates:
- A human-in-the-loop approval workflow

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your Google ADK calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/crew/job_posting.md

```markdown
**Job Title: Software Engineer**

**Company: AgentOps.ai**

**Location: [Location/Remote]**

**About AgentOps.ai:**
At AgentOps.ai, we are revolutionizing the field of conversational AI by providing robust, scalable, and efficient solutions that ensure seamless integration and optimal performance of AI agents. Our mission is to empower businesses with cutting-edge AI technologies that enhance customer experiences and drive operational efficiency. We believe in fostering a culture of innovation, collaboration, and continuous learning.

**Introduction:**
Are you passionate about building innovative software solutions and enhancing AI agent observability? AgentOps.ai is looking for a skilled Software Engineer to join our dynamic team. In this role, you will have the opportunity to work on groundbreaking projects that involve tracking and analyzing the performance, behavior, and interactions of AI agents in real-time. If you are a detail-oriented problem solver with a desire to create impactful software, we want to hear from you!

**Role Description:**
As a Software Engineer at AgentOps.ai, you will be responsible for designing and building tools and frameworks to automate the development, testing, deployment, and management of services and products. You will play a key role in planning and executing the full software development lifecycle for assigned projects, ensuring scalability and efficiency of distributed software and applications. Collaboration with product managers and user-experience designers will be essential to influence the strategy and delivery of next-wave product features and system capabilities.

**Responsibilities:**
- Design and build tools and frameworks to automate the development, testing, deployment, and management of services and products.
- Plan and execute the full software development lifecycle for each assigned project, adhering to company standards and expectations.
- Plan and scale distributed software and applications using synchronous and asynchronous design patterns.
- Work with product managers and user-experience designers to influence the strategy and delivery of next-wave product features and system capabilities.
- Track, document, and maintain software and network system functionality, leveraging opportunities to improve engineering.

**Required Skills and Qualifications:**
- **Programming Languages:** Proficiency in Java, Python, and C++ is essential.
- **Scripting and Automation:** Strong ability in scripting and test automation.
- **Web Technologies:** Proficiency with HTML5, CSS3, and content management systems.
- **Relational Databases:** Working knowledge of ORM and SQL technologies.
- **Software Development:** Experience with rapid development cycles in a web-based environment, including full software development lifecycle.
- **Frameworks:** Knowledge of frameworks such as Wicket, GWT, and Spring MVC.
- **Engineering Experience:** Five or more years of experience as an engineer of software and networking platforms.
- **Development Experience:** Seven or more years of combined professional and academic experience in relevant programming languages.
- **Documentation:** Proven ability to document design processes, including development, testing, analytics, and troubleshooting.
- **Web Application Development:** Experience in developing web applications with multiple technologies.
- **Network Systems:** Experience in testing and evaluating current networking systems.
- **Collaboration:** Ability to work with global teams to produce project plans and analyze project operations.
- **Problem-Solving:** Highly motivated to find technical issues and fix them with meticulous code.
- **Detail-Oriented:** Focus on creating software and networking platforms free of faulty programming without compromising site reliability.
- **Innovation:** Ability to visualize, design, and develop innovative software platforms.
- **Continuous Learning:** Desire to continue professional growth through training and education.
- **Educational Background:** Bachelor‚Äôs degree (or equivalent) in software engineering or information technology.

**Company Benefits:**
- Competitive salary and performance bonuses
- Health, dental, and vision insurance
- Generous paid time off and holidays
- Professional development opportunities
- Collaborative and innovative work environment
- Free sandwiches (yes, you read that right!)

**How to Apply:**
If you are ready to take your career to the next level and join a company that values innovation and excellence, apply today by submitting your resume and cover letter to [email address] with the subject line "Software Engineer Application - [Your Name]". We look forward to meeting you!

**AgentOps.ai is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.**
```

## examples/litellm/README.md

# LiteLLM Examples with AgentOps

This directory contains examples of using LiteLLM with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops litellm
  ```

## Examples

### 1. LiteLLM Example

Example: `litellm_example`
This example demonstrates:
- Multi-provider LLM access through LiteLLM

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your LiteLLM calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/crewai/README.md

# CrewAI Examples with AgentOps

This directory contains examples of using CrewAI with AgentOps instrumentation for various multi-agent automation tasks.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops 'crewai[tools]'
  ```

## Examples

### 1. Job Posting

Example: `job_posting`
This example demonstrates:
- A job posting automation workflow

### 2. Markdown Validator

Example: `markdown_validator`

This example demonstrates:
- A markdown validation agent

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your CrewAI calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/llamaindex/README.md

# LlamaIndex AgentOps Integration Example

This example demonstrates how to use AgentOps with LlamaIndex for observability and monitoring of your context-augmented generative AI applications.

## Setup

1. Install required packages:
```bash
pip install agentops llama-index-instrumentation-agentops llama-index python-dotenv
```

2. Set your API keys:
```bash
export AGENTOPS_API_KEY="your_agentops_api_key"
export OPENAI_API_KEY="your_openai_api_key"
```

## Files

- `llamaindex_example.py` - Python script example
- `llamaindex_example.ipynb` - Jupyter notebook example

## Usage

Run the Python script:
```bash
python llamaindex_example.py
```

Or open and run the Jupyter notebook:
```bash
jupyter notebook llamaindex_example.ipynb
```

After running, check your AgentOps dashboard for the recorded session.


## examples/ag2/README.md

# AG2 Examples with AgentOps

This directory contains examples of using AG2 (AutoGen 2.0) with AgentOps instrumentation for various multi-agent conversation tasks.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops ag2
  ```

## Examples

### 1. Asynchronous Human Input

Example: `async_human_input`

This example demonstrates:
- Asynchronous human input handling

### 2. Wikipedia Search Tool

Example: `tools_wikipedia_search`

This example demonstrates:
- Wikipedia search tool integration

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your AG2 calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/google_genai/README.md

# Google GenAI Examples with AgentOps

This directory contains examples of using Google's Generative AI with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops google-generativeai
  ```

## Examples

### 1. Gemini Example

Example: `gemini_example`
This example demonstrates:
- Basic Gemini API usage with AgentOps

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your Google GenAI calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/langchain/README.md

# LangChain Examples with AgentOps

This directory contains examples of using LangChain with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops langchain langchain_openai
  ```

## Examples

### 1. LangChain Examples

Example: `langchain_examples`
This example demonstrates:
- Comprehensive LangChain usage examples

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your LangChain calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/watsonx/README.md

# IBM Watsonx AI Examples with AgentOps

This directory contains examples of using IBM Watsonx AI with AgentOps instrumentation for various natural language processing tasks.

## Prerequisites

- IBM Watsonx AI account with API key
- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops ibm-watsonx-ai python-dotenv
  ```

## Environment Setup

Create a `.env` file in your project root with the following values:

```
WATSONX_URL=https://your-region.ml.cloud.ibm.com
WATSONX_API_KEY=your-api-key-here
WATSONX_PROJECT_ID=your-project-id-here
```

## Examples

### 1. Basic Text Generation and Chat Completion

Example: `watsonx-text-chat`
This example demonstrates:
- Basic text generation with IBM Watsonx AI
- Chat completion with system and user messages
- Multiple examples of chat interactions

### 2. Streaming Generation

Example: `watsonx-streaming`

This example demonstrates:
- Streaming text generation
- Streaming chat completion
- Processing streaming responses

### 3. Tokenization and Model Details

Example: `watsonx-tokenization-model`

This example demonstrates:
- Tokenizing text with IBM Watsonx AI models
- Retrieving model details
- Comparing tokenization between different models

## IBM Watsonx AI Models

The examples use the following IBM Watsonx AI models:
- `google/flan-ul2`: A text generation model
- `meta-llama/llama-3-3-70b-instruct`: A chat completion model

You can explore other available models through the IBM Watsonx platform.

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your IBM Watsonx AI calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/xai/README.md

# xAI Examples with AgentOps

This directory contains examples of using the xAI (Grok) API with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops openai
  ```

## Examples

### 1. Grok Examples

Example: `grok_examples`
This example demonstrates:
- Basic Grok API usage

### 2. Grok Vision Examples

Example: `grok_vision_examples`

This example demonstrates:
- Vision capabilities with Grok

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your xAI calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/autogen/README.md

# AutoGen Examples with AgentOps

This directory contains examples of using Microsoft AutoGen with AgentOps instrumentation for various agent-based tasks.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops "ag2[retrievechat]"
  ```

## Examples

### 1. Agent Chat

Example: `AgentChat`
This example demonstrates:
- Basic agent chat functionality

### 2. Math Agent

Example: `MathAgent`

This example demonstrates:
- A mathematical problem-solving agent

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your AutoGen calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/openai_agents/README.md

# OpenAI Agents Examples with AgentOps

This directory contains examples of using the OpenAI Agents SDK with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops openai-agents
  ```

## Examples

### 1. Agent Patterns

Example: `agent_patterns`
This example demonstrates:
- Common agent design patterns

### 2. Agent Tools

Example: `agents_tools`

This example demonstrates:
- Agent tool integration

### 3. Customer Service Agent

Example: `customer_service_agent`

This example demonstrates:
- Customer service automation

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your OpenAI Agents calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


## examples/smolagents/README.md

# Smolagents Examples

This directory contains examples demonstrating how to use Smolagents with AgentOps for agent monitoring and observability.

## Examples

### 1. Simple Task Agent (`simple_task_agent.py`)
A minimal example showing how to create a single agent that can answer questions using web search. This is the best starting point for understanding Smolagents basics.

**Features:**
- Basic agent setup with search capabilities
- AgentOps integration for tracking
- Error handling and session management

### 2. Multi-Agent System (`multi_smolagents_system.py`)
A more complex example demonstrating a hierarchical multi-agent system with:
- Manager agent coordinating multiple specialized agents
- Web search agent with custom tools
- Code interpreter capabilities
- Tool creation and usage

### 3. Text to SQL Agent (`text_to_sql.py`)
An example showing how to create an agent that can convert natural language queries into SQL statements.

## Running the Examples

1. Install dependencies:
```bash
pip install agentops smolagents python-dotenv
```

2. Set up your API keys in a `.env` file:
```env
AGENTOPS_API_KEY=your_agentops_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

3. Run any example:
```bash
python simple_task_agent.py
```

4. View the results in your [AgentOps Dashboard](https://app.agentops.ai/sessions)

## Key Concepts

- **Agents**: AI assistants that can use tools and reason through problems
- **Tools**: Functions that agents can call to interact with external systems
- **Models**: LLM backends via LiteLLM (supports OpenAI, Anthropic, etc.)
- **AgentOps Integration**: Automatic tracking of all agent activities

## Learn More

- [Smolagents Documentation](https://github.com/huggingface/smolagents)
- [AgentOps Documentation](https://docs.agentops.ai)
- [Full Integration Guide](https://docs.agentops.ai/v2/integrations/smolagents)


## examples/openai/README.md

# OpenAI Examples with AgentOps

This directory contains examples of using the OpenAI API with AgentOps instrumentation.

## Prerequisites

- Python >= 3.10 < 3.13
- Install required dependencies:
  ```
  pip install agentops openai
  ```

## Examples

### 1. Multi-Tool Orchestration

Example: `multi_tool_orchestration`
This example demonstrates:
- Complex tool orchestration

### 2. Asynchronous OpenAI API calls

Example: `openai_example_async`

This example demonstrates:
- Asynchronous OpenAI API calls

### 3. Synchronous OpenAI API calls

Example: `openai_example_sync`

This example demonstrates:
- Synchronous OpenAI API calls

### 4. Web Search

Example: `web_search`

This example demonstrates:
- Web search functionality

### 5. o3 Responses API

Example: `o3_responses_example`

This example demonstrates:
- OpenAI's o3 reasoning model with the Responses API
- Tool calls and structured reasoning
- Complex decision-making scenarios
- AgentOps integration with reasoning models

## AgentOps Integration

These examples show how to use AgentOps to monitor and analyze your AI applications. AgentOps automatically instruments your OpenAI calls to provide insights into performance, usage patterns, and model behavior.

To learn more about AgentOps, visit [https://www.agentops.ai](https://www.agentops.ai)


