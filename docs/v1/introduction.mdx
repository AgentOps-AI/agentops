---
title: "Introduction"
description: "AgentOps is the developer favorite platform for testing, debugging, and deploying AI agents and LLM apps."
mode: "wide"
---

## Integrate with developer favorite agent frameworks

<CardGroup cols={2}>
  <Card title="OpenAI Agents SDK" icon={<img src="/images/external/openai/openai-logomark.png" alt="OpenAI Agents SDK" />} iconType="image" href="/v1/integrations/agentssdk" />
  <Card title="CrewAI" icon={<img src="/images/external/crew/crew.png" alt="CrewAI"/>} iconType="image" href="/v1/integrations/crewai" />
  <Card title="AG2" icon={<img src="/images/external/autogen/ag2.svg" alt="AG2"/>} iconType="image" href="/v1/integrations/autogen" />
  <Card title="AutoGen" icon={<img src="/images/external/microsoft/microsoft_logo.svg" alt="AutoGen" />} iconType="image" href="/v1/integrations/autogen" />
  <Card title="Anthropic" icon={<img src="/images/external/anthropic/anthropic_icon_slate.png" alt="Anthropic" />} iconType="image" href="/v1/integrations/anthropic" />
  <Card title="Ollama" icon={<img src="/images/external/ollama/ollama-icon.png" alt="Ollama" />} iconType="image" href="/v1/integrations/ollama" />
  <Card title="Cohere" icon={<img src="/images/external/cohere/cohere-logo.svg" alt="Cohere" />} iconType="image" href="/v1/integrations/cohere" />
  <Card title="Groq" icon="bolt" href="https://docs.agentops.ai/v1/integrations/groq" />
</CardGroup>

Observability and monitoring for your AI agents and LLM apps. And we do it all in just two lines of code...
<CodeGroup>
	```python python
	import agentops
	agentops.init(<INSERT YOUR API KEY HERE>)
	```
</CodeGroup>
... that logs everything back to your AgentOps Dashboard.

## The AgentOps Dashboard

<Check>[Give us a star](https://github.com/AgentOps-AI/agentops) to bookmark on GitHub, save for later üñáÔ∏è)</Check>

With just two lines of code, you can free yourself from the chains of the terminal and, instead, visualize your agents' behavior
in your AgentOps Dashboard. After setting up AgentOps, each execution of your program is recorded as a session and the above
data is automatically recorded for you.

The examples below were captured with two lines of code.

### Session Drilldown
Here you will find a list of all of your previously recorded sessions and useful data about each such as total execution time.
You also get helpful debugging info such as any SDK versions you were on if you're building on a supported agent framework like Crew or AutoGen.
LLM calls are presented as a familiar chat history view, and charts give you a breakdown of the types of events that were called and how long they took.

<Frame type="glass" caption="Session Summary">
  <img height="200" src="/images/session-summary.gif" />
</Frame>

Find any past sessions from your Session Drawer.
<Frame type="glass" caption="Session Drawer">
  <img height="250" src="/images/session-drawer.gif" />
</Frame>

Most powerful of all is the Session Waterfall. On the left, a time visualization of all your LLM calls, Action events, Tool calls, and Errors.
On the right, specific details about the event you've selected on the waterfall. For instance the exact prompt and completion for a given LLM call. 
Most of which has been automatically recorded for you.

<Frame type="glass" caption="Session Waterfall">
  <img height="200" src="/images/session-waterfall.gif" />
</Frame>


### Session Overview
View a meta-analysis of all of your sessions in a single view.
<Frame type="glass" caption="Session Overview">
  <img height="200" src="/images/overview.png" />
</Frame>

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>
