---
title: LiteLLM
description: "Call the latest models using the OpenAI format including:  
Llama, Mistral, Claude, Gemini, Gemma, DALL-E, Whisper"
---

## LiteLLM

From [LiteLLM's docs](https://docs.litellm.ai/docs/):

Call 400+ LLMs using the same input/output Format

- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
- Consistent output. Text responses will always be available at `['choices'][0]['message']['content']`.
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI)
- Track spend & set budgets per project

LiteLLM also supports many [providers](https://docs.litellm.ai/docs/providers).

## Using AgentOps with LiteLLM

### Requires `litellm>=1.3.1`

AgentOps requires a minor adjustment to how you call LiteLLM.

```python python
# Do not use LiteLLM like this
# from litellm import completion
# ...
# response = completion(model="claude-3", messages=messages)

# Use LiteLLM like this
import litellm
...
response = litellm.completion(model="claude-3", messages=messages)
# or
response = await litellm.acompletion(model="claude-3", messages=messages)

```

<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/link_to_api_button.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
<script type="module" src="/scripts/adjust_api_dynamically.js"></script>
