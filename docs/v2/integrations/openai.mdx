---
title: OpenAI
description: "Track and analyze your OpenAI API calls with AgentOps"
---

AgentOps seamlessly integrates with [OpenAI's Python SDK](https://github.com/openai/openai-python), allowing you to track and analyze all your OpenAI API calls automatically.

## Installation

<CodeGroup>
```bash pip
pip install agentops openai
```
```bash poetry
poetry add agentops openai
```
```bash uv
uv pip install agentops openai
```
</CodeGroup>

## Setting Up API Keys

Before using OpenAI with AgentOps, you need to set up your API keys. You can obtain:
- **OPENAI_API_KEY**: From the [OpenAI Platform](https://platform.openai.com/api-keys)
- **AGENTOPS_API_KEY**: From your [AgentOps Dashboard](https://app.agentops.ai/)

Then to set them up, you can either export them as environment variables or set them in a `.env` file.

<CodeGroup>
```bash Export to CLI
export OPENAI_API_KEY="your_openai_api_key_here"
export AGENTOPS_API_KEY="your_agentops_api_key_here"
```
```txt Set in .env file
OPENAI_API_KEY="your_openai_api_key_here"
AGENTOPS_API_KEY="your_agentops_api_key_here"
```
</CodeGroup>

Then load the environment variables in your Python code:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Set up environment variables with fallback values
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["AGENTOPS_API_KEY"] = os.getenv("AGENTOPS_API_KEY")
```

## Usage

Initialize AgentOps at the beginning of your application to automatically track all OpenAI API calls:

```python
import agentops
from openai import OpenAI
      
# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Make API calls as usual - AgentOps will track them automatically
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)

print(response.choices[0].message.content)
```

## Examples

<CodeGroup>
```python Streaming
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Make a streaming API call
stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about AI."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

```python Function Calling
import json
import agentops
from openai import OpenAI

# Initialize AgentOps
agentops.init()

# Create OpenAI client
client = OpenAI()

# Define tools
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    }
                },
                "required": ["location"],
            },
        },
    }
]

# Function implementation
def get_weather(location):
    return json.dumps({"location": location, "temperature": "72", "unit": "fahrenheit", "forecast": ["sunny", "windy"]})

# Make a function call API request
messages = [
    {"role": "system", "content": "You are a helpful weather assistant."},
    {"role": "user", "content": "What's the weather like in Boston?"}
]

response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=tools,
    tool_choice="auto",
)

# Process response
response_message = response.choices[0].message
messages.append({"role": "assistant", "content": response_message.content, "tool_calls": response_message.tool_calls})

if response_message.tool_calls:
    # Process each tool call
    for tool_call in response_message.tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)
        
        if function_name == "get_weather":
            function_response = get_weather(function_args.get("location"))
            
            # Add tool response to messages
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": function_response,
                }
            )
    
    # Get a new response from the model
    second_response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
    )
    
    print(second_response.choices[0].message.content)
else:
    print(response_message.content)
```
</CodeGroup>

## More Examples
<CardGroup cols={2}>
  <Card title="Multi-Tool Orchestration" icon="notebook" href="/v2/examples/openai">
    Advanced multi-tool RAG example
  </Card>
  <Card title="Async OpenAI Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/openai_example_async.ipynb" newTab={true}>
    Demonstrates asynchronous calls with the OpenAI SDK.
  </Card>
  <Card title="Sync OpenAI Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/openai_example_sync.ipynb" newTab={true}>
    Shows synchronous calls with the OpenAI SDK.
  </Card>
  <Card title="Web Search Example" icon="notebook" href="https://github.com/AgentOps-AI/agentops/blob/main/examples/openai/web_search.ipynb" newTab={true}>
    Example of integrating web search capabilities.
  </Card>
</CardGroup>


<script type="module" src="/scripts/github_stars.js"></script>
<script type="module" src="/scripts/scroll-img-fadein-animation.js"></script>
<script type="module" src="/scripts/button_heartbeat_animation.js"></script>
<script type="css" src="/styles/styles.css"></script>
