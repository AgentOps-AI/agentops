{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db479f8",
   "metadata": {},
   "source": [
    "Microsoft Autogen Assistant Chat Example\n",
    "\n",
    "Overview\n",
    "This script demonstrates how to build an interactive AI assistant chat using Microsoft Autogen and AgentOps.\n",
    "You will see how to:\n",
    "  - Initialize an assistant agent with OpenAI's GPT-4 model\n",
    "  - Equip the agent with a calculator tool for step-by-step math problem solving\n",
    "  - Track and log all agent interactions automatically with AgentOps\n",
    "  - Simulate a conversation loop where the assistant and user exchange messages until the task is complete\n",
    "This approach is useful for building conversational AI agents that can reason through complex tasks, show their work, and provide transparent, auditable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee08996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentOps ensures all actions are tracked, making it easy to monitor, debug, and analyze your agent's performance in real time.\n",
    "#\n",
    "# First let's install the required packages\n",
    "%pip install -U ag2[autogen-agentchat]\n",
    "%pip install -U \"autogen-ext[openai]\"\n",
    "%pip install -U agentops\n",
    "%pip install -U python-dotenv\n",
    "%pip install -U nest_asyncio\n",
    "# Then import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69965752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.core.error import (\n",
    "    StdinNotImplementedError,\n",
    ")\n",
    "from typing import Annotated, Literal\n",
    "import agentops\n",
    "import nest_asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import CancellationToken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ed335",
   "metadata": {},
   "source": [
    "Next, we'll set our API keys. There are several ways to do this, the code below is just the most foolproof way for the purposes of this notebook. It accounts for both users who use environment variables and those who just want to set the API Key here in this notebook.\n",
    "\n",
    "[Get an AgentOps API key](https://agentops.ai/settings/projects)\n",
    "\n",
    "1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...\n",
    "\n",
    "2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"AGENTOPS_API_KEY\"] = os.getenv(\"AGENTOPS_API_KEY\", \"your_api_key_here\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your_openai_api_key_here\")\n",
    "# Ensure API key is available\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n",
    "# Define model and API key\n",
    "model_name = \"gpt-4-turbo\"  # Or \"gpt-4o\" / \"gpt-4o-mini\" as per migration guide examples\n",
    "# Create the model client\n",
    "model_client = OpenAIChatCompletionClient(model=model_name, api_key=openai_api_key, seed=42, temperature=0)\n",
    "# When initializing AgentOps, you can pass in optional tags to help filter sessions\n",
    "agentops.init(trace_name=\"autogen-agent-chat\", tags=[\"autogen-agent-chat\", \"agentops-example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b26d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a simple calculator tool the assistant can use\n",
    "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
    "def calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\n",
    "    if operator == \"+\":\n",
    "        return a + b\n",
    "    elif operator == \"-\":\n",
    "        return a - b\n",
    "    elif operator == \"*\":\n",
    "        return a * b\n",
    "    elif operator == \"/\":\n",
    "        return int(a / b)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the assistant agent with access to the calculator tool\n",
    "assistant = AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant with access to a calculator tool. \"\n",
    "    \"When given a mathematical expression, you must break it down and use the calculator tool \"\n",
    "    \"for ALL arithmetic operations until you reach the final answer. \"\n",
    "    \"Do not stop until you have calculated the complete final result. \"\n",
    "    \"Show your work step by step and provide the final numerical answer. \"\n",
    "    \"Only say 'TERMINATE' after you have completed the entire calculation.\",\n",
    "    model_client=model_client,\n",
    "    tools=[calculator],\n",
    "    reflect_on_tool_use=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fa14c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a user proxy agent to represent the human user\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    description=\"A user proxy agent that represents the human user.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695b8fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Main async function to run the chat loop\n",
    "async def main():\n",
    "    # The initial question for the assistant to solve\n",
    "    initial_message = \"What is (1423 - 123) / 3 + (32 + 23) * 5? Please complete the entire calculation step by step and show the final answer.\"\n",
    "    print(f\"User: {initial_message}\")\n",
    "\n",
    "    # Start the conversation history with the user's question\n",
    "    conversation_history = [TextMessage(content=initial_message, source=\"user\")]\n",
    "\n",
    "    max_turns = 10  # Prevent infinite loops\n",
    "    turn = 0\n",
    "\n",
    "    # Conversation loop: user and assistant take turns\n",
    "    while turn < max_turns:\n",
    "        turn += 1\n",
    "        print(f\"\\n--- Turn {turn} ---\")\n",
    "\n",
    "        # Send the conversation so far to the assistant and get a response\n",
    "        response = await assistant.on_messages(conversation_history, CancellationToken())\n",
    "\n",
    "        if response.chat_message:\n",
    "            assistant_message = response.chat_message\n",
    "            print(f\"Assistant: {assistant_message.to_text()}\")\n",
    "\n",
    "            # Check if the assistant has finished the calculation\n",
    "            message_text = assistant_message.to_text()\n",
    "            if \"TERMINATE\" in message_text or \"final answer\" in message_text.lower():\n",
    "                print(\"Calculation completed!\")\n",
    "                break\n",
    "\n",
    "            # Add assistant's message to the conversation for the next turn\n",
    "            conversation_history.append(TextMessage(content=message_text, source=\"assistant\"))\n",
    "\n",
    "            # If not done, prompt the assistant to continue\n",
    "            if turn < max_turns - 1: \n",
    "                continue_prompt = \"Please continue with the remaining calculations to get the final answer.\"\n",
    "                conversation_history.append(TextMessage(content=continue_prompt, source=\"user\"))\n",
    "                print(f\"User: {continue_prompt}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Assistant did not provide a response.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6385c5c",
   "metadata": {},
   "source": [
    "You can view data on this run at [app.agentops.ai](app.agentops.ai).\n",
    "\n",
    "The dashboard will display LLM events for each message sent by each agent, including those made by the human user."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
