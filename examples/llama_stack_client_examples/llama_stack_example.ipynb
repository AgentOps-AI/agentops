{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Stack Client Examples\n",
    "Use the llama_stack_client library to interact with a Llama Stack server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (0.0.57)\n",
      "Collecting llama-stack-client\n",
      "  Downloading llama_stack_client-0.0.58-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (4.7.0)\n",
      "Requirement already satisfied: click in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (0.28.0)\n",
      "Requirement already satisfied: pandas in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (3.0.48)\n",
      "Requirement already satisfied: pyaml in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (24.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (2.10.3)\n",
      "Requirement already satisfied: rich in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (13.9.4)\n",
      "Requirement already satisfied: sniffio in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client) (2024.2)\n",
      "Requirement already satisfied: wcwidth in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from prompt-toolkit->llama-stack-client) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pyaml->llama-stack-client) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from rich->llama-stack-client) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from rich->llama-stack-client) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client) (1.17.0)\n",
      "Downloading llama_stack_client-0.0.58-py3-none-any.whl (286 kB)\n",
      "Installing collected packages: llama-stack-client\n",
      "  Attempting uninstall: llama-stack-client\n",
      "    Found existing installation: llama_stack_client 0.0.57\n",
      "    Uninstalling llama_stack_client-0.0.57:\n",
      "      Successfully uninstalled llama_stack_client-0.0.57\n",
      "Successfully installed llama-stack-client-0.0.58\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-stack\n",
      "  Downloading llama_stack-0.0.58-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting blobfile (from llama-stack)\n",
      "  Using cached blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire (from llama-stack)\n",
      "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (0.28.0)\n",
      "Collecting huggingface-hub (from llama-stack)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-models>=0.0.58 (from llama-stack)\n",
      "  Downloading llama_models-0.0.58-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: llama-stack-client>=0.0.58 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (0.0.58)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (3.0.48)\n",
      "Requirement already satisfied: python-dotenv in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (1.0.1)\n",
      "Requirement already satisfied: pydantic>=2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (2.10.3)\n",
      "Requirement already satisfied: requests in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (57.4.0)\n",
      "Requirement already satisfied: termcolor in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack) (2.5.0)\n",
      "Requirement already satisfied: PyYAML in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-models>=0.0.58->llama-stack) (6.0.2)\n",
      "Collecting jinja2 (from llama-models>=0.0.58->llama-stack)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting tiktoken (from llama-models>=0.0.58->llama-stack)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting Pillow (from llama-models>=0.0.58->llama-stack)\n",
      "  Using cached pillow-11.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (4.7.0)\n",
      "Requirement already satisfied: click in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: pyaml in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (24.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from llama-stack-client>=0.0.58->llama-stack) (4.12.2)\n",
      "Requirement already satisfied: certifi in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpx->llama-stack) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpx->llama-stack) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpx->llama-stack) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-stack) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic>=2->llama-stack) (2.27.1)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile->llama-stack)\n",
      "  Using cached pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from blobfile->llama-stack) (2.2.3)\n",
      "Collecting lxml>=4.9 (from blobfile->llama-stack)\n",
      "  Downloading lxml-5.3.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Collecting filelock>=3.0 (from blobfile->llama-stack)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->llama-stack)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from huggingface-hub->llama-stack) (23.2)\n",
      "Requirement already satisfied: wcwidth in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from requests->llama-stack) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from rich->llama-stack) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client>=0.0.58->llama-stack) (1.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->llama-models>=0.0.58->llama-stack)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client>=0.0.58->llama-stack) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client>=0.0.58->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client>=0.0.58->llama-stack) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pandas->llama-stack-client>=0.0.58->llama-stack) (2024.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->llama-models>=0.0.58->llama-stack)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client>=0.0.58->llama-stack) (1.17.0)\n",
      "Downloading llama_stack-0.0.58-py3-none-any.whl (446 kB)\n",
      "Downloading llama_models-0.0.58-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached pillow-11.0.0-cp310-cp310-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl (982 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.4/982.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=28249a5b845d2594cddd5e302164aa8818158be391c1a1b5f0ae4d10c50bd63c\n",
      "  Stored in directory: /Users/a/Library/Caches/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: regex, pycryptodomex, Pillow, MarkupSafe, lxml, fsspec, fire, filelock, tiktoken, jinja2, huggingface-hub, blobfile, llama-models, llama-stack\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 blobfile-3.0.0 filelock-3.16.1 fire-0.7.0 fsspec-2024.10.0 huggingface-hub-0.26.5 jinja2-3.1.4 llama-models-0.0.58 llama-stack-0.0.58 lxml-5.3.0 pycryptodomex-3.21.0 regex-2024.11.6 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: agentops in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (2.32.3)\n",
      "Requirement already satisfied: psutil==5.9.8 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (5.9.8)\n",
      "Requirement already satisfied: packaging==23.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (23.2)\n",
      "Requirement already satisfied: termcolor>=2.3.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (2.5.0)\n",
      "Requirement already satisfied: PyYAML<7.0,>=5.3 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (6.0.2)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.22.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from agentops) (1.28.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.22.0->agentops) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.22.0->agentops) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->agentops) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->agentops) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->agentops) (1.28.2)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->agentops) (5.29.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->agentops) (0.49b2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->agentops) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->agentops) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->agentops) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->agentops) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->agentops) (2024.8.30)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->agentops) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->agentops) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fastapi\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from fastapi) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (4.7.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Installing collected packages: starlette, fastapi\n",
      "Successfully installed fastapi-0.115.6 starlette-0.41.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opentelemetry-api in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (1.28.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api) (8.5.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opentelemetry-sdk in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-api==1.28.2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-sdk) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-sdk) (0.49b2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-sdk) (4.12.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api==1.28.2->opentelemetry-sdk) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from opentelemetry-api==1.28.2->opentelemetry-sdk) (8.5.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api==1.28.2->opentelemetry-sdk) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/a/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api==1.28.2->opentelemetry-sdk) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U llama-stack-client\n",
    "%pip install -U llama-stack\n",
    "%pip install -U agentops\n",
    "%pip install -U python-dotenv\n",
    "%pip install -U fastapi\n",
    "%pip install opentelemetry-api\n",
    "%pip install opentelemetry-sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps:  WARNING: agentops is out of date. Please update with the command: 'pip install --upgrade agentops'\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "from llama_stack_client.types import UserMessage\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import agentops\n",
    "\n",
    "load_dotenv()\n",
    "AGENTOPS_API_KEY = os.getenv(\"AGENTOPS_API_KEY\") or \"<your_agentops_key>\"\n",
    "\n",
    "agentops.init(AGENTOPS_API_KEY, default_tags=[\"llama-stack-client-example\"], auto_start_session=False)\n",
    "\n",
    "host = \"0.0.0.0\" # LLAMA_STACK_HOST\n",
    "port = 5001 # LLAMA_STACK_PORT\n",
    "\n",
    "full_host = f\"http://{host}:{port}\"\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=f\"{full_host}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Canary 1 - Completion with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=ceea2686-c0ed-4190-b106-eeae88ffe5ca\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mAssistant> \u001b[0m\u001b[33mSil\u001b[0m\u001b[33ment\u001b[0m\u001b[33m lunar\u001b[0m\u001b[33m glow\u001b[0m\u001b[97m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: Session Stats - \u001b[1mDuration:\u001b[0m 7.9s | \u001b[1mCost:\u001b[0m $0.00 | \u001b[1mLLMs:\u001b[0m 1 | \u001b[1mTools:\u001b[0m 0 | \u001b[1mActions:\u001b[0m 0 | \u001b[1mErrors:\u001b[0m 0\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=ceea2686-c0ed-4190-b106-eeae88ffe5ca\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agentops.start_session()\n",
    "response = client.inference.chat_completion(\n",
    "    messages=[\n",
    "        UserMessage(\n",
    "            content=\"hello world, write me a 3 word poem about the moon\",\n",
    "            role=\"user\",\n",
    "        ),\n",
    "    ],\n",
    "    model_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "async for log in EventLogger().log(response):\n",
    "    log.print()\n",
    "\n",
    "agentops.end_session(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Canary Example 2 - Completion without Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=990b495b-e4c6-4c78-97d9-21dd47101ff3\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agentops\u001b[38;5;241m.\u001b[39mstart_session()\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mUserMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite me a 3 word poem about the moon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mcompletion_message\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m agentops\u001b[38;5;241m.\u001b[39mend_session(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/agentops/llms/llama_stack_client.py:252\u001b[0m, in \u001b[0;36mLlamaStackClientProvider._override_complete.<locals>.patched_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 252\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_response(result, kwargs, init_timestamp, session\u001b[38;5;241m=\u001b[39msession)\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/resources/inference.py:217\u001b[0m, in \u001b[0;36mInferenceResource.chat_completion\u001b[0;34m(self, messages, model_id, logprobs, response_format, sampling_params, stream, tool_choice, tool_prompt_format, tools, x_llama_stack_provider_data, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    211\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstrip_not_given({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-LlamaStack-ProviderData\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_llama_stack_provider_data}),\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    214\u001b[0m }\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    216\u001b[0m     InferenceChatCompletionResponse,\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/alpha/inference/chat-completion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_prompt_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_prompt_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43minference_chat_completion_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceChatCompletionParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mInferenceChatCompletionResponse\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Union types cannot be passed in as arguments in the type system\u001b[39;49;00m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mInferenceChatCompletionResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1251\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1260\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1261\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1262\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/projects/12_2024/agentops/venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}"
     ]
    }
   ],
   "source": [
    "agentops.start_session()\n",
    "response = client.inference.chat_completion(\n",
    "    messages=[\n",
    "        UserMessage(\n",
    "            content=\"write me a 3 word poem about the moon\",\n",
    "            role=\"user\",\n",
    "        ),\n",
    "    ],\n",
    "    model_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(f\"> Response: {response.completion_message.content}\")\n",
    "agentops.end_session(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Canary Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=48206eed-d5d8-4979-ab6e-3577faff5ad4\u001b[0m\u001b[0m\n",
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=f0f95a35-876f-478d-9542-fe3261ad3d18\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No available shields. Disable safety.\n",
      "Using model: meta-llama/Llama-3.2-1B-Instruct\n",
      "response=<generator object LlamaStackClientProvider.handle_response.<locals>.agent_generator at 0x10f44b370>\n",
      "\u001b[30m\u001b[0m\u001b[33minference> \u001b[0m\u001b[33mHello\u001b[0m\u001b[33m!\u001b[0m\u001b[33m How\u001b[0m\u001b[33m can\u001b[0m\u001b[33m I\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m you\u001b[0m\u001b[33m today\u001b[0m\u001b[33m?\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: Multiple sessions detected. You must use session.record(). More info: https://docs.agentops.ai/v1/concepts/core-concepts#session-management\n",
      "\u001b[31;1m🖇 AgentOps: Could not record event. Start a session by calling agentops.start_session().\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0mresponse=<generator object LlamaStackClientProvider.handle_response.<locals>.agent_generator at 0x10f44a3b0>\n",
      "\u001b[30m\u001b[0m\u001b[33minference> \u001b[0m\u001b[36m\u001b[0m\u001b[36mbr\u001b[0m\u001b[36mave\u001b[0m\u001b[36m_search\u001b[0m\u001b[36m.call\u001b[0m\u001b[36m(query\u001b[0m\u001b[36m=\"\u001b[0m\u001b[36mN\u001b[0m\u001b[36mBA\u001b[0m\u001b[36m Western\u001b[0m\u001b[36m Conference\u001b[0m\u001b[36m Sem\u001b[0m\u001b[36mif\u001b[0m\u001b[36minals\u001b[0m\u001b[36m \u001b[0m\u001b[36m201\u001b[0m\u001b[36m4\u001b[0m\u001b[36m teams\u001b[0m\u001b[36m\")\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: Multiple sessions detected. You must use session.record(). More info: https://docs.agentops.ai/v1/concepts/core-concepts#session-management\n",
      "\u001b[31;1m🖇 AgentOps: Could not record event. Start a session by calling agentops.start_session().\u001b[0m\n",
      "🖇 AgentOps: Multiple sessions detected. You must use session.record(). More info: https://docs.agentops.ai/v1/concepts/core-concepts#session-management\n",
      "\u001b[31;1m🖇 AgentOps: Could not record event. Start a session by calling agentops.start_session().\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:brave_search Args:{'query': 'NBA Western Conference Semifinals 2014 teams'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:brave_search Response:{\"query\": \"NBA Western Conference Semifinals 2014 teams\", \"top_k\": [{\"title\": \"2014 NBA playoffs - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/2014_NBA_playoffs\", \"description\": \"This would be the last Game 7 won by a road <strong>team</strong> until the 2016 <strong>NBA</strong> Finals. Game 4 of the Heat\\u2013Nets series saw LeBron James record a Heat franchise playoff high 49 points. He eventually led the Miami Heat to their fourth consecutive Eastern <strong>Conference</strong> Finals appearance with a win in Game 5. With a Game 5 win over the Portland Trail Blazers, the San Antonio Spurs advanced to the <strong>Western</strong> ...\", \"type\": \"search_result\"}, {\"title\": \"2014 NBA Western Conference Semifinals - Trail Blazers vs. Spurs | Basketball-Reference.com\", \"url\": \"https://www.basketball-reference.com/playoffs/2014-nba-western-conference-semifinals-trail-blazers-vs-spurs.html\", \"description\": \"Summary and statistics for the 2014 NBA Western Conference Semifinals - <strong>Trail Blazers</strong> vs. Spurs\", \"type\": \"search_result\"}, {\"title\": \"2014 NBA Playoffs Summary | Basketball-Reference.com\", \"url\": \"https://www.basketball-reference.com/playoffs/NBA_2014.html\", \"description\": \"Checkout the Results, Statistics, Playoff Leaders, Per Game Stats, Advanced Stats and more for the <strong>2014</strong> <strong>NBA</strong> playoffs on Basketball-Reference.com\", \"type\": \"search_result\"}]}\u001b[0m\n",
      "\u001b[35mshield_call> No Violation\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m winning\u001b[0m\u001b[33m team\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m NBA\u001b[0m\u001b[33m Western\u001b[0m\u001b[33m Conference\u001b[0m\u001b[33m semif\u001b[0m\u001b[33minals\u001b[0m\u001b[33m of\u001b[0m\u001b[33m \u001b[0m\u001b[33m201\u001b[0m\u001b[33m4\u001b[0m\u001b[33m was\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Portland\u001b[0m\u001b[33m Trail\u001b[0m\u001b[33m Blazers\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m game\u001b[0m\u001b[33m was\u001b[0m\u001b[33m played\u001b[0m\u001b[33m between\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Trail\u001b[0m\u001b[33m Blazers\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m San\u001b[0m\u001b[33m Antonio\u001b[0m\u001b[33m Spurs\u001b[0m\u001b[33m,\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Spurs\u001b[0m\u001b[33m ultimately\u001b[0m\u001b[33m advancing\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Western\u001b[0m\u001b[33m Conference\u001b[0m\u001b[33m Finals\u001b[0m\u001b[33m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: Multiple sessions detected. You must use session.record(). More info: https://docs.agentops.ai/v1/concepts/core-concepts#session-management\n",
      "\u001b[31;1m🖇 AgentOps: Could not record event. Start a session by calling agentops.start_session().\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: API Key is invalid: {<your_agentops_key>}.\n",
      "\t    Find your API key at https://app.agentops.ai/settings/projects\n",
      "🖇 AgentOps: Could not end session - multiple sessions detected. You must use session.end_session() instead of agentops.end_session() More info: https://docs.agentops.ai/v1/concepts/core-concepts#session-management\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "\n",
    "agentops.start_session()\n",
    "\n",
    "LLAMA_STACK_PORT = 5001\n",
    "\n",
    "# Replace with actual API keys for functionality\n",
    "BRAVE_SEARCH_API_KEY = os.getenv(\"BRAVE_SEARCH_API_KEY\") or \"your-brave-search-api-key\"\n",
    "\n",
    "async def agent_test():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=f\"http://0.0.0.0:{LLAMA_STACK_PORT}\",\n",
    "    )\n",
    "\n",
    "    available_shields = [shield.identifier for shield in client.shields.list()]\n",
    "    if not available_shields:\n",
    "        print(\"No available shields. Disable safety.\")\n",
    "    else:\n",
    "        print(f\"Available shields found: {available_shields}\")\n",
    "    available_models = [model.identifier for model in client.models.list()]\n",
    "    if not available_models:\n",
    "        raise ValueError(\"No available models\")\n",
    "    else:\n",
    "        selected_model = available_models[0]\n",
    "        print(f\"Using model: {selected_model}\")\n",
    "\n",
    "    agent_config = AgentConfig(\n",
    "        model=selected_model,\n",
    "        instructions=\"You are a helpful assistant. Just say hello as a greeting.\",\n",
    "        sampling_params={\n",
    "            \"strategy\": \"greedy\",\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 0.9,\n",
    "        },\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"brave_search\",\n",
    "                \"engine\": \"brave\",\n",
    "                \"api_key\": BRAVE_SEARCH_API_KEY,\n",
    "            }\n",
    "        ],\n",
    "        tool_choice=\"auto\",\n",
    "        tool_prompt_format=\"json\",\n",
    "        input_shields=available_shields if available_shields else [],\n",
    "        output_shields=available_shields if available_shields else [],\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "    agent = Agent(client, agent_config)\n",
    "    user_prompts = [\n",
    "        \"Hello\",\n",
    "        \"Which players played in the winning team of the NBA western conference semifinals of 2014, please use tools\",\n",
    "    ]\n",
    "\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "        )\n",
    "\n",
    "        print(f\"{response=}\")\n",
    "\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "agentops.start_session()\n",
    "\n",
    "await agent_test()\n",
    "\n",
    "agentops.end_session(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖇 AgentOps: Session Stats - \u001b[1mDuration:\u001b[0m 28.8s | \u001b[1mCost:\u001b[0m $0.00 | \u001b[1mLLMs:\u001b[0m 0 | \u001b[1mTools:\u001b[0m 0 | \u001b[1mActions:\u001b[0m 0 | \u001b[1mErrors:\u001b[0m 0\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=990b495b-e4c6-4c78-97d9-21dd47101ff3\u001b[0m\u001b[0m\n",
      "🖇 AgentOps: Session Stats - \u001b[1mDuration:\u001b[0m 19.6s | \u001b[1mCost:\u001b[0m $0.00 | \u001b[1mLLMs:\u001b[0m 0 | \u001b[1mTools:\u001b[0m 0 | \u001b[1mActions:\u001b[0m 0 | \u001b[1mErrors:\u001b[0m 0\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=516a6f7f-56b5-4f04-bad6-a42d76fc7f55\u001b[0m\u001b[0m\n",
      "🖇 AgentOps: Session Stats - \u001b[1mDuration:\u001b[0m 9.8s | \u001b[1mCost:\u001b[0m $0.00 | \u001b[1mLLMs:\u001b[0m 0 | \u001b[1mTools:\u001b[0m 0 | \u001b[1mActions:\u001b[0m 0 | \u001b[1mErrors:\u001b[0m 0\n",
      "🖇 AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=e6a248fb-b78c-4fd4-bffe-50a0a8065bfa\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agentops.end_all_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
