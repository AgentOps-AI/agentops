services:
  ollama:
    hostname: ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"
    image: ollama/ollama:latest
    volumes:
      - ~/.ollama:/root/.ollama
    environment:
      OLLAMA_DEBUG: 1
    command: []
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    healthcheck:
      test: ["CMD", "bash", "-c", "</dev/tcp/localhost/11434"]
      interval: 3s
      timeout: 5s
      retries: 5
    networks:
      - ollama-network

  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=ollama
      - INFERENCE_MODEL=${OLLAMA_MODEL:-llama3.2:3b-instruct-fp16}
      - SAFETY_MODEL=${SAFETY_MODEL:-}
    volumes:
      - ~/.ollama:/root/.ollama
      - ./pull-models.sh:/root/pull-models.sh
    entrypoint: ["/root/pull-models.sh"]
    networks:
      - ollama-network

  llamastack:
    depends_on:
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    image: ${LLAMA_STACK_IMAGE:-llamastack/distribution-ollama}
    volumes:
      - ~/.llama:/root/.llama
      - ./run${SAFETY_MODEL:+-with-safety}.yaml:/root/run.yaml
    ports:
      - "${LLAMA_STACK_PORT:-5001}:${LLAMA_STACK_PORT:-5001}"
    environment:
      - INFERENCE_MODEL=${INFERENCE_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      - SAFETY_MODEL=${SAFETY_MODEL:-}
      - OLLAMA_URL=http://ollama:11434
    entrypoint: >
        python -m llama_stack.distribution.server.server --yaml-config /root/run.yaml --port ${LLAMA_STACK_PORT:-5001}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 60s
    networks:
      - ollama-network

networks:
  ollama-network:
    driver: bridge
volumes:
  ollama-init:
  llamastack:
