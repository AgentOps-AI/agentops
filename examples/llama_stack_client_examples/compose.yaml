services:
  ollama:
    hostname: ollama
    # extra_hosts:
      # - "host.docker.internal:host-gateway"
    image: ollama/ollama:latest
    volumes:
      - ~/.ollama:/root/.ollama
    environment:
      OLLAMA_DEBUG: 1
    command: []
    deploy:
      resources:
        limits:
          memory: 4G    # Set maximum memory
        reservations:
          memory: 2G    # Set minimum memory reservation
    healthcheck:
      test: ["CMD", "sh", "-c", "echo -e \"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\" | nc localhost 11434 | grep -q \"HTTP/1.1 200\""]
      interval: 3s
      timeout: 5s
      retries: 5
    networks:
      - ollama-network

  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=ollama
      - INFERENCE_MODEL=${INFERENCE_MODEL:-llama3.2:3b-instruct-fp16}
      - SAFETY_MODEL=${SAFETY_MODEL:-}
    volumes:
      - ~/.ollama:/root/.ollama
      - ./pull-models.sh:/root/pull-models.sh
    entrypoint: ["/root/pull-models.sh"]
    networks:
      - ollama-network

  llamastack:
    depends_on:
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_started
    image: ${LLAMA_STACK_IMAGE:-llamastack/distribution-ollama}
    volumes:
      - ~/.llama:/root/.llama
      # Link to ollama run.yaml file
      # - ~/local/llama-stack/:/app/llama-stack-source
      - ./run${SAFETY_MODEL:+-with-safety}.yaml:/root/run.yaml
    ports:
      - "${LLAMA_STACK_PORT:-5001}:${LLAMA_STACK_PORT:-5001}"
    environment:
      - INFERENCE_MODEL=${INFERENCE_MODEL}
      - SAFETY_MODEL=${SAFETY_MODEL:-}
      - OLLAMA_URL=http://ollama:11434
    entrypoint: >
        python -m llama_stack.distribution.server.server --yaml-config /root/run.yaml --port ${LLAMA_STACK_PORT:-5001}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 60s
  # notebook:
  #   image: python:3.12
  #   depends_on:
  #     llamastack:
  #       condition: service_started
  #   network_mode: ${NETWORK_MODE:-bridge}
  #   volumes:
  #     - ./notebook.ipynb:/app/notebook.ipynb
  #   command: >
  #     bash -c "pip install llama-stack-client jupyter nbconvert &&
  #     jupyter nbconvert --to python /app/notebook.ipynb &&
  #     python /app/notebook.py"
  #   restart: "no"
networks:
  ollama-network:
    driver: bridge
volumes:
  ollama-init:
  llamastack:
