services:
  # Ollama server service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_server
    ports:
      - "11434:11434" # Map Ollama's port to host
    volumes:
      - ~/.ollama/models:/root/.ollama # Persist data (e.g., downloaded models)
    entrypoint: ["ollama", "serve"] # Start the Ollama server
    restart: always # Ensure Ollama server restarts on failure

  # Ephemeral service to trigger model download
  model_downloader:
    image: curlimages/curl:latest # Use a lightweight image with curl
    depends_on:
      - ollama # Ensure the Ollama server starts first
    entrypoint: >
      sh -c "sleep 5 &&
      curl -X POST http://ollama:11434/api/pull -d '{\"model\": \"llama3.2:1b-instruct-fp16\"}'"
    restart: "no" # Ensure this service doesn't restart


  tester:
    image: curlimages/curl:latest # Use a lightweight image with curl
    depends_on:
      - model_downloader # Ensure the Ollama server starts first
    entrypoint: >
      sh -c "sleep 5 &&
      curl -X POST http://ollama:11434/api/generate -d '{\"model\": \"llama3.2:1b-instruct-fp16\",\"prompt\": \"Say 3 words\"}'"
    restart: "no" # Ensure this service doesn't restart

  llama-stack:
    image: llamastack/distribution-ollama
    container_name: llama_stack_server
    ports:
      - "5001:5001"
    volumes:
      - "~/.ollama/models:/root/.ollama"
      - "./llama-stack-server-config.yaml:/root/my-run.yaml"
    environment:
      - INFERENCE_MODEL=meta-llama/Llama-3.2-1B-Instruct
      - OLLAMA_URL=http://ollama:11434
    command: >
      --yaml-config /root/my-run.yaml
      --port 5001
    platform: linux/amd64
    depends_on:
      - ollama
      - model_downloader
      - tester

networks:
  default:
    driver: bridge
