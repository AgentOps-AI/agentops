{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentOps Ollama Integration\n",
    "\n",
    "This example demonstrates how to use AgentOps to monitor your Ollama LLM calls.\n",
    "\n",
    "First let's install the required packages\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: Make sure you have Ollama installed and running locally before running this notebook. You can install it from [ollama.ai](https://ollama.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.8)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: agentops in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (0.3.17)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from agentops) (2.32.3)\n",
      "Requirement already satisfied: psutil==5.9.8 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from agentops) (5.9.8)\n",
      "Requirement already satisfied: packaging==23.2 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from agentops) (23.2)\n",
      "Requirement already satisfied: termcolor>=2.3.0 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from agentops) (2.5.0)\n",
      "Requirement already satisfied: PyYAML<7.0,>=5.3 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from agentops) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->agentops) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->agentops) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->agentops) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->agentops) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/gpt_desk/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ollama\n",
    "%pip install -U agentops\n",
    "%pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import agentops\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set our API keys. For Ollama, we'll need to make sure Ollama is running locally.\n",
    "[Get an AgentOps API key](https://agentops.ai/settings/projects)\n",
    "\n",
    "1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...\n",
    "2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our environment variables\n",
    "load_dotenv()\n",
    "\n",
    "AGENTOPS_API_KEY = os.getenv(\"AGENTOPS_API_KEY\") or \"<your_agentops_key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üñá AgentOps: AgentOps has already been initialized. If you are trying to start a session, call agentops.start_session() instead.\n",
      "üñá AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=3dbe53dd-c2c8-4e2a-9cbe-60b467af3e6f\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agentops.session.Session at 0x1205d0f50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize AgentOps with some default tags\n",
    "agentops.init(AGENTOPS_API_KEY, default_tags=[\"ollama-example\"])\n",
    "agentops.start_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some basic calls to Ollama. Make sure you have pulled the model first, use the following or replace with whichever model you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Real-time Monitoring: AgentOps provides real-time monitoring of Language Learning Models (LLMs), allowing developers and teams to quickly identify and address any issues that may arise during operation.\n",
      "\n",
      "2. Improved Performance: By continuously monitoring the performance of LLMs, developers can optimize them for better efficiency, accuracy, and reliability. This leads to a better user experience.\n",
      "\n",
      "3. Error Detection and Resolution: AgentOps helps in detecting errors and anomalies in the models, enabling developers to quickly resolve issues before they impact users or cause significant downtime.\n",
      "\n",
      "4. Predictive Analytics: With its advanced analytics capabilities, AgentOps can help predict potential issues or bottlenecks, allowing teams to proactively address them and maintain optimal performance.\n",
      "\n",
      "5. Cost Optimization: By identifying areas where the model can be optimized for better resource utilization, AgentOps can help reduce costs associated with running LLMs.\n",
      "\n",
      "6. Compliance and Security: AgentOps can assist in ensuring compliance with relevant industry standards and regulations, as well as helping to secure LLMs against potential threats or vulnerabilities.\n",
      "\n",
      "7. Scalability: As the usage of LLMs grows, AgentOps can scale seamlessly to handle increased demand, ensuring that the models continue to perform optimally even under high load conditions.\n",
      "\n",
      "8. Integration Capabilities: AgentOps can be easily integrated with other tools and systems used by development teams, providing a unified view of the entire LLM deployment ecosystem. This simplifies management and makes it easier to identify areas for improvement.\n",
      "\n",
      "9. Continuous Learning: By continuously monitoring and analyzing the performance of LLMs, AgentOps enables developers to learn from past experiences and improve the models over time, leading to better overall performance.\n",
      "\n",
      "10. Reduced Time-to-Market: With AgentOps handling many aspects of model monitoring and optimization, development teams can focus on other critical tasks, reducing the time-to-market for new LLM-based products or services.\n"
     ]
    }
   ],
   "source": [
    "# Basic completion,\n",
    "response = ollama.chat(model='mistral',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What are the benefits of using AgentOps for monitoring LLMs?',\n",
    "    }]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try streaming responses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Silent guardians,\n",
      "\n",
      "Invisible threads weave,\n",
      "Wisdom in every node. Silent guardians,\n",
      "\n",
      "Invisible threads weave,\n",
      "Wisdom in every node."
     ]
    }
   ],
   "source": [
    "# Streaming Example\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Write a haiku about monitoring AI agents',\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Real-time monitoring: AgentOps provides real-time visibility into the performance of your LLM applications, allowing you to quickly identify and resolve issues before they impact users.\n",
      "\n",
      "2. In-depth insights: With customizable dashboards and alerts, AgentOps helps you gain deep insights into the behavior of your applications, enabling you to optimize their performance and improve the user experience.\n",
      "\n",
      "3. Easy integration: AgentOps offers seamless integration with popular technologies and frameworks such as Kubernetes, Docker, and Cloud Foundry, making it simple to deploy and manage across diverse environments.\n"
     ]
    }
   ],
   "source": [
    "# Conversation Example\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is AgentOps?'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'AgentOps is a monitoring and observability platform for LLM applications.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Can you give me 3 key features?'\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=messages\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° **Note**: In production environments, you should add proper error handling around the Ollama calls and use `agentops.end_session(\"Error\")` when exceptions occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's end our AgentOps session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üñá AgentOps: Session Stats - \u001b[1mDuration:\u001b[0m 30.1s | \u001b[1mCost:\u001b[0m $0.00 | \u001b[1mLLMs:\u001b[0m 3 | \u001b[1mTools:\u001b[0m 0 | \u001b[1mActions:\u001b[0m 0 | \u001b[1mErrors:\u001b[0m 0\n",
      "üñá AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=3dbe53dd-c2c8-4e2a-9cbe-60b467af3e6f\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agentops.end_session(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_desk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
