{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# AgentOps Ollama Integration\n",
       "\n",
       "This example demonstrates how to use AgentOps to monitor your Ollama LLM calls.\n",
       "\n",
       "First let's install the required packages\n",
       "\n",
       "> ‚ö†Ô∏è **Important**: Make sure you have Ollama installed and running locally before running this notebook. You can install it from [ollama.ai](https://ollama.com)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "%pip install -U ollama\n",
       "%pip install -U agentops\n",
       "%pip install -U python-dotenv"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Then import them"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "import ollama\n",
       "import agentops\n",
       "import os\n",
       "from dotenv import load_dotenv\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Next, we'll set our API keys. For Ollama, we'll need to make sure Ollama is running locally.\n",
       "[Get an AgentOps API key](https://agentops.ai/settings/projects)\n",
       "\n",
       "1. Create an environment variable in a .env file or other method. By default, the AgentOps `init()` function will look for an environment variable named `AGENTOPS_API_KEY`. Or...\n",
       "2. Replace `<your_agentops_key>` below and pass in the optional `api_key` parameter to the AgentOps `init(api_key=...)` function. Remember not to commit your API key to a public repo!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Let's load our environment variables\n",
       "load_dotenv()\n",
       "\n",
       "AGENTOPS_API_KEY = os.getenv(\"AGENTOPS_API_KEY\") or \"<your_agentops_key>\""
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize AgentOps with some default tags\n",
       "agentops.init(AGENTOPS_API_KEY, default_tags=[\"ollama-example\"])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Now let's make some basic calls to Ollama. Make sure you have pulled the model first, use the following or replace with whichever model you want to use."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "ollama.pull(\"mistral\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Basic completion,\n",
       "response = ollama.chat(model='mistral',\n",
       "    messages=[{\n",
       "        'role': 'user',\n",
       "        'content': 'What are the benefits of using AgentOps for monitoring LLMs?',\n",
       "    }]\n",
       ")\n",
       "print(response['message']['content'])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Let's try streaming responses as well"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Streaming Example\n",
       "stream = ollama.chat(\n",
       "    model='mistral',\n",
       "    messages=[{\n",
       "        'role': 'user',\n",
       "        'content': 'Write a haiku about monitoring AI agents',\n",
       "    }],\n",
       "    stream=True\n",
       ")\n",
       "\n",
       "for chunk in stream:\n",
       "    print(chunk['message']['content'], end='')\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Conversation Example\n",
       "messages = [\n",
       "    {\n",
       "        'role': 'user',\n",
       "        'content': 'What is AgentOps?'\n",
       "    },\n",
       "    {\n",
       "        'role': 'assistant',\n",
       "        'content': 'AgentOps is a monitoring and observability platform for LLM applications.'\n",
       "    },\n",
       "    {\n",
       "        'role': 'user',\n",
       "        'content': 'Can you give me 3 key features?'\n",
       "    }\n",
       "]\n",
       "\n",
       "response = ollama.chat(\n",
       "    model='mistral',\n",
       "    messages=messages\n",
       ")\n",
       "print(response['message']['content'])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "> üí° **Note**: In production environments, you should add proper error handling around the Ollama calls and use `agentops.end_session(\"Error\")` when exceptions occur."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Finally, let's end our AgentOps session"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "agentops.end_session(\"Success\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "gpt_desk",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   