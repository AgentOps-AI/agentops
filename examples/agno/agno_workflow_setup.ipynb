{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Workflow Setup with Intelligent Caching\n",
    "\n",
    "This notebook demonstrates how to build custom workflows in Agno that can orchestrate complex agent interactions with performance optimizations.\n",
    "\n",
    "## What are Workflows?\n",
    "\n",
    "Workflows are powerful abstractions that allow you to:\n",
    "- **Orchestrate** multiple agents and teams in complex patterns\n",
    "- **Maintain state** across multiple invocations\n",
    "- **Implement custom logic** like caching, routing, or validation\n",
    "- **Optimize performance** through intelligent design patterns\n",
    "\n",
    "## Why Use Caching?\n",
    "\n",
    "Caching is particularly valuable for AI agents because:\n",
    "- **Cost Reduction**: Avoid redundant API calls for identical queries\n",
    "- **Performance**: Instant responses for repeated questions\n",
    "- **Development**: Faster iteration during testing\n",
    "- **User Experience**: Immediate responses for common queries\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This caching workflow pattern is ideal for:\n",
    "- **FAQ Systems**: Where users ask similar questions repeatedly\n",
    "- **Development/Testing**: To avoid API costs during iteration\n",
    "- **Customer Support**: For common inquiries with standard responses\n",
    "- **Documentation Assistants**: Where queries about specific topics repeat\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "Ensure you have:\n",
    "- **AgentOps API key** from [AgentOps](https://agentops.ai)\n",
    "- **OpenAI API key** from [OpenAI](https://openai.com)\n",
    "\n",
    "Create a `.env` file:\n",
    "```\n",
    "AGENTOPS_API_KEY=your_agentops_key\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "```\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Implementation Overview\n",
    "\n",
    "Let's build a caching workflow that demonstrates how to optimize agent performance through intelligent response caching.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "First, we'll import the necessary components for building workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from agno.agent import Agent, RunResponse \n",
    "from agno.team import Team\n",
    "import asyncio\n",
    "import agentops\n",
    "from dotenv import load_dotenv\n",
    "from agno.workflow import Workflow\n",
    "from agno.utils.pprint import pprint_run_response\n",
    "from agno.models.openai import OpenAIChat\n",
    "from agno.utils.log import logger\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "Load environment variables and initialize AgentOps monitoring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AgentOps for monitoring workflow execution\n",
    "agentops.init(api_key=os.getenv(\"AGENTOPS_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"gpt-4o-mini\"  # Default model for agents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcd36544",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Environment Validation\n",
    "\n",
    "Before proceeding, let's ensure all required API keys are configured:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f2612",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_environment():\n",
    "    \"\"\"\n",
    "    Verify that all required API keys are properly configured.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all required environment variables are set\n",
    "    \"\"\"\n",
    "    required_vars = [\"AGENTOPS_API_KEY\", \"OPENAI_API_KEY\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "    if missing_vars:\n",
    "        print(f\"Missing required environment variables: {missing_vars}\")\n",
    "        print(\"Please set these in your .env file or environment\")\n",
    "        return False\n",
    "\n",
    "    print(\"✓ Environment variables checked successfully\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68b12e89",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Building the CacheWorkflow Class\n",
    "\n",
    "Now let's create our custom workflow class that implements intelligent caching:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Workflow Base Class**: Inherit from `Workflow` to get state management capabilities\n",
    "2. **Session State**: Persistent storage that survives across workflow runs\n",
    "3. **Agent Integration**: Embed agents as workflow attributes\n",
    "4. **Custom Logic**: Implement caching in the `run()` method\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Check Cache**: Look for existing responses in session state\n",
    "2. **Cache Hit**: Return immediately without API call\n",
    "3. **Cache Miss**: Generate new response and cache it\n",
    "4. **Stream Support**: Maintain real-time response streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f388879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheWorkflow(Workflow):\n",
    "    \"\"\"\n",
    "    A workflow that demonstrates intelligent caching capabilities.\n",
    "    \n",
    "    This workflow:\n",
    "    - Caches agent responses to avoid redundant API calls\n",
    "    - Maintains session state across multiple invocations\n",
    "    - Provides instant responses for repeated queries\n",
    "    - Reduces costs and improves performance\n",
    "    \n",
    "    Use cases:\n",
    "    - FAQ systems where questions repeat frequently\n",
    "    - Development/testing to avoid repeated API calls\n",
    "    - Systems with predictable query patterns\n",
    "    \"\"\"\n",
    "\n",
    "    # Workflow metadata (descriptive, not functional)\n",
    "    description: str = \"A workflow that caches previous outputs for efficiency\"\n",
    "\n",
    "    # Initialize agents as workflow attributes\n",
    "    # This agent will be used to generate responses when cache misses occur\n",
    "    agent = Agent(\n",
    "        model=OpenAIChat(id=MODEL_ID),\n",
    "        description=\"General purpose agent for generating responses\"\n",
    "    )\n",
    "\n",
    "    def run(self, message: str) -> Iterator[RunResponse]:\n",
    "        \"\"\"\n",
    "        Execute the workflow with caching logic.\n",
    "        \n",
    "        This method:\n",
    "        1. Checks if the response is already cached\n",
    "        2. Returns cached response immediately if found\n",
    "        3. Generates new response if not cached\n",
    "        4. Caches the new response for future use\n",
    "        \n",
    "        Args:\n",
    "            message: The input query to process\n",
    "            \n",
    "        Yields:\n",
    "            RunResponse: Streamed response chunks\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking cache for '{message}'\")\n",
    "        \n",
    "        # Check if we've already processed this exact message\n",
    "        # session_state persists across workflow runs\n",
    "        if self.session_state.get(message):\n",
    "            logger.info(f\"Cache hit for '{message}'\")\n",
    "            # Return cached response immediately (no API call needed)\n",
    "            yield RunResponse(\n",
    "                run_id=self.run_id, \n",
    "                content=self.session_state.get(message)\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Cache miss - need to generate new response\n",
    "        logger.info(f\"Cache miss for '{message}'\")\n",
    "        \n",
    "        # Run the agent and stream the response\n",
    "        # Using stream=True for real-time output\n",
    "        yield from self.agent.run(message, stream=True)\n",
    "\n",
    "        # After streaming completes, cache the full response\n",
    "        # This makes future requests for the same message instant\n",
    "        self.session_state[message] = self.agent.run_response.content\n",
    "        logger.info(f\"Cached response for future use\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa918b08",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Demonstration Function\n",
    "\n",
    "The following function showcases the dramatic performance improvement from caching:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a925ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_workflows():\n",
    "    \"\"\"\n",
    "    Demonstrate workflow capabilities with caching.\n",
    "    \n",
    "    This function shows:\n",
    "    - How to create and use custom workflows\n",
    "    - The performance benefits of caching\n",
    "    - Session state persistence\n",
    "    - Response streaming\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"WORKFLOWS WITH INTELLIGENT CACHING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Create an instance of our caching workflow\n",
    "        print(\"\\n1. Creating CacheWorkflow instance...\")\n",
    "        workflow = CacheWorkflow()\n",
    "        print(\"   ✓ Workflow initialized with caching capabilities\")\n",
    "\n",
    "        # First run - this will be a cache miss\n",
    "        print(\"\\n2. First run (expecting cache miss):\")\n",
    "        print(\"   This will make an API call and take ~1-2 seconds\")\n",
    "        \n",
    "        # Run workflow with a test message\n",
    "        response: Iterator[RunResponse] = workflow.run(message=\"Tell me a joke.\")\n",
    "        \n",
    "        # Pretty print the response with timing information\n",
    "        pprint_run_response(response, markdown=True, show_time=True)\n",
    "\n",
    "        # Second run - this should be a cache hit\n",
    "        print(\"\\n3. Second run (expecting cache hit):\")\n",
    "        print(\"   This should return instantly from cache\")\n",
    "        \n",
    "        # Run workflow with the same message\n",
    "        response: Iterator[RunResponse] = workflow.run(message=\"Tell me a joke.\")\n",
    "        \n",
    "        # Pretty print the response - notice the instant response time\n",
    "        pprint_run_response(response, markdown=True, show_time=True)\n",
    "        \n",
    "        print(\"\\n✓ Workflow demonstration completed\")\n",
    "        print(\"\\nNotice the performance difference:\")\n",
    "        print(\"- First run: Makes API call, takes time\")\n",
    "        print(\"- Second run: Returns from cache instantly\")\n",
    "        print(\"- Same content, but much faster delivery\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during workflow demonstration: {e}\")\n",
    "        print(\"This might be due to API issues or configuration problems\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35d5cfd4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Main Orchestration\n",
    "\n",
    "The main function coordinates the entire demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function that orchestrates the workflow demonstration.\n",
    "    \n",
    "    This async function handles:\n",
    "    - Environment validation\n",
    "    - Running the workflow demonstration\n",
    "    - Error handling and user feedback\n",
    "    \"\"\"\n",
    "    print(\"Welcome to Agno Workflow Demo\")\n",
    "    print(\"This demo showcases custom workflows with caching capabilities\")\n",
    "    print()\n",
    "    \n",
    "    # Validate environment setup\n",
    "    if not check_environment():\n",
    "        print(\"Cannot proceed without proper API configuration\")\n",
    "        return\n",
    "\n",
    "    # Run demonstration\n",
    "    print(\"\\nStarting workflow demonstration...\")\n",
    "\n",
    "    try:\n",
    "        demonstrate_workflows()\n",
    "        print(\"\\n\\n✓ Workflow demo completed successfully!\")\n",
    "        print(\"\\nKey Takeaways:\")\n",
    "        print(\"- Workflows enable custom agent orchestration\")\n",
    "        print(\"- Caching dramatically improves performance\")\n",
    "        print(\"- Session state persists across runs\")\n",
    "        print(\"- Streaming responses provide real-time feedback\")\n",
    "        print(\"- AgentOps tracks all workflow executions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Demo failed: {e}\")\n",
    "        print(\"Please check your API keys and network connection\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e242b650",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Execute the Demo\n",
    "\n",
    "Run the following cell to see the caching workflow in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Entry point for the script.\n",
    "    \n",
    "    Uses asyncio to run the main function, maintaining consistency\n",
    "    with other examples and preparing for async operations.\n",
    "    \"\"\"\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
