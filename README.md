# AgentOps ğŸ•µï¸

AI agents suck. Weâ€™re fixing that.

Build your next agent with evals, observability, and replay analytics. AgentOps is the toolkit for evaluating and developing robust and reliable AI agents.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Latest Release ğŸ“¦
`version: 0.0.5`
This is an alpha release for early testers.

Agentops is still in closed alpha. You can sign up for an API key [here](https://forms.gle/mFAP4XEoaiKXb2Xh9).

# Quick Start

```pip install agentops```

### Analytics in 4 lines of code
Initialize the AgentOps client, and automatically get analytics on every LLM call.

```python python
import openai # Make sure openai is imported before instantiating an AgentOps client.
import agentops

# Beginning of program's code (i.e. main.py, __init__.py)
ao_client = agentops.Client(<INSERT YOUR API KEY HERE>)

... 
# End of program
ao_client.end_session('Success')
# Woohoo You're done ğŸ‰
```

Refer to our [API documentation](http://docs.agentops.ai) for detailed instructions.


### Why AgentOps? ğŸ¤”

Agent developers often work with little to no visibility into agent testing performance. This means their agents never leave the lab. We're changing that. 

AgentOps is the easiest way to evaluate, grade, and test agents. Our mission is to make sure your agents are ready for production.

## Evaluations Roadmap ğŸ§­

| Platform | Dashboard | Evals |
|---|---|---|
|âœ… Python SDK | âœ… Multi-session and Cross-session metrics | ğŸš§ Evaluation playground + leaderboard |
|ğŸš§ Evaluation builder API | âœ… Custom event tag trackingÂ | ğŸ”œ Agent scorecards |
|ğŸ”œ Javascript/Typescript SDK | ğŸš§ Session replays| ğŸ”œ Custom eval metrics |


## Debugging Roadmap ğŸ§­

| Performance testing | Environments | LAA (LLM augmented agents) specific tests | Reasoning and execution testing |
|---|---|---|---|
|âœ… Event latency analysis | ğŸ”œ Non-stationary environment testing | ğŸ”œ LLM non-deterministic function detection | ğŸš§ Infinite loops and recursive thought detection |
|âœ… Agent workflow execution pricing | ğŸ”œ Multi-modal environments | ğŸ”œ Token limit overflow flags | ğŸ”œ Faulty reasoning detection |
|ğŸ”œ Success validators (external) | ğŸ”œ Execution containers | ğŸ”œ Context limit overflow flags | ğŸ”œ Generative code validators |
|ğŸ”œ Agent controllers/skill tests | ğŸ”œ Honeypot and prompt injection evaluation | ğŸ”œ API bill tracking | ğŸ”œ Error breakpoint analysis |
|ğŸ”œ Information context constraint testing | ğŸ”œ Anti-agent roadblocks (i.e. Captchas) | | |
|ğŸ”œ Regression testing | | | |

## Agent Arena ğŸ¥Š
(coming soon!)

## Time travel debugging ğŸ”®
(coming soon!)

# Installation & Usage ğŸ“˜

```bash
pip install agentops
```

# Join the Revolution ğŸ‰

Is there a feature you'd like to see AgentOps cover? Just raise it in the issues tab, and we'll work on adding it to the roadmap.

We're on a mission to improve AI agents, and we want you to be a part of it. Start building your next agent with Agent SDK today!
